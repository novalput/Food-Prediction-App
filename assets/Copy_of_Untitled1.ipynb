{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu5xKB-ygsAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e73478f-060b-46c9-ffec-15b618a5000f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ–¥ï¸ GPU Information:\n",
            "========================================\n",
            "TensorFlow version: 2.18.0\n",
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Built with CUDA: True\n",
            "PyTorch CUDA available: True\n",
            "GPU Name: Tesla T4\n",
            "GPU Memory: 15.8 GB\n",
            "âœ… GPU computation test passed!\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Check GPU availability\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "print(\"ğŸ–¥ï¸ GPU Information:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# TensorFlow GPU check\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
        "print(f\"Built with CUDA: {tf.test.is_built_with_cuda()}\")\n",
        "\n",
        "# PyTorch GPU check (for reference)\n",
        "print(f\"PyTorch CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "# Test GPU computation\n",
        "with tf.device('/GPU:0'):\n",
        "    a = tf.random.normal([1000, 1000])\n",
        "    b = tf.random.normal([1000, 1000])\n",
        "    c = tf.matmul(a, b)\n",
        "    print(\"âœ… GPU computation test passed!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RpIhAUAF5QG6",
        "outputId": "6d651089-79cd-4c7a-9b26-3f5c380d264e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Download Food-101 dataset directly\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "def download_food101_to_drive():\n",
        "    drive_path = '/content/drive/MyDrive/food-101'  # Lokasi di Google Drive\n",
        "    tar_path = '/content/drive/MyDrive/food-101.tar.gz'\n",
        "\n",
        "    print(\"ğŸ“¥ Downloading Food-101 dataset...\")\n",
        "\n",
        "    url = \"http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\"\n",
        "\n",
        "    if not os.path.exists(tar_path):\n",
        "        print(\"Downloading... This will take a few minutes.\")\n",
        "        urllib.request.urlretrieve(url, tar_path)\n",
        "        print(\"âœ… Download complete!\")\n",
        "    else:\n",
        "        print(\"Dataset archive already exists in Drive!\")\n",
        "\n",
        "    # Extract dataset\n",
        "    if not os.path.exists(drive_path):\n",
        "        print(\"ğŸ“‚ Extracting dataset to Google Drive...\")\n",
        "        with tarfile.open(tar_path, 'r:gz') as tar:\n",
        "            tar.extractall(path='/content/drive/MyDrive/')\n",
        "        print(\"âœ… Extraction complete!\")\n",
        "    else:\n",
        "        print(\"Dataset already extracted in Drive!\")\n",
        "\n",
        "    print(f\"Dataset location: {drive_path}\")\n",
        "    return drive_path\n",
        "\n",
        "# Download and extract to Google Drive\n",
        "dataset_path = download_food101_to_drive()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz8NAlBzhFxU",
        "outputId": "dfc786e1-61c3-4717-9bf2-cc7634f0425f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¥ Downloading Food-101 dataset...\n",
            "Dataset archive already exists in Drive!\n",
            "Dataset already extracted in Drive!\n",
            "Dataset location: /content/drive/MyDrive/food-101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# STEP 4: Path setup\n",
        "base_dir = \"food-101\"\n",
        "train_dir = os.path.join(base_dir, \"images\")\n",
        "image_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "# STEP 5: Data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2,\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")"
      ],
      "metadata": {
        "id": "yvMUu82_2nJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9fdeb01-8215-4de9-b3be-5261309530ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 80800 images belonging to 101 classes.\n",
            "Found 20200 images belonging to 101 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Build model (EfficientNetB0 for speed + accuracy)\n",
        "base_model = tf.keras.applications.EfficientNetB0(\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "base_model.trainable = False\n",
        "\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(101, activation='softmax')  # 101 classes\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "nz9JnL2nhKU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c97ccf7e-0846-4d61-c996-c1e02730e0dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Train\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# STEP 8: Save model\n",
        "model.save(\"food101_model.h5\")"
      ],
      "metadata": {
        "id": "ziz5CEip25vd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00e93637-8147-42ee-eaf5-39f2d26e9254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m 381/2525\u001b[0m \u001b[32mâ”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m15:24\u001b[0m 431ms/step - accuracy: 0.0070 - loss: 4.6369"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Optimized dataset creation for Colab\n",
        "def create_colab_dataset():\n",
        "    \"\"\"\n",
        "    Create optimized dataset pipeline for Google Colab\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"ğŸ“Š Creating optimized dataset pipeline...\")\n",
        "\n",
        "    # Read classes\n",
        "    with open(config.CLASSES_FILE, 'r') as f:\n",
        "        all_classes = [line.strip() for line in f.readlines()]\n",
        "\n",
        "    print(f\"Number of classes: {len(all_classes)}\")\n",
        "\n",
        "    # Collect all image paths and labels\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    for class_idx, class_name in enumerate(all_classes):\n",
        "        class_dir = os.path.join(config.DATA_DIR, class_name)\n",
        "        if os.path.exists(class_dir):\n",
        "            class_images = []\n",
        "            for img_file in os.listdir(class_dir):\n",
        "                if img_file.lower().endswith(('.jpg', '.jpeg')):\n",
        "                    img_path = os.path.join(class_dir, img_file)\n",
        "                    if os.path.getsize(img_path) > 0:  # Check file is not empty\n",
        "                        class_images.append(img_path)\n",
        "\n",
        "            print(f\"Class {class_name}: {len(class_images)} images\")\n",
        "            image_paths.extend(class_images)\n",
        "            labels.extend([class_idx] * len(class_images))\n",
        "\n",
        "    print(f\"Total images: {len(image_paths)}\")\n",
        "\n",
        "    # Convert to arrays\n",
        "    image_paths = np.array(image_paths)\n",
        "    labels = tf.keras.utils.to_categorical(labels, num_classes=len(all_classes))\n",
        "\n",
        "    # Split dataset\n",
        "    total_samples = len(image_paths)\n",
        "    train_size = int(0.8 * total_samples)\n",
        "\n",
        "    # Shuffle\n",
        "    indices = np.random.permutation(total_samples)\n",
        "    train_indices = indices[:train_size]\n",
        "    val_indices = indices[train_size:]\n",
        "\n",
        "    # Create datasets\n",
        "    train_paths = image_paths[train_indices]\n",
        "    train_labels = labels[train_indices]\n",
        "    val_paths = image_paths[val_indices]\n",
        "    val_labels = labels[val_indices]\n",
        "\n",
        "    # Preprocessing function\n",
        "    def preprocess_image(image_path, label):\n",
        "        image = tf.io.read_file(image_path)\n",
        "        image = tf.image.decode_jpeg(image, channels=3)\n",
        "        image = tf.image.resize(image, [config.IMG_SIZE, config.IMG_SIZE])\n",
        "        image = tf.cast(image, tf.float32) / 255.0\n",
        "        return image, label\n",
        "\n",
        "    # Training augmentation\n",
        "    def train_augmentation(image, label):\n",
        "        image = tf.image.random_flip_left_right(image)\n",
        "        image = tf.image.random_brightness(image, 0.2)\n",
        "        image = tf.image.random_contrast(image, 0.8, 1.2)\n",
        "        image = tf.image.random_saturation(image, 0.8, 1.2)\n",
        "        image = tf.image.random_hue(image, 0.1)\n",
        "\n",
        "        # Random rotation\n",
        "        image = tf.image.rot90(image, k=tf.random.uniform([], 0, 4, dtype=tf.int32))\n",
        "\n",
        "        # Ensure values stay in valid range\n",
        "        image = tf.clip_by_value(image, 0.0, 1.0)\n",
        "        return image, label\n",
        "\n",
        "    # Create TensorFlow datasets\n",
        "    AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "    # Training dataset\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
        "    train_ds = train_ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "    train_ds = train_ds.map(train_augmentation, num_parallel_calls=AUTOTUNE)\n",
        "    train_ds = train_ds.batch(config.BATCH_SIZE)\n",
        "    train_ds = train_ds.prefetch(AUTOTUNE)\n",
        "\n",
        "    # Validation dataset\n",
        "    val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
        "    val_ds = val_ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "    val_ds = val_ds.batch(config.BATCH_SIZE)\n",
        "    val_ds = val_ds.prefetch(AUTOTUNE)\n",
        "\n",
        "    print(f\"âœ… Dataset created successfully!\")\n",
        "    print(f\"Training samples: {len(train_paths)}\")\n",
        "    print(f\"Validation samples: {len(val_paths)}\")\n",
        "\n",
        "    return train_ds, val_ds, all_classes\n",
        "\n",
        "# Create dataset\n",
        "train_dataset, val_dataset, class_names = create_colab_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiRsYY5G3COK",
        "outputId": "b007c70a-e3c9-4eb0-ef40-5b2ec86d0999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Creating optimized dataset pipeline...\n",
            "Number of classes: 101\n",
            "Class apple_pie: 1000 images\n",
            "Class baby_back_ribs: 1000 images\n",
            "Class baklava: 1000 images\n",
            "Class beef_carpaccio: 1000 images\n",
            "Class beef_tartare: 1000 images\n",
            "Class beet_salad: 1000 images\n",
            "Class beignets: 1000 images\n",
            "Class bibimbap: 1000 images\n",
            "Class bread_pudding: 1000 images\n",
            "Class breakfast_burrito: 1000 images\n",
            "Class bruschetta: 1000 images\n",
            "Class caesar_salad: 1000 images\n",
            "Class cannoli: 1000 images\n",
            "Class caprese_salad: 1000 images\n",
            "Class carrot_cake: 1000 images\n",
            "Class ceviche: 1000 images\n",
            "Class cheesecake: 1000 images\n",
            "Class cheese_plate: 1000 images\n",
            "Class chicken_curry: 1000 images\n",
            "Class chicken_quesadilla: 1000 images\n",
            "Class chicken_wings: 1000 images\n",
            "Class chocolate_cake: 1000 images\n",
            "Class chocolate_mousse: 1000 images\n",
            "Class churros: 1000 images\n",
            "Class clam_chowder: 1000 images\n",
            "Class club_sandwich: 1000 images\n",
            "Class crab_cakes: 1000 images\n",
            "Class creme_brulee: 1000 images\n",
            "Class croque_madame: 1000 images\n",
            "Class cup_cakes: 1000 images\n",
            "Class deviled_eggs: 1000 images\n",
            "Class donuts: 1000 images\n",
            "Class dumplings: 1000 images\n",
            "Class edamame: 1000 images\n",
            "Class eggs_benedict: 1000 images\n",
            "Class escargots: 1000 images\n",
            "Class falafel: 1000 images\n",
            "Class filet_mignon: 1000 images\n",
            "Class fish_and_chips: 1000 images\n",
            "Class foie_gras: 1000 images\n",
            "Class french_fries: 1000 images\n",
            "Class french_onion_soup: 1000 images\n",
            "Class french_toast: 1000 images\n",
            "Class fried_calamari: 1000 images\n",
            "Class fried_rice: 1000 images\n",
            "Class frozen_yogurt: 1000 images\n",
            "Class garlic_bread: 1000 images\n",
            "Class gnocchi: 1000 images\n",
            "Class greek_salad: 1000 images\n",
            "Class grilled_cheese_sandwich: 1000 images\n",
            "Class grilled_salmon: 1000 images\n",
            "Class guacamole: 1000 images\n",
            "Class gyoza: 1000 images\n",
            "Class hamburger: 1000 images\n",
            "Class hot_and_sour_soup: 1000 images\n",
            "Class hot_dog: 1000 images\n",
            "Class huevos_rancheros: 1000 images\n",
            "Class hummus: 1000 images\n",
            "Class ice_cream: 1000 images\n",
            "Class lasagna: 1000 images\n",
            "Class lobster_bisque: 1000 images\n",
            "Class lobster_roll_sandwich: 1000 images\n",
            "Class macaroni_and_cheese: 1000 images\n",
            "Class macarons: 1000 images\n",
            "Class miso_soup: 1000 images\n",
            "Class mussels: 1000 images\n",
            "Class nachos: 1000 images\n",
            "Class omelette: 1000 images\n",
            "Class onion_rings: 1000 images\n",
            "Class oysters: 1000 images\n",
            "Class pad_thai: 1000 images\n",
            "Class paella: 1000 images\n",
            "Class pancakes: 1000 images\n",
            "Class panna_cotta: 1000 images\n",
            "Class peking_duck: 1000 images\n",
            "Class pho: 1000 images\n",
            "Class pizza: 1000 images\n",
            "Class pork_chop: 1000 images\n",
            "Class poutine: 1000 images\n",
            "Class prime_rib: 1000 images\n",
            "Class pulled_pork_sandwich: 1000 images\n",
            "Class ramen: 1000 images\n",
            "Class ravioli: 1000 images\n",
            "Class red_velvet_cake: 1000 images\n",
            "Class risotto: 1000 images\n",
            "Class samosa: 1000 images\n",
            "Class sashimi: 1000 images\n",
            "Class scallops: 1000 images\n",
            "Class seaweed_salad: 1000 images\n",
            "Class shrimp_and_grits: 1000 images\n",
            "Class spaghetti_bolognese: 1000 images\n",
            "Class spaghetti_carbonara: 1000 images\n",
            "Class spring_rolls: 1000 images\n",
            "Class steak: 1000 images\n",
            "Class strawberry_shortcake: 1000 images\n",
            "Class sushi: 1000 images\n",
            "Class tacos: 1000 images\n",
            "Class takoyaki: 1000 images\n",
            "Class tiramisu: 1000 images\n",
            "Class tuna_tartare: 1000 images\n",
            "Class waffles: 1000 images\n",
            "Total images: 101000\n",
            "âœ… Dataset created successfully!\n",
            "Training samples: 80800\n",
            "Validation samples: 20200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Create optimized model for Colab\n",
        "def create_optimized_model():\n",
        "    \"\"\"\n",
        "    Create model optimized for Colab GPU training\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"ğŸ—ï¸ Building optimized model...\")\n",
        "\n",
        "    # Use EfficientNet for better accuracy (optional)\n",
        "    # base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    # Or use MobileNetV2 for faster training\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "\n",
        "    # Data augmentation layers (for consistency)\n",
        "    x = tf.keras.layers.RandomFlip(\"horizontal\")(inputs)\n",
        "    x = tf.keras.layers.RandomRotation(0.1)(x)\n",
        "    x = tf.keras.layers.RandomZoom(0.1)(x)\n",
        "\n",
        "    # Base model\n",
        "    x = base_model(x, training=False)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # Classification head\n",
        "    x = Dense(1024)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.activations.relu(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    x = Dense(512)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.activations.relu(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    # Output layer (float32 for mixed precision)\n",
        "    outputs = Dense(len(class_names), activation='softmax', dtype='float32')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    print(\"âœ… Model created!\")\n",
        "    print(f\"Total parameters: {model.count_params():,}\")\n",
        "\n",
        "    return model, base_model\n",
        "\n",
        "model, base_model = create_optimized_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu8b2L6k3HLR",
        "outputId": "391071d7-cc37-4a75-d910-0d2e57cb08dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ—ï¸ Building optimized model...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "âœ… Model created!\n",
            "Total parameters: 4,152,485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Simplified training pipeline\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "\n",
        "def simple_training_pipeline():\n",
        "    \"\"\"\n",
        "    Simplified training pipeline tanpa kompleksitas berlebihan\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"ğŸš€ Starting simplified training...\")\n",
        "\n",
        "    # Phase 1: Frozen base model\n",
        "    print(\"\\nğŸ“š PHASE 1: Training with frozen base model\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']  # Hapus top_5_accuracy jika menyebabkan masalah\n",
        "    )\n",
        "\n",
        "    # Simple callbacks\n",
        "    callbacks_phase1 = [\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=3,\n",
        "            verbose=1\n",
        "        ),\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=5,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Train Phase 1\n",
        "    print(\"Starting Phase 1 training...\")\n",
        "    history1 = model.fit(\n",
        "        train_dataset,\n",
        "        epochs=10,\n",
        "        validation_data=val_dataset,\n",
        "        callbacks=callbacks_phase1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Phase 2: Fine-tuning\n",
        "    print(\"\\nğŸ”§ PHASE 2: Fine-tuning\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Unfreeze base model\n",
        "    base_model.trainable = True\n",
        "\n",
        "    # Freeze early layers\n",
        "    for layer in base_model.layers[:-20]:  # Reduced number\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Recompile with lower LR\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.0001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Simple callbacks for Phase 2\n",
        "    callbacks_phase2 = [\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_accuracy',\n",
        "            factor=0.5,\n",
        "            patience=3,\n",
        "            mode='max',\n",
        "            verbose=1\n",
        "        ),\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=5,\n",
        "            mode='max',\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            'best_model.h5',\n",
        "            save_best_only=True,\n",
        "            monitor='val_accuracy',\n",
        "            mode='max',\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Train Phase 2\n",
        "    print(\"Starting Phase 2 training...\")\n",
        "    history2 = model.fit(\n",
        "        train_dataset,\n",
        "        epochs=15,\n",
        "        validation_data=val_dataset,\n",
        "        callbacks=callbacks_phase2,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Final evaluation\n",
        "    print(\"\\nğŸ“Š Final Results:\")\n",
        "    final_loss, final_acc = model.evaluate(val_dataset, verbose=0)\n",
        "    print(f\"Final Validation Accuracy: {final_acc:.4f} ({final_acc*100:.2f}%)\")\n",
        "\n",
        "    # Save model\n",
        "    model.save('food_classifier_final.h5')\n",
        "    print(\"âœ… Model saved as 'food_classifier_final.h5'\")\n",
        "\n",
        "    # Try TFLite conversion\n",
        "    try:\n",
        "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "        tflite_model = converter.convert()\n",
        "\n",
        "        with open('food_classifier.tflite', 'wb') as f:\n",
        "            f.write(tflite_model)\n",
        "        print(\"âœ… TFLite model saved as 'food_classifier.tflite'\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ TFLite conversion error: {e}\")\n",
        "\n",
        "    # Save labels\n",
        "    with open('labels.txt', 'w') as f:\n",
        "        for label in class_names:\n",
        "            f.write(label + '\\n')\n",
        "    print(\"âœ… Labels saved as 'labels.txt'\")\n",
        "\n",
        "    return model, history1, history2\n",
        "\n",
        "# Run simplified training\n",
        "model, hist1, hist2 = simple_training_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsJRWMI53Tqx",
        "outputId": "243f3e94-0c37-4bd3-b4f1-0dc7f76ea182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Starting simplified training...\n",
            "\n",
            "ğŸ“š PHASE 1: Training with frozen base model\n",
            "==================================================\n",
            "Starting Phase 1 training...\n",
            "Epoch 1/10\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 251ms/step - accuracy: 0.2115 - loss: 3.4440 - val_accuracy: 0.4546 - val_loss: 2.1432 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 246ms/step - accuracy: 0.3384 - loss: 2.6813 - val_accuracy: 0.4861 - val_loss: 2.0032 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 236ms/step - accuracy: 0.3657 - loss: 2.5521 - val_accuracy: 0.4903 - val_loss: 1.9763 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 233ms/step - accuracy: 0.3794 - loss: 2.4955 - val_accuracy: 0.5056 - val_loss: 1.9265 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 230ms/step - accuracy: 0.3922 - loss: 2.4378 - val_accuracy: 0.5130 - val_loss: 1.8933 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 229ms/step - accuracy: 0.3981 - loss: 2.4024 - val_accuracy: 0.5160 - val_loss: 1.8821 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 227ms/step - accuracy: 0.4043 - loss: 2.3752 - val_accuracy: 0.5224 - val_loss: 1.8663 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 229ms/step - accuracy: 0.4111 - loss: 2.3553 - val_accuracy: 0.5218 - val_loss: 1.8500 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 228ms/step - accuracy: 0.4158 - loss: 2.3231 - val_accuracy: 0.5269 - val_loss: 1.8349 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 220ms/step - accuracy: 0.4203 - loss: 2.2973 - val_accuracy: 0.5288 - val_loss: 1.8234 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "\n",
            "ğŸ”§ PHASE 2: Fine-tuning\n",
            "==================================================\n",
            "Starting Phase 2 training...\n",
            "Epoch 1/15\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.4027 - loss: 2.3888\n",
            "Epoch 1: val_accuracy improved from -inf to 0.53455, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 229ms/step - accuracy: 0.4027 - loss: 2.3887 - val_accuracy: 0.5346 - val_loss: 1.8022 - learning_rate: 1.0000e-04\n",
            "Epoch 2/15\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.4753 - loss: 2.0617\n",
            "Epoch 2: val_accuracy improved from 0.53455 to 0.57772, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 236ms/step - accuracy: 0.4753 - loss: 2.0617 - val_accuracy: 0.5777 - val_loss: 1.6286 - learning_rate: 1.0000e-04\n",
            "Epoch 3/15\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.5037 - loss: 1.9287\n",
            "Epoch 3: val_accuracy improved from 0.57772 to 0.58629, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 234ms/step - accuracy: 0.5037 - loss: 1.9287 - val_accuracy: 0.5863 - val_loss: 1.5970 - learning_rate: 1.0000e-04\n",
            "Epoch 4/15\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.5268 - loss: 1.8450\n",
            "Epoch 4: val_accuracy improved from 0.58629 to 0.59371, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 227ms/step - accuracy: 0.5268 - loss: 1.8449 - val_accuracy: 0.5937 - val_loss: 1.5541 - learning_rate: 1.0000e-04\n",
            "Epoch 5/15\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.5405 - loss: 1.7717\n",
            "Epoch 5: val_accuracy improved from 0.59371 to 0.60629, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 228ms/step - accuracy: 0.5405 - loss: 1.7717 - val_accuracy: 0.6063 - val_loss: 1.5131 - learning_rate: 1.0000e-04\n",
            "Epoch 6/15\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.5531 - loss: 1.7193\n",
            "Epoch 6: val_accuracy improved from 0.60629 to 0.61931, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 229ms/step - accuracy: 0.5531 - loss: 1.7193 - val_accuracy: 0.6193 - val_loss: 1.4650 - learning_rate: 1.0000e-04\n",
            "Epoch 7/15\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.5633 - loss: 1.6743\n",
            "Epoch 7: val_accuracy improved from 0.61931 to 0.62777, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 232ms/step - accuracy: 0.5633 - loss: 1.6743 - val_accuracy: 0.6278 - val_loss: 1.4313 - learning_rate: 1.0000e-04\n",
            "Epoch 8/15\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.5706 - loss: 1.6278\n",
            "Epoch 8: val_accuracy improved from 0.62777 to 0.63134, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 239ms/step - accuracy: 0.5706 - loss: 1.6278 - val_accuracy: 0.6313 - val_loss: 1.4072 - learning_rate: 1.0000e-04\n",
            "Epoch 9/15\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.5796 - loss: 1.5931\n",
            "Epoch 9: val_accuracy did not improve from 0.63134\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 231ms/step - accuracy: 0.5796 - loss: 1.5931 - val_accuracy: 0.6182 - val_loss: 1.4544 - learning_rate: 1.0000e-04\n",
            "Epoch 10/15\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.5914 - loss: 1.5502\n",
            "Epoch 10: val_accuracy improved from 0.63134 to 0.63337, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 231ms/step - accuracy: 0.5914 - loss: 1.5502 - val_accuracy: 0.6334 - val_loss: 1.3963 - learning_rate: 1.0000e-04\n",
            "Epoch 11/15\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.5975 - loss: 1.5227\n",
            "Epoch 11: val_accuracy improved from 0.63337 to 0.64926, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 238ms/step - accuracy: 0.5975 - loss: 1.5227 - val_accuracy: 0.6493 - val_loss: 1.3431 - learning_rate: 1.0000e-04\n",
            "Epoch 12/15\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.6045 - loss: 1.4891\n",
            "Epoch 12: val_accuracy did not improve from 0.64926\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 238ms/step - accuracy: 0.6045 - loss: 1.4891 - val_accuracy: 0.6416 - val_loss: 1.3601 - learning_rate: 1.0000e-04\n",
            "Epoch 13/15\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.6135 - loss: 1.4584\n",
            "Epoch 13: val_accuracy improved from 0.64926 to 0.65074, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 233ms/step - accuracy: 0.6135 - loss: 1.4584 - val_accuracy: 0.6507 - val_loss: 1.3294 - learning_rate: 1.0000e-04\n",
            "Epoch 14/15\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.6193 - loss: 1.4282\n",
            "Epoch 14: val_accuracy did not improve from 0.65074\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 237ms/step - accuracy: 0.6193 - loss: 1.4282 - val_accuracy: 0.6424 - val_loss: 1.3670 - learning_rate: 1.0000e-04\n",
            "Epoch 15/15\n",
            "\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.6265 - loss: 1.4115\n",
            "Epoch 15: val_accuracy improved from 0.65074 to 0.65089, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1263/1263\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 231ms/step - accuracy: 0.6265 - loss: 1.4115 - val_accuracy: 0.6509 - val_loss: 1.3310 - learning_rate: 1.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 15.\n",
            "\n",
            "ğŸ“Š Final Results:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Validation Accuracy: 0.6509 (65.09%)\n",
            "âœ… Model saved as 'food_classifier_final.h5'\n",
            "Saved artifact at '/tmp/tmp6isu5qvu'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_155')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 101), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  137187261267216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256713680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256714640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187261267408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187261267792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187261267600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256713488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256714448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256715792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256714256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256716368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256717904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256716560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256717712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256716752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256719632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256720976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256721360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256721168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256720016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256722512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256722128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256722320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256722704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256720592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256724816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256725200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256725584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256725392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256720208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256726736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256727120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256727504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256727312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256723664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256728656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256728272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256727888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256729232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256724432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916958480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916959824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916960208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916960016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916958672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916959248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916961936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916962320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916962128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916958288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916963472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916963856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916964240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916964048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916960976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916965392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916965776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916966160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916965968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916961552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916967312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916967696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916968080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916967888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916963088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916969232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916969616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916970000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916969808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916965008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916971152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916971536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916971920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916971728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916966928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916973072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916973456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916974224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916973648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916968848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916972304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917351504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917352656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916972688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917352464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917353808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917354192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917354576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917354384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917352272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917355728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917356112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917356496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917356304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917351696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917357648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917358032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917358416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917358224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917353424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917359568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917359952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917360336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917360144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917355344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917361488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917361872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917362256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917362064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917357264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917363408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917363792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917364176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917363984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917359184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917365328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917365712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917366096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917365904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917361104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917367248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917351888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917367632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917367440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917364944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917876176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917877520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917877904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917877712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917876368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917879056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917879440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917879824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917879632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917876944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917880976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917881360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917881744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917881552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917877136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917882896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917883280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917883664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917883472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917878672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917884816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917885200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917885584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917885392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917880592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917886736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917887120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917887504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917887312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917882512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917888656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917889040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917889424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917889232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917884432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917890576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917890960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917891728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917891152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917886352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917889808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918269968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918269008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917890192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918269200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918271312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918271696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918272080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918271888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918269392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918273232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918273616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918274000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918273808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918269776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918275152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918275536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918275920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918275728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918270928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918277072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918277456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918277840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918277648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918272848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918278992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918279376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918279760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918279568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918274768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918280912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918281296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918281680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918281488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918276688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918282832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918283216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918283600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918283408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918278608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918284752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918270160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918285136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918284944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918282448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918647184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918647568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918647952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918647760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918646032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918649104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918649488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918649872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918649680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918645840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918651024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918651408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918651792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918651600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918646608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918652944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918653328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918653712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918653520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918648720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918654864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918655248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918655632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918655440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918650640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918656784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918657168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918657552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918657360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918652560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918658704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918659088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918659472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918659280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918654480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918660624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918661008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918661776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918661200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918656400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918659856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251487760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251486800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918660240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251486992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251489104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251489488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251489872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251487952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251487184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251492176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251493520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251493712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251492944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251488336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251493328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251492752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251496208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251496400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251495632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251494480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251496016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251495440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251498896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "âš ï¸ TFLite conversion error: Could not translate MLIR to FlatBuffer.<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %171 = \"tf.Conv2D\"(%170, %9) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 2, 2, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x224x224x3xf16>, tensor<3x3x3x32xf16>) -> tensor<?x112x112x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %176 = \"tf.Relu6\"(%175) {device = \"\"} : (tensor<?x112x112x32xf16>) -> tensor<?x112x112x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %177 = \"tf.DepthwiseConv2dNative\"(%176, %5) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x112x112x32xf16>, tensor<3x3x32x1xf16>) -> tensor<?x112x112x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %182 = \"tf.Relu6\"(%181) {device = \"\"} : (tensor<?x112x112x32xf16>) -> tensor<?x112x112x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %183 = \"tf.Conv2D\"(%182, %2) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x112x112x32xf16>, tensor<1x1x32x16xf16>) -> tensor<?x112x112x16xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %188 = \"tf.Conv2D\"(%187, %89) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x112x112x16xf16>, tensor<1x1x16x96xf16>) -> tensor<?x112x112x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %193 = \"tf.Relu6\"(%192) {device = \"\"} : (tensor<?x112x112x96xf16>) -> tensor<?x112x112x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_pad_1/Pad@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Pad' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_pad_1/Pad@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %194 = \"tf.Pad\"(%193, %162) {device = \"\"} : (tensor<?x112x112x96xf16>, tensor<4x2xi32>) -> tensor<?x113x113x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_pad_1/Pad@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %195 = \"tf.DepthwiseConv2dNative\"(%194, %92) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x113x113x96xf16>, tensor<3x3x96x1xf16>) -> tensor<?x56x56x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %200 = \"tf.Relu6\"(%199) {device = \"\"} : (tensor<?x56x56x96xf16>) -> tensor<?x56x56x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %201 = \"tf.Conv2D\"(%200, %86) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x96xf16>, tensor<1x1x96x24xf16>) -> tensor<?x56x56x24xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %206 = \"tf.Conv2D\"(%205, %80) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x24xf16>, tensor<1x1x24x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %211 = \"tf.Relu6\"(%210) {device = \"\"} : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %212 = \"tf.DepthwiseConv2dNative\"(%211, %83) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x56x56x144xf16>, tensor<3x3x144x1xf16>) -> tensor<?x56x56x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %217 = \"tf.Relu6\"(%216) {device = \"\"} : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %218 = \"tf.Conv2D\"(%217, %77) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x144xf16>, tensor<1x1x144x24xf16>) -> tensor<?x56x56x24xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %223 = \"tf.AddV2\"(%205, %222) {device = \"\"} : (tensor<?x56x56x24xf16>, tensor<?x56x56x24xf16>) -> tensor<?x56x56x24xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %224 = \"tf.Conv2D\"(%223, %71) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x24xf16>, tensor<1x1x24x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %229 = \"tf.Relu6\"(%228) {device = \"\"} : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_pad_1/Pad@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Pad' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_pad_1/Pad@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %230 = \"tf.Pad\"(%229, %162) {device = \"\"} : (tensor<?x56x56x144xf16>, tensor<4x2xi32>) -> tensor<?x57x57x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_pad_1/Pad@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %231 = \"tf.DepthwiseConv2dNative\"(%230, %74) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x57x57x144xf16>, tensor<3x3x144x1xf16>) -> tensor<?x28x28x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %236 = \"tf.Relu6\"(%235) {device = \"\"} : (tensor<?x28x28x144xf16>) -> tensor<?x28x28x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %237 = \"tf.Conv2D\"(%236, %68) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x144xf16>, tensor<1x1x144x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %242 = \"tf.Conv2D\"(%241, %62) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %247 = \"tf.Relu6\"(%246) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %248 = \"tf.DepthwiseConv2dNative\"(%247, %65) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %253 = \"tf.Relu6\"(%252) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %254 = \"tf.Conv2D\"(%253, %59) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<1x1x192x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %259 = \"tf.AddV2\"(%241, %258) {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %260 = \"tf.Conv2D\"(%259, %53) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %265 = \"tf.Relu6\"(%264) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %266 = \"tf.DepthwiseConv2dNative\"(%265, %56) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %271 = \"tf.Relu6\"(%270) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %272 = \"tf.Conv2D\"(%271, %50) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<1x1x192x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %277 = \"tf.AddV2\"(%259, %276) {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %278 = \"tf.Conv2D\"(%277, %44) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %283 = \"tf.Relu6\"(%282) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_pad_1/Pad@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Pad' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_pad_1/Pad@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %284 = \"tf.Pad\"(%283, %162) {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<4x2xi32>) -> tensor<?x29x29x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_pad_1/Pad@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %285 = \"tf.DepthwiseConv2dNative\"(%284, %47) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x29x29x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x14x14x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %290 = \"tf.Relu6\"(%289) {device = \"\"} : (tensor<?x14x14x192xf16>) -> tensor<?x14x14x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %291 = \"tf.Conv2D\"(%290, %41) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x192xf16>, tensor<1x1x192x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %296 = \"tf.Conv2D\"(%295, %35) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %301 = \"tf.Relu6\"(%300) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %302 = \"tf.DepthwiseConv2dNative\"(%301, %38) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %307 = \"tf.Relu6\"(%306) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %308 = \"tf.Conv2D\"(%307, %32) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %313 = \"tf.AddV2\"(%295, %312) {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %314 = \"tf.Conv2D\"(%313, %26) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %319 = \"tf.Relu6\"(%318) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %320 = \"tf.DepthwiseConv2dNative\"(%319, %29) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %325 = \"tf.Relu6\"(%324) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %326 = \"tf.Conv2D\"(%325, %23) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %331 = \"tf.AddV2\"(%313, %330) {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %332 = \"tf.Conv2D\"(%331, %17) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %337 = \"tf.Relu6\"(%336) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %338 = \"tf.DepthwiseConv2dNative\"(%337, %20) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %343 = \"tf.Relu6\"(%342) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %344 = \"tf.Conv2D\"(%343, %14) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %349 = \"tf.AddV2\"(%331, %348) {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %350 = \"tf.Conv2D\"(%349, %152) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %355 = \"tf.Relu6\"(%354) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %356 = \"tf.DepthwiseConv2dNative\"(%355, %155) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %361 = \"tf.Relu6\"(%360) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %362 = \"tf.Conv2D\"(%361, %149) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %367 = \"tf.Conv2D\"(%366, %143) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %372 = \"tf.Relu6\"(%371) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %373 = \"tf.DepthwiseConv2dNative\"(%372, %146) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %378 = \"tf.Relu6\"(%377) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %379 = \"tf.Conv2D\"(%378, %140) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<1x1x576x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %384 = \"tf.AddV2\"(%366, %383) {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %385 = \"tf.Conv2D\"(%384, %134) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %390 = \"tf.Relu6\"(%389) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %391 = \"tf.DepthwiseConv2dNative\"(%390, %137) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %396 = \"tf.Relu6\"(%395) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %397 = \"tf.Conv2D\"(%396, %131) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<1x1x576x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %402 = \"tf.AddV2\"(%384, %401) {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %403 = \"tf.Conv2D\"(%402, %125) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %408 = \"tf.Relu6\"(%407) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_pad_1/Pad@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Pad' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_pad_1/Pad@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %409 = \"tf.Pad\"(%408, %162) {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<4x2xi32>) -> tensor<?x15x15x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_pad_1/Pad@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %410 = \"tf.DepthwiseConv2dNative\"(%409, %128) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x15x15x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x7x7x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %415 = \"tf.Relu6\"(%414) {device = \"\"} : (tensor<?x7x7x576xf16>) -> tensor<?x7x7x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %416 = \"tf.Conv2D\"(%415, %122) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x576xf16>, tensor<1x1x576x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %421 = \"tf.Conv2D\"(%420, %116) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %426 = \"tf.Relu6\"(%425) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %427 = \"tf.DepthwiseConv2dNative\"(%426, %119) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %432 = \"tf.Relu6\"(%431) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %433 = \"tf.Conv2D\"(%432, %113) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<1x1x960x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %438 = \"tf.AddV2\"(%420, %437) {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %439 = \"tf.Conv2D\"(%438, %107) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %444 = \"tf.Relu6\"(%443) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %445 = \"tf.DepthwiseConv2dNative\"(%444, %110) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %450 = \"tf.Relu6\"(%449) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %451 = \"tf.Conv2D\"(%450, %104) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<1x1x960x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %456 = \"tf.AddV2\"(%438, %455) {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_add_1/Add@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %457 = \"tf.Conv2D\"(%456, %98) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %462 = \"tf.Relu6\"(%461) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %463 = \"tf.DepthwiseConv2dNative\"(%462, %101) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_1/depthwise@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %468 = \"tf.Relu6\"(%467) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %469 = \"tf.Conv2D\"(%468, %95) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<1x1x960x320xf16>) -> tensor<?x7x7x320xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_project_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv_1_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv_1_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %474 = \"tf.Conv2D\"(%473, %8) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x320xf16>, tensor<1x1x320x1280xf16>) -> tensor<?x7x7x1280xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv_1_1/convolution@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/out_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/out_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %479 = \"tf.Relu6\"(%478) {device = \"\"} : (tensor<?x7x7x1280xf16>) -> tensor<?x7x7x1280xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/out_relu_1/Relu6@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1/MatMul@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.MatMul' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1/MatMul@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %483 = \"tf.MatMul\"(%482, %165) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x1280xf16>, tensor<1024x1280xf16>) -> tensor<?x1024xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1/MatMul@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1/BiasAdd@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.BiasAdd' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1/BiasAdd@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %484 = \"tf.BiasAdd\"(%483, %156) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x1024xf16>, tensor<1024xf16>) -> tensor<?x1024xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1/BiasAdd@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/Relu@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/Relu@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %489 = \"tf.Relu\"(%488) {device = \"\"} : (tensor<?x1024xf16>) -> tensor<?x1024xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/Relu@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1_2/MatMul@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.MatMul' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1_2/MatMul@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %490 = \"tf.MatMul\"(%489, %166) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x1024xf16>, tensor<512x1024xf16>) -> tensor<?x512xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1_2/MatMul@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1_2/BiasAdd@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.BiasAdd' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1_2/BiasAdd@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %491 = \"tf.BiasAdd\"(%490, %157) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x512xf16>, tensor<512xf16>) -> tensor<?x512xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1_2/BiasAdd@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/Relu_1@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/Relu_1@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %496 = \"tf.Relu\"(%495) {device = \"\"} : (tensor<?x512xf16>) -> tensor<?x512xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/Relu_1@__inference_function_598412\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_599519\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: failed while converting: 'main': \n",
            "Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \n",
            "TF Select ops: AddV2, BiasAdd, Conv2D, DepthwiseConv2dNative, MatMul, Pad, Relu, Relu6\n",
            "Details:\n",
            "\ttf.AddV2(tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> (tensor<?x14x14x64xf16>) : {device = \"\"}\n",
            "\ttf.AddV2(tensor<?x14x14x96xf16>, tensor<?x14x14x96xf16>) -> (tensor<?x14x14x96xf16>) : {device = \"\"}\n",
            "\ttf.AddV2(tensor<?x28x28x32xf16>, tensor<?x28x28x32xf16>) -> (tensor<?x28x28x32xf16>) : {device = \"\"}\n",
            "\ttf.AddV2(tensor<?x56x56x24xf16>, tensor<?x56x56x24xf16>) -> (tensor<?x56x56x24xf16>) : {device = \"\"}\n",
            "\ttf.AddV2(tensor<?x7x7x160xf16>, tensor<?x7x7x160xf16>) -> (tensor<?x7x7x160xf16>) : {device = \"\"}\n",
            "\ttf.BiasAdd(tensor<?x1024xf16>, tensor<1024xf16>) -> (tensor<?x1024xf16>) : {data_format = \"NHWC\", device = \"\"}\n",
            "\ttf.BiasAdd(tensor<?x512xf16>, tensor<512xf16>) -> (tensor<?x512xf16>) : {data_format = \"NHWC\", device = \"\"}\n",
            "\ttf.Conv2D(tensor<?x112x112x16xf16>, tensor<1x1x16x96xf16>) -> (tensor<?x112x112x96xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x112x112x32xf16>, tensor<1x1x32x16xf16>) -> (tensor<?x112x112x16xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x14x14x192xf16>, tensor<1x1x192x64xf16>) -> (tensor<?x14x14x64xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> (tensor<?x14x14x64xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x14x14x384xf16>, tensor<1x1x384x96xf16>) -> (tensor<?x14x14x96xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x14x14x576xf16>, tensor<1x1x576x96xf16>) -> (tensor<?x14x14x96xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> (tensor<?x14x14x384xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> (tensor<?x14x14x576xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x224x224x3xf16>, tensor<3x3x3x32xf16>) -> (tensor<?x112x112x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 2, 2, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x28x28x144xf16>, tensor<1x1x144x32xf16>) -> (tensor<?x28x28x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x28x28x192xf16>, tensor<1x1x192x32xf16>) -> (tensor<?x28x28x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> (tensor<?x28x28x192xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x56x56x144xf16>, tensor<1x1x144x24xf16>) -> (tensor<?x56x56x24xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x56x56x24xf16>, tensor<1x1x24x144xf16>) -> (tensor<?x56x56x144xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x56x56x96xf16>, tensor<1x1x96x24xf16>) -> (tensor<?x56x56x24xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> (tensor<?x7x7x960xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x7x7x320xf16>, tensor<1x1x320x1280xf16>) -> (tensor<?x7x7x1280xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x7x7x576xf16>, tensor<1x1x576x160xf16>) -> (tensor<?x7x7x160xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x7x7x960xf16>, tensor<1x1x960x160xf16>) -> (tensor<?x7x7x160xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x7x7x960xf16>, tensor<1x1x960x320xf16>) -> (tensor<?x7x7x320xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x112x112x32xf16>, tensor<3x3x32x1xf16>) -> (tensor<?x112x112x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x113x113x96xf16>, tensor<3x3x96x1xf16>) -> (tensor<?x56x56x96xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> (tensor<?x14x14x384xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x14x14x576xf16>, tensor<3x3x576x1xf16>) -> (tensor<?x14x14x576xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x15x15x576xf16>, tensor<3x3x576x1xf16>) -> (tensor<?x7x7x576xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x28x28x192xf16>, tensor<3x3x192x1xf16>) -> (tensor<?x28x28x192xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x29x29x192xf16>, tensor<3x3x192x1xf16>) -> (tensor<?x14x14x192xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x56x56x144xf16>, tensor<3x3x144x1xf16>) -> (tensor<?x56x56x144xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x57x57x144xf16>, tensor<3x3x144x1xf16>) -> (tensor<?x28x28x144xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> (tensor<?x7x7x960xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n",
            "\ttf.MatMul(tensor<?x1024xf16>, tensor<512x1024xf16>) -> (tensor<?x512xf16>) : {grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}\n",
            "\ttf.MatMul(tensor<?x1280xf16>, tensor<1024x1280xf16>) -> (tensor<?x1024xf16>) : {grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}\n",
            "\ttf.Pad(tensor<?x112x112x96xf16>, tensor<4x2xi32>) -> (tensor<?x113x113x96xf16>) : {device = \"\"}\n",
            "\ttf.Pad(tensor<?x14x14x576xf16>, tensor<4x2xi32>) -> (tensor<?x15x15x576xf16>) : {device = \"\"}\n",
            "\ttf.Pad(tensor<?x28x28x192xf16>, tensor<4x2xi32>) -> (tensor<?x29x29x192xf16>) : {device = \"\"}\n",
            "\ttf.Pad(tensor<?x56x56x144xf16>, tensor<4x2xi32>) -> (tensor<?x57x57x144xf16>) : {device = \"\"}\n",
            "\ttf.Relu(tensor<?x1024xf16>) -> (tensor<?x1024xf16>) : {device = \"\"}\n",
            "\ttf.Relu(tensor<?x512xf16>) -> (tensor<?x512xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x112x112x32xf16>) -> (tensor<?x112x112x32xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x112x112x96xf16>) -> (tensor<?x112x112x96xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x14x14x192xf16>) -> (tensor<?x14x14x192xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x14x14x384xf16>) -> (tensor<?x14x14x384xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x14x14x576xf16>) -> (tensor<?x14x14x576xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x28x28x144xf16>) -> (tensor<?x28x28x144xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x28x28x192xf16>) -> (tensor<?x28x28x192xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x56x56x144xf16>) -> (tensor<?x56x56x144xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x56x56x96xf16>) -> (tensor<?x56x56x96xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x7x7x1280xf16>) -> (tensor<?x7x7x1280xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x7x7x576xf16>) -> (tensor<?x7x7x576xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x7x7x960xf16>) -> (tensor<?x7x7x960xf16>) : {device = \"\"}\n",
            "\n",
            "<unknown>:0: note: see current operation: \n",
            "\"func.func\"() <{arg_attrs = [{tf_saved_model.index_path = [\"keras_tensor_155\"]}], function_type = (tensor<?x224x224x3xf32>) -> tensor<?x101xf32>, res_attrs = [{tf_saved_model.index_path = [\"output_0\"]}], sym_name = \"main\"}> ({\n",
            "^bb0(%arg0: tensor<?x224x224x3xf32>):\n",
            "  %0 = \"arith.constant\"() <{value = dense<[-1.79617035, 3.17504382, 16.2980614, 24.0359573, 13.8513613, 3.49967337, -0.0168914795, 7.31451178, -13.8333397, 3.81765318, -2.79449153, 18.9408112, -18.930912, -11.2135153, 7.71239185, -2.86968422]> : tensor<16xf32>}> : () -> tensor<16xf32>\n",
            "  %1 = \"arith.constant\"() <{value = dense<[4.83980179, 7.01497889, 6.59778308, 7.4643445, 3.82265639, 6.78915548, 5.1570487, 4.63491488, 4.46355867, 6.7842741, 7.31219244, 4.33215332, 6.80388832, 6.5691595, 5.91440105, 4.81517363]> : tensor<16xf32>}> : () -> tensor<16xf32>\n",
            "  %2 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x32x16xf16>}> : () -> tensor<1x1x32x16xf16>\n",
            "  %3 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %4 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %5 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x32x1xf16>}> : () -> tensor<3x3x32x1xf16>\n",
            "  %6 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1280xf32>}> : () -> tensor<1280xf32>\n",
            "  %7 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1280xf32>}> : () -> tensor<1280xf32>\n",
            "  %8 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x320x1280xf16>}> : () -> tensor<1x1x320x1280xf16>\n",
            "  %9 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x3x32xf16>}> : () -> tensor<3x3x3x32xf16>\n",
            "  %10 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %11 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %12 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %13 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %14 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x384x64xf16>}> : () -> tensor<1x1x384x64xf16>\n",
            "  %15 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %16 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %17 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x64x384xf16>}> : () -> tensor<1x1x64x384xf16>\n",
            "  %18 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %19 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %20 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x384x1xf16>}> : () -> tensor<3x3x384x1xf16>\n",
            "  %21 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %22 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %23 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x384x64xf16>}> : () -> tensor<1x1x384x64xf16>\n",
            "  %24 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %25 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %26 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x64x384xf16>}> : () -> tensor<1x1x64x384xf16>\n",
            "  %27 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %28 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %29 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x384x1xf16>}> : () -> tensor<3x3x384x1xf16>\n",
            "  %30 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %31 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %32 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x384x64xf16>}> : () -> tensor<1x1x384x64xf16>\n",
            "  %33 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %34 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %35 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x64x384xf16>}> : () -> tensor<1x1x64x384xf16>\n",
            "  %36 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %37 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %38 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x384x1xf16>}> : () -> tensor<3x3x384x1xf16>\n",
            "  %39 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %40 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %41 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x192x64xf16>}> : () -> tensor<1x1x192x64xf16>\n",
            "  %42 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %43 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %44 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x32x192xf16>}> : () -> tensor<1x1x32x192xf16>\n",
            "  %45 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %46 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %47 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x192x1xf16>}> : () -> tensor<3x3x192x1xf16>\n",
            "  %48 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %49 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %50 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x192x32xf16>}> : () -> tensor<1x1x192x32xf16>\n",
            "  %51 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %52 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %53 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x32x192xf16>}> : () -> tensor<1x1x32x192xf16>\n",
            "  %54 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %55 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %56 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x192x1xf16>}> : () -> tensor<3x3x192x1xf16>\n",
            "  %57 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %58 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %59 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x192x32xf16>}> : () -> tensor<1x1x192x32xf16>\n",
            "  %60 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %61 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %62 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x32x192xf16>}> : () -> tensor<1x1x32x192xf16>\n",
            "  %63 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %64 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %65 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x192x1xf16>}> : () -> tensor<3x3x192x1xf16>\n",
            "  %66 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %67 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %68 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x144x32xf16>}> : () -> tensor<1x1x144x32xf16>\n",
            "  %69 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %70 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %71 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x24x144xf16>}> : () -> tensor<1x1x24x144xf16>\n",
            "  %72 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %73 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %74 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x144x1xf16>}> : () -> tensor<3x3x144x1xf16>\n",
            "  %75 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<24xf32>}> : () -> tensor<24xf32>\n",
            "  %76 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<24xf32>}> : () -> tensor<24xf32>\n",
            "  %77 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x144x24xf16>}> : () -> tensor<1x1x144x24xf16>\n",
            "  %78 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %79 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %80 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x24x144xf16>}> : () -> tensor<1x1x24x144xf16>\n",
            "  %81 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %82 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %83 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x144x1xf16>}> : () -> tensor<3x3x144x1xf16>\n",
            "  %84 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<24xf32>}> : () -> tensor<24xf32>\n",
            "  %85 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<24xf32>}> : () -> tensor<24xf32>\n",
            "  %86 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x96x24xf16>}> : () -> tensor<1x1x96x24xf16>\n",
            "  %87 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %88 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %89 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x16x96xf16>}> : () -> tensor<1x1x16x96xf16>\n",
            "  %90 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %91 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %92 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x96x1xf16>}> : () -> tensor<3x3x96x1xf16>\n",
            "  %93 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<320xf32>}> : () -> tensor<320xf32>\n",
            "  %94 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<320xf32>}> : () -> tensor<320xf32>\n",
            "  %95 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x960x320xf16>}> : () -> tensor<1x1x960x320xf16>\n",
            "  %96 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %97 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %98 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x160x960xf16>}> : () -> tensor<1x1x160x960xf16>\n",
            "  %99 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %100 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %101 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x960x1xf16>}> : () -> tensor<3x3x960x1xf16>\n",
            "  %102 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n",
            "  %103 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n",
            "  %104 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x960x160xf16>}> : () -> tensor<1x1x960x160xf16>\n",
            "  %105 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %106 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %107 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x160x960xf16>}> : () -> tensor<1x1x160x960xf16>\n",
            "  %108 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %109 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %110 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x960x1xf16>}> : () -> tensor<3x3x960x1xf16>\n",
            "  %111 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n",
            "  %112 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n",
            "  %113 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x960x160xf16>}> : () -> tensor<1x1x960x160xf16>\n",
            "  %114 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %115 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %116 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x160x960xf16>}> : () -> tensor<1x1x160x960xf16>\n",
            "  %117 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %118 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %119 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x960x1xf16>}> : () -> tensor<3x3x960x1xf16>\n",
            "  %120 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n",
            "  %121 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n",
            "  %122 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x576x160xf16>}> : () -> tensor<1x1x576x160xf16>\n",
            "  %123 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %124 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %125 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x96x576xf16>}> : () -> tensor<1x1x96x576xf16>\n",
            "  %126 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %127 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %128 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x576x1xf16>}> : () -> tensor<3x3x576x1xf16>\n",
            "  %129 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %130 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %131 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x576x96xf16>}> : () -> tensor<1x1x576x96xf16>\n",
            "  %132 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %133 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %134 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x96x576xf16>}> : () -> tensor<1x1x96x576xf16>\n",
            "  %135 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %136 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %137 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x576x1xf16>}> : () -> tensor<3x3x576x1xf16>\n",
            "  %138 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %139 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %140 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x576x96xf16>}> : () -> tensor<1x1x576x96xf16>\n",
            "  %141 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %142 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %143 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x96x576xf16>}> : () -> tensor<1x1x96x576xf16>\n",
            "  %144 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %145 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %146 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x576x1xf16>}> : () -> tensor<3x3x576x1xf16>\n",
            "  %147 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %148 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %149 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x384x96xf16>}> : () -> tensor<1x1x384x96xf16>\n",
            "  %150 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %151 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %152 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x64x384xf16>}> : () -> tensor<1x1x64x384xf16>\n",
            "  %153 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %154 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %155 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x384x1xf16>}> : () -> tensor<3x3x384x1xf16>\n",
            "  %156 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1024xf16>}> : () -> tensor<1024xf16>\n",
            "  %157 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<512xf16>}> : () -> tensor<512xf16>\n",
            "  %158 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1024xf32>}> : () -> tensor<1024xf32>\n",
            "  %159 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1024xf32>}> : () -> tensor<1024xf32>\n",
            "  %160 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<512xf32>}> : () -> tensor<512xf32>\n",
            "  %161 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<512xf32>}> : () -> tensor<512xf32>\n",
            "  %162 = \"arith.constant\"() <{value = dense<[[0, 0], [0, 1], [0, 1], [0, 0]]> : tensor<4x2xi32>}> : () -> tensor<4x2xi32>\n",
            "  %163 = \"arith.constant\"() <{value = dense<[1, 2]> : tensor<2xi32>}> : () -> tensor<2xi32>\n",
            "  %164 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<101xf32>}> : () -> tensor<101xf32>\n",
            "  %165 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1024x1280xf16>}> : () -> tensor<1024x1280xf16>\n",
            "  %166 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<512x1024xf16>}> : () -> tensor<512x1024xf16>\n",
            "  %167 = \"tfl.pseudo_qconst\"() <{qtype = tensor<101x512x!quant.uniform<i8<-127:127>:f32:0, {0.0035196880655964527,0.003034102869784738,0.0029790898946326547,0.0034422384003015955,0.00286281789381673,0.0030478196819936198,0.003764879515790564,0.003543796032432496,0.0031637364015804502,0.0027744364081405278,0.0024476149889427847,0.0031610408167200765,0.0029627478967501424,0.0039326790280229465,0.0031387458636066108,0.0034845112346288728,0.0030931189773589609,0.0032800393780385417,0.0036839767703859823,0.003220272815133643,0.0034213941397629385,0.0032939842836124692,0.003287345640302643,0.0029239199292941357,0.0033778398055729904,0.003882211728358832,0.0025053380981205015,0.0045974465805714527,0.0032684007498223013,0.0044137145590594437,0.0032357212126724365,0.0038919059310372419,0.0032753409363153411,0.0042975835912809597,0.0033498709126720278,0.0029546031801719365,0.0029587001781763993,0.0033694843607624684,0.0028530413240898311,0.0028612221789172314,0.0038846372150060701,0.0040789673647542634,0.0028956363520284338,0.0032025132122940904,0.0031092629188627708,0.0038356220159004991,0.0038850044640969103,0.0034358346556115338,0.0038642432745986098,0.0031052731622861125,0.003401375661684772,0.003270303874503909,0.0036903842696993368,0.0035367948333109456,0.0042598463418915518,0.0028427222112971029,0.0032642432085172399,0.0032351033424767922,0.0039963919346726788,0.0027833245401307355,0.0042847981603126827,0.0034589420153400092,0.0034374484396356296,0.003624631898609672,0.0037511497970641127,0.0031140409116669904,0.0033612079977050542,0.0022823880976579319,0.0045012498465109999,0.0043973964969004232,0.0035856248825553833,0.0037857871355972891,0.0026739720753797395,0.0034905364663582149,0.0034097167919939896,0.0039276145105286847,0.0034584543836398388,0.0035291572255412426,0.0031558035865543394,0.0032488952471515327,0.0030280698941448541,0.003758564474075798,0.0027166464197354054,0.0039795975046833668,0.0031718977792995181,0.0025550577584214098,0.0030911316083172173,0.0027391469384741595,0.0036072670005437898,0.0029970374633008102,0.004290158823719175,0.0056339633746409976,0.0033783023282298891,0.0030751214252682183,0.0038292652040015996,0.0030709946249413678,0.0032122611530183806,0.0037165678392245076,0.0034231632713257799,0.0027992281857437975,0.0032070337787387876}>>, value = dense_resource<__elided__> : tensor<101x512xi8>}> : () -> tensor<101x512x!quant.uniform<i8<-127:127>:f32:0, {0.0035196880655964527,0.003034102869784738,0.0029790898946326547,0.0034422384003015955,0.00286281789381673,0.0030478196819936198,0.003764879515790564,0.003543796032432496,0.0031637364015804502,0.0027744364081405278,0.0024476149889427847,0.0031610408167200765,0.0029627478967501424,0.0039326790280229465,0.0031387458636066108,0.0034845112346288728,0.0030931189773589609,0.0032800393780385417,0.0036839767703859823,0.003220272815133643,0.0034213941397629385,0.0032939842836124692,0.003287345640302643,0.0029239199292941357,0.0033778398055729904,0.003882211728358832,0.0025053380981205015,0.0045974465805714527,0.0032684007498223013,0.0044137145590594437,0.0032357212126724365,0.0038919059310372419,0.0032753409363153411,0.0042975835912809597,0.0033498709126720278,0.0029546031801719365,0.0029587001781763993,0.0033694843607624684,0.0028530413240898311,0.0028612221789172314,0.0038846372150060701,0.0040789673647542634,0.0028956363520284338,0.0032025132122940904,0.0031092629188627708,0.0038356220159004991,0.0038850044640969103,0.0034358346556115338,0.0038642432745986098,0.0031052731622861125,0.003401375661684772,0.003270303874503909,0.0036903842696993368,0.0035367948333109456,0.0042598463418915518,0.0028427222112971029,0.0032642432085172399,0.0032351033424767922,0.0039963919346726788,0.0027833245401307355,0.0042847981603126827,0.0034589420153400092,0.0034374484396356296,0.003624631898609672,0.0037511497970641127,0.0031140409116669904,0.0033612079977050542,0.0022823880976579319,0.0045012498465109999,0.0043973964969004232,0.0035856248825553833,0.0037857871355972891,0.0026739720753797395,0.0034905364663582149,0.0034097167919939896,0.0039276145105286847,0.0034584543836398388,0.0035291572255412426,0.0031558035865543394,0.0032488952471515327,0.0030280698941448541,0.003758564474075798,0.0027166464197354054,0.0039795975046833668,0.0031718977792995181,0.0025550577584214098,0.0030911316083172173,0.0027391469384741595,0.0036072670005437898,0.0029970374633008102,0.004290158823719175,0.0056339633746409976,0.0033783023282298891,0.0030751214252682183,0.0038292652040015996,0.0030709946249413678,0.0032122611530183806,0.0037165678392245076,0.0034231632713257799,0.0027992281857437975,0.0032070337787387876}>>\n",
            "  %168 = \"tfl.cast\"(%arg0) : (tensor<?x224x224x3xf32>) -> tensor<?x224x224x3xf16>\n",
            "  %169 = \"tfl.cast\"(%168) : (tensor<?x224x224x3xf16>) -> tensor<?x224x224x3xf32>\n",
            "  %170 = \"tfl.cast\"(%169) : (tensor<?x224x224x3xf32>) -> tensor<?x224x224x3xf16>\n",
            "  %171 = \"tf.Conv2D\"(%170, %9) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 2, 2, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x224x224x3xf16>, tensor<3x3x3x32xf16>) -> tensor<?x112x112x32xf16>\n",
            "  %172 = \"tfl.cast\"(%171) : (tensor<?x112x112x32xf16>) -> tensor<?x112x112x32xf32>\n",
            "  %173 = \"tfl.mul\"(%172, %11) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x32xf32>, tensor<32xf32>) -> tensor<?x112x112x32xf32>\n",
            "  %174 = \"tfl.add\"(%173, %10) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x32xf32>, tensor<32xf32>) -> tensor<?x112x112x32xf32>\n",
            "  %175 = \"tfl.cast\"(%174) : (tensor<?x112x112x32xf32>) -> tensor<?x112x112x32xf16>\n",
            "  %176 = \"tf.Relu6\"(%175) {device = \"\"} : (tensor<?x112x112x32xf16>) -> tensor<?x112x112x32xf16>\n",
            "  %177 = \"tf.DepthwiseConv2dNative\"(%176, %5) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x112x112x32xf16>, tensor<3x3x32x1xf16>) -> tensor<?x112x112x32xf16>\n",
            "  %178 = \"tfl.cast\"(%177) : (tensor<?x112x112x32xf16>) -> tensor<?x112x112x32xf32>\n",
            "  %179 = \"tfl.mul\"(%178, %4) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x32xf32>, tensor<32xf32>) -> tensor<?x112x112x32xf32>\n",
            "  %180 = \"tfl.add\"(%179, %3) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x32xf32>, tensor<32xf32>) -> tensor<?x112x112x32xf32>\n",
            "  %181 = \"tfl.cast\"(%180) : (tensor<?x112x112x32xf32>) -> tensor<?x112x112x32xf16>\n",
            "  %182 = \"tf.Relu6\"(%181) {device = \"\"} : (tensor<?x112x112x32xf16>) -> tensor<?x112x112x32xf16>\n",
            "  %183 = \"tf.Conv2D\"(%182, %2) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x112x112x32xf16>, tensor<1x1x32x16xf16>) -> tensor<?x112x112x16xf16>\n",
            "  %184 = \"tfl.cast\"(%183) : (tensor<?x112x112x16xf16>) -> tensor<?x112x112x16xf32>\n",
            "  %185 = \"tfl.mul\"(%184, %1) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x16xf32>, tensor<16xf32>) -> tensor<?x112x112x16xf32>\n",
            "  %186 = \"tfl.add\"(%185, %0) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x16xf32>, tensor<16xf32>) -> tensor<?x112x112x16xf32>\n",
            "  %187 = \"tfl.cast\"(%186) : (tensor<?x112x112x16xf32>) -> tensor<?x112x112x16xf16>\n",
            "  %188 = \"tf.Conv2D\"(%187, %89) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x112x112x16xf16>, tensor<1x1x16x96xf16>) -> tensor<?x112x112x96xf16>\n",
            "  %189 = \"tfl.cast\"(%188) : (tensor<?x112x112x96xf16>) -> tensor<?x112x112x96xf32>\n",
            "  %190 = \"tfl.mul\"(%189, %88) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x96xf32>, tensor<96xf32>) -> tensor<?x112x112x96xf32>\n",
            "  %191 = \"tfl.add\"(%190, %87) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x96xf32>, tensor<96xf32>) -> tensor<?x112x112x96xf32>\n",
            "  %192 = \"tfl.cast\"(%191) : (tensor<?x112x112x96xf32>) -> tensor<?x112x112x96xf16>\n",
            "  %193 = \"tf.Relu6\"(%192) {device = \"\"} : (tensor<?x112x112x96xf16>) -> tensor<?x112x112x96xf16>\n",
            "  %194 = \"tf.Pad\"(%193, %162) {device = \"\"} : (tensor<?x112x112x96xf16>, tensor<4x2xi32>) -> tensor<?x113x113x96xf16>\n",
            "  %195 = \"tf.DepthwiseConv2dNative\"(%194, %92) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x113x113x96xf16>, tensor<3x3x96x1xf16>) -> tensor<?x56x56x96xf16>\n",
            "  %196 = \"tfl.cast\"(%195) : (tensor<?x56x56x96xf16>) -> tensor<?x56x56x96xf32>\n",
            "  %197 = \"tfl.mul\"(%196, %91) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x96xf32>, tensor<96xf32>) -> tensor<?x56x56x96xf32>\n",
            "  %198 = \"tfl.add\"(%197, %90) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x96xf32>, tensor<96xf32>) -> tensor<?x56x56x96xf32>\n",
            "  %199 = \"tfl.cast\"(%198) : (tensor<?x56x56x96xf32>) -> tensor<?x56x56x96xf16>\n",
            "  %200 = \"tf.Relu6\"(%199) {device = \"\"} : (tensor<?x56x56x96xf16>) -> tensor<?x56x56x96xf16>\n",
            "  %201 = \"tf.Conv2D\"(%200, %86) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x96xf16>, tensor<1x1x96x24xf16>) -> tensor<?x56x56x24xf16>\n",
            "  %202 = \"tfl.cast\"(%201) : (tensor<?x56x56x24xf16>) -> tensor<?x56x56x24xf32>\n",
            "  %203 = \"tfl.mul\"(%202, %85) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x24xf32>, tensor<24xf32>) -> tensor<?x56x56x24xf32>\n",
            "  %204 = \"tfl.add\"(%203, %84) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x24xf32>, tensor<24xf32>) -> tensor<?x56x56x24xf32>\n",
            "  %205 = \"tfl.cast\"(%204) : (tensor<?x56x56x24xf32>) -> tensor<?x56x56x24xf16>\n",
            "  %206 = \"tf.Conv2D\"(%205, %80) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x24xf16>, tensor<1x1x24x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "  %207 = \"tfl.cast\"(%206) : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf32>\n",
            "  %208 = \"tfl.mul\"(%207, %79) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x144xf32>, tensor<144xf32>) -> tensor<?x56x56x144xf32>\n",
            "  %209 = \"tfl.add\"(%208, %78) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x144xf32>, tensor<144xf32>) -> tensor<?x56x56x144xf32>\n",
            "  %210 = \"tfl.cast\"(%209) : (tensor<?x56x56x144xf32>) -> tensor<?x56x56x144xf16>\n",
            "  %211 = \"tf.Relu6\"(%210) {device = \"\"} : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "  %212 = \"tf.DepthwiseConv2dNative\"(%211, %83) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x56x56x144xf16>, tensor<3x3x144x1xf16>) -> tensor<?x56x56x144xf16>\n",
            "  %213 = \"tfl.cast\"(%212) : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf32>\n",
            "  %214 = \"tfl.mul\"(%213, %82) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x144xf32>, tensor<144xf32>) -> tensor<?x56x56x144xf32>\n",
            "  %215 = \"tfl.add\"(%214, %81) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x144xf32>, tensor<144xf32>) -> tensor<?x56x56x144xf32>\n",
            "  %216 = \"tfl.cast\"(%215) : (tensor<?x56x56x144xf32>) -> tensor<?x56x56x144xf16>\n",
            "  %217 = \"tf.Relu6\"(%216) {device = \"\"} : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "  %218 = \"tf.Conv2D\"(%217, %77) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x144xf16>, tensor<1x1x144x24xf16>) -> tensor<?x56x56x24xf16>\n",
            "  %219 = \"tfl.cast\"(%218) : (tensor<?x56x56x24xf16>) -> tensor<?x56x56x24xf32>\n",
            "  %220 = \"tfl.mul\"(%219, %76) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x24xf32>, tensor<24xf32>) -> tensor<?x56x56x24xf32>\n",
            "  %221 = \"tfl.add\"(%220, %75) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x24xf32>, tensor<24xf32>) -> tensor<?x56x56x24xf32>\n",
            "  %222 = \"tfl.cast\"(%221) : (tensor<?x56x56x24xf32>) -> tensor<?x56x56x24xf16>\n",
            "  %223 = \"tf.AddV2\"(%205, %222) {device = \"\"} : (tensor<?x56x56x24xf16>, tensor<?x56x56x24xf16>) -> tensor<?x56x56x24xf16>\n",
            "  %224 = \"tf.Conv2D\"(%223, %71) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x24xf16>, tensor<1x1x24x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "  %225 = \"tfl.cast\"(%224) : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf32>\n",
            "  %226 = \"tfl.mul\"(%225, %70) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x144xf32>, tensor<144xf32>) -> tensor<?x56x56x144xf32>\n",
            "  %227 = \"tfl.add\"(%226, %69) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x144xf32>, tensor<144xf32>) -> tensor<?x56x56x144xf32>\n",
            "  %228 = \"tfl.cast\"(%227) : (tensor<?x56x56x144xf32>) -> tensor<?x56x56x144xf16>\n",
            "  %229 = \"tf.Relu6\"(%228) {device = \"\"} : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "  %230 = \"tf.Pad\"(%229, %162) {device = \"\"} : (tensor<?x56x56x144xf16>, tensor<4x2xi32>) -> tensor<?x57x57x144xf16>\n",
            "  %231 = \"tf.DepthwiseConv2dNative\"(%230, %74) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x57x57x144xf16>, tensor<3x3x144x1xf16>) -> tensor<?x28x28x144xf16>\n",
            "  %232 = \"tfl.cast\"(%231) : (tensor<?x28x28x144xf16>) -> tensor<?x28x28x144xf32>\n",
            "  %233 = \"tfl.mul\"(%232, %73) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x144xf32>, tensor<144xf32>) -> tensor<?x28x28x144xf32>\n",
            "  %234 = \"tfl.add\"(%233, %72) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x144xf32>, tensor<144xf32>) -> tensor<?x28x28x144xf32>\n",
            "  %235 = \"tfl.cast\"(%234) : (tensor<?x28x28x144xf32>) -> tensor<?x28x28x144xf16>\n",
            "  %236 = \"tf.Relu6\"(%235) {device = \"\"} : (tensor<?x28x28x144xf16>) -> tensor<?x28x28x144xf16>\n",
            "  %237 = \"tf.Conv2D\"(%236, %68) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x144xf16>, tensor<1x1x144x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "  %238 = \"tfl.cast\"(%237) : (tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf32>\n",
            "  %239 = \"tfl.mul\"(%238, %67) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x32xf32>, tensor<32xf32>) -> tensor<?x28x28x32xf32>\n",
            "  %240 = \"tfl.add\"(%239, %66) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x32xf32>, tensor<32xf32>) -> tensor<?x28x28x32xf32>\n",
            "  %241 = \"tfl.cast\"(%240) : (tensor<?x28x28x32xf32>) -> tensor<?x28x28x32xf16>\n",
            "  %242 = \"tf.Conv2D\"(%241, %62) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %243 = \"tfl.cast\"(%242) : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf32>\n",
            "  %244 = \"tfl.mul\"(%243, %61) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %245 = \"tfl.add\"(%244, %60) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %246 = \"tfl.cast\"(%245) : (tensor<?x28x28x192xf32>) -> tensor<?x28x28x192xf16>\n",
            "  %247 = \"tf.Relu6\"(%246) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %248 = \"tf.DepthwiseConv2dNative\"(%247, %65) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %249 = \"tfl.cast\"(%248) : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf32>\n",
            "  %250 = \"tfl.mul\"(%249, %64) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %251 = \"tfl.add\"(%250, %63) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %252 = \"tfl.cast\"(%251) : (tensor<?x28x28x192xf32>) -> tensor<?x28x28x192xf16>\n",
            "  %253 = \"tf.Relu6\"(%252) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %254 = \"tf.Conv2D\"(%253, %59) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<1x1x192x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "  %255 = \"tfl.cast\"(%254) : (tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf32>\n",
            "  %256 = \"tfl.mul\"(%255, %58) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x32xf32>, tensor<32xf32>) -> tensor<?x28x28x32xf32>\n",
            "  %257 = \"tfl.add\"(%256, %57) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x32xf32>, tensor<32xf32>) -> tensor<?x28x28x32xf32>\n",
            "  %258 = \"tfl.cast\"(%257) : (tensor<?x28x28x32xf32>) -> tensor<?x28x28x32xf16>\n",
            "  %259 = \"tf.AddV2\"(%241, %258) {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "  %260 = \"tf.Conv2D\"(%259, %53) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %261 = \"tfl.cast\"(%260) : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf32>\n",
            "  %262 = \"tfl.mul\"(%261, %52) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %263 = \"tfl.add\"(%262, %51) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %264 = \"tfl.cast\"(%263) : (tensor<?x28x28x192xf32>) -> tensor<?x28x28x192xf16>\n",
            "  %265 = \"tf.Relu6\"(%264) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %266 = \"tf.DepthwiseConv2dNative\"(%265, %56) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %267 = \"tfl.cast\"(%266) : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf32>\n",
            "  %268 = \"tfl.mul\"(%267, %55) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %269 = \"tfl.add\"(%268, %54) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %270 = \"tfl.cast\"(%269) : (tensor<?x28x28x192xf32>) -> tensor<?x28x28x192xf16>\n",
            "  %271 = \"tf.Relu6\"(%270) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %272 = \"tf.Conv2D\"(%271, %50) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<1x1x192x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "  %273 = \"tfl.cast\"(%272) : (tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf32>\n",
            "  %274 = \"tfl.mul\"(%273, %49) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x32xf32>, tensor<32xf32>) -> tensor<?x28x28x32xf32>\n",
            "  %275 = \"tfl.add\"(%274, %48) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x32xf32>, tensor<32xf32>) -> tensor<?x28x28x32xf32>\n",
            "  %276 = \"tfl.cast\"(%275) : (tensor<?x28x28x32xf32>) -> tensor<?x28x28x32xf16>\n",
            "  %277 = \"tf.AddV2\"(%259, %276) {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "  %278 = \"tf.Conv2D\"(%277, %44) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %279 = \"tfl.cast\"(%278) : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf32>\n",
            "  %280 = \"tfl.mul\"(%279, %43) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %281 = \"tfl.add\"(%280, %42) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %282 = \"tfl.cast\"(%281) : (tensor<?x28x28x192xf32>) -> tensor<?x28x28x192xf16>\n",
            "  %283 = \"tf.Relu6\"(%282) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %284 = \"tf.Pad\"(%283, %162) {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<4x2xi32>) -> tensor<?x29x29x192xf16>\n",
            "  %285 = \"tf.DepthwiseConv2dNative\"(%284, %47) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x29x29x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x14x14x192xf16>\n",
            "  %286 = \"tfl.cast\"(%285) : (tensor<?x14x14x192xf16>) -> tensor<?x14x14x192xf32>\n",
            "  %287 = \"tfl.mul\"(%286, %46) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x192xf32>, tensor<192xf32>) -> tensor<?x14x14x192xf32>\n",
            "  %288 = \"tfl.add\"(%287, %45) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x192xf32>, tensor<192xf32>) -> tensor<?x14x14x192xf32>\n",
            "  %289 = \"tfl.cast\"(%288) : (tensor<?x14x14x192xf32>) -> tensor<?x14x14x192xf16>\n",
            "  %290 = \"tf.Relu6\"(%289) {device = \"\"} : (tensor<?x14x14x192xf16>) -> tensor<?x14x14x192xf16>\n",
            "  %291 = \"tf.Conv2D\"(%290, %41) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x192xf16>, tensor<1x1x192x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %292 = \"tfl.cast\"(%291) : (tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf32>\n",
            "  %293 = \"tfl.mul\"(%292, %40) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %294 = \"tfl.add\"(%293, %39) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %295 = \"tfl.cast\"(%294) : (tensor<?x14x14x64xf32>) -> tensor<?x14x14x64xf16>\n",
            "  %296 = \"tf.Conv2D\"(%295, %35) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %297 = \"tfl.cast\"(%296) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %298 = \"tfl.mul\"(%297, %34) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %299 = \"tfl.add\"(%298, %33) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %300 = \"tfl.cast\"(%299) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %301 = \"tf.Relu6\"(%300) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %302 = \"tf.DepthwiseConv2dNative\"(%301, %38) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %303 = \"tfl.cast\"(%302) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %304 = \"tfl.mul\"(%303, %37) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %305 = \"tfl.add\"(%304, %36) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %306 = \"tfl.cast\"(%305) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %307 = \"tf.Relu6\"(%306) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %308 = \"tf.Conv2D\"(%307, %32) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %309 = \"tfl.cast\"(%308) : (tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf32>\n",
            "  %310 = \"tfl.mul\"(%309, %31) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %311 = \"tfl.add\"(%310, %30) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %312 = \"tfl.cast\"(%311) : (tensor<?x14x14x64xf32>) -> tensor<?x14x14x64xf16>\n",
            "  %313 = \"tf.AddV2\"(%295, %312) {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %314 = \"tf.Conv2D\"(%313, %26) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %315 = \"tfl.cast\"(%314) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %316 = \"tfl.mul\"(%315, %25) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %317 = \"tfl.add\"(%316, %24) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %318 = \"tfl.cast\"(%317) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %319 = \"tf.Relu6\"(%318) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %320 = \"tf.DepthwiseConv2dNative\"(%319, %29) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %321 = \"tfl.cast\"(%320) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %322 = \"tfl.mul\"(%321, %28) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %323 = \"tfl.add\"(%322, %27) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %324 = \"tfl.cast\"(%323) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %325 = \"tf.Relu6\"(%324) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %326 = \"tf.Conv2D\"(%325, %23) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %327 = \"tfl.cast\"(%326) : (tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf32>\n",
            "  %328 = \"tfl.mul\"(%327, %22) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %329 = \"tfl.add\"(%328, %21) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %330 = \"tfl.cast\"(%329) : (tensor<?x14x14x64xf32>) -> tensor<?x14x14x64xf16>\n",
            "  %331 = \"tf.AddV2\"(%313, %330) {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %332 = \"tf.Conv2D\"(%331, %17) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %333 = \"tfl.cast\"(%332) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %334 = \"tfl.mul\"(%333, %16) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %335 = \"tfl.add\"(%334, %15) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %336 = \"tfl.cast\"(%335) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %337 = \"tf.Relu6\"(%336) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %338 = \"tf.DepthwiseConv2dNative\"(%337, %20) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %339 = \"tfl.cast\"(%338) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %340 = \"tfl.mul\"(%339, %19) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %341 = \"tfl.add\"(%340, %18) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %342 = \"tfl.cast\"(%341) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %343 = \"tf.Relu6\"(%342) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %344 = \"tf.Conv2D\"(%343, %14) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %345 = \"tfl.cast\"(%344) : (tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf32>\n",
            "  %346 = \"tfl.mul\"(%345, %13) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %347 = \"tfl.add\"(%346, %12) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %348 = \"tfl.cast\"(%347) : (tensor<?x14x14x64xf32>) -> tensor<?x14x14x64xf16>\n",
            "  %349 = \"tf.AddV2\"(%331, %348) {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %350 = \"tf.Conv2D\"(%349, %152) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %351 = \"tfl.cast\"(%350) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %352 = \"tfl.mul\"(%351, %151) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %353 = \"tfl.add\"(%352, %150) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %354 = \"tfl.cast\"(%353) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %355 = \"tf.Relu6\"(%354) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %356 = \"tf.DepthwiseConv2dNative\"(%355, %155) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %357 = \"tfl.cast\"(%356) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %358 = \"tfl.mul\"(%357, %154) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %359 = \"tfl.add\"(%358, %153) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %360 = \"tfl.cast\"(%359) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %361 = \"tf.Relu6\"(%360) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %362 = \"tf.Conv2D\"(%361, %149) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "  %363 = \"tfl.cast\"(%362) : (tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf32>\n",
            "  %364 = \"tfl.mul\"(%363, %148) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x96xf32>, tensor<96xf32>) -> tensor<?x14x14x96xf32>\n",
            "  %365 = \"tfl.add\"(%364, %147) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x96xf32>, tensor<96xf32>) -> tensor<?x14x14x96xf32>\n",
            "  %366 = \"tfl.cast\"(%365) : (tensor<?x14x14x96xf32>) -> tensor<?x14x14x96xf16>\n",
            "  %367 = \"tf.Conv2D\"(%366, %143) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %368 = \"tfl.cast\"(%367) : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf32>\n",
            "  %369 = \"tfl.mul\"(%368, %142) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %370 = \"tfl.add\"(%369, %141) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %371 = \"tfl.cast\"(%370) : (tensor<?x14x14x576xf32>) -> tensor<?x14x14x576xf16>\n",
            "  %372 = \"tf.Relu6\"(%371) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %373 = \"tf.DepthwiseConv2dNative\"(%372, %146) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %374 = \"tfl.cast\"(%373) : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf32>\n",
            "  %375 = \"tfl.mul\"(%374, %145) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %376 = \"tfl.add\"(%375, %144) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %377 = \"tfl.cast\"(%376) : (tensor<?x14x14x576xf32>) -> tensor<?x14x14x576xf16>\n",
            "  %378 = \"tf.Relu6\"(%377) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %379 = \"tf.Conv2D\"(%378, %140) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<1x1x576x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "  %380 = \"tfl.cast\"(%379) : (tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf32>\n",
            "  %381 = \"tfl.mul\"(%380, %139) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x96xf32>, tensor<96xf32>) -> tensor<?x14x14x96xf32>\n",
            "  %382 = \"tfl.add\"(%381, %138) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x96xf32>, tensor<96xf32>) -> tensor<?x14x14x96xf32>\n",
            "  %383 = \"tfl.cast\"(%382) : (tensor<?x14x14x96xf32>) -> tensor<?x14x14x96xf16>\n",
            "  %384 = \"tf.AddV2\"(%366, %383) {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "  %385 = \"tf.Conv2D\"(%384, %134) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %386 = \"tfl.cast\"(%385) : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf32>\n",
            "  %387 = \"tfl.mul\"(%386, %133) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %388 = \"tfl.add\"(%387, %132) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %389 = \"tfl.cast\"(%388) : (tensor<?x14x14x576xf32>) -> tensor<?x14x14x576xf16>\n",
            "  %390 = \"tf.Relu6\"(%389) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %391 = \"tf.DepthwiseConv2dNative\"(%390, %137) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %392 = \"tfl.cast\"(%391) : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf32>\n",
            "  %393 = \"tfl.mul\"(%392, %136) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %394 = \"tfl.add\"(%393, %135) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %395 = \"tfl.cast\"(%394) : (tensor<?x14x14x576xf32>) -> tensor<?x14x14x576xf16>\n",
            "  %396 = \"tf.Relu6\"(%395) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %397 = \"tf.Conv2D\"(%396, %131) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<1x1x576x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "  %398 = \"tfl.cast\"(%397) : (tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf32>\n",
            "  %399 = \"tfl.mul\"(%398, %130) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x96xf32>, tensor<96xf32>) -> tensor<?x14x14x96xf32>\n",
            "  %400 = \"tfl.add\"(%399, %129) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x96xf32>, tensor<96xf32>) -> tensor<?x14x14x96xf32>\n",
            "  %401 = \"tfl.cast\"(%400) : (tensor<?x14x14x96xf32>) -> tensor<?x14x14x96xf16>\n",
            "  %402 = \"tf.AddV2\"(%384, %401) {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "  %403 = \"tf.Conv2D\"(%402, %125) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %404 = \"tfl.cast\"(%403) : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf32>\n",
            "  %405 = \"tfl.mul\"(%404, %124) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %406 = \"tfl.add\"(%405, %123) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %407 = \"tfl.cast\"(%406) : (tensor<?x14x14x576xf32>) -> tensor<?x14x14x576xf16>\n",
            "  %408 = \"tf.Relu6\"(%407) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %409 = \"tf.Pad\"(%408, %162) {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<4x2xi32>) -> tensor<?x15x15x576xf16>\n",
            "  %410 = \"tf.DepthwiseConv2dNative\"(%409, %128) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x15x15x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x7x7x576xf16>\n",
            "  %411 = \"tfl.cast\"(%410) : (tensor<?x7x7x576xf16>) -> tensor<?x7x7x576xf32>\n",
            "  %412 = \"tfl.mul\"(%411, %127) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x576xf32>, tensor<576xf32>) -> tensor<?x7x7x576xf32>\n",
            "  %413 = \"tfl.add\"(%412, %126) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x576xf32>, tensor<576xf32>) -> tensor<?x7x7x576xf32>\n",
            "  %414 = \"tfl.cast\"(%413) : (tensor<?x7x7x576xf32>) -> tensor<?x7x7x576xf16>\n",
            "  %415 = \"tf.Relu6\"(%414) {device = \"\"} : (tensor<?x7x7x576xf16>) -> tensor<?x7x7x576xf16>\n",
            "  %416 = \"tf.Conv2D\"(%415, %122) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x576xf16>, tensor<1x1x576x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "  %417 = \"tfl.cast\"(%416) : (tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf32>\n",
            "  %418 = \"tfl.mul\"(%417, %121) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x160xf32>, tensor<160xf32>) -> tensor<?x7x7x160xf32>\n",
            "  %419 = \"tfl.add\"(%418, %120) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x160xf32>, tensor<160xf32>) -> tensor<?x7x7x160xf32>\n",
            "  %420 = \"tfl.cast\"(%419) : (tensor<?x7x7x160xf32>) -> tensor<?x7x7x160xf16>\n",
            "  %421 = \"tf.Conv2D\"(%420, %116) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %422 = \"tfl.cast\"(%421) : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf32>\n",
            "  %423 = \"tfl.mul\"(%422, %115) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %424 = \"tfl.add\"(%423, %114) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %425 = \"tfl.cast\"(%424) : (tensor<?x7x7x960xf32>) -> tensor<?x7x7x960xf16>\n",
            "  %426 = \"tf.Relu6\"(%425) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %427 = \"tf.DepthwiseConv2dNative\"(%426, %119) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %428 = \"tfl.cast\"(%427) : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf32>\n",
            "  %429 = \"tfl.mul\"(%428, %118) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %430 = \"tfl.add\"(%429, %117) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %431 = \"tfl.cast\"(%430) : (tensor<?x7x7x960xf32>) -> tensor<?x7x7x960xf16>\n",
            "  %432 = \"tf.Relu6\"(%431) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %433 = \"tf.Conv2D\"(%432, %113) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<1x1x960x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "  %434 = \"tfl.cast\"(%433) : (tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf32>\n",
            "  %435 = \"tfl.mul\"(%434, %112) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x160xf32>, tensor<160xf32>) -> tensor<?x7x7x160xf32>\n",
            "  %436 = \"tfl.add\"(%435, %111) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x160xf32>, tensor<160xf32>) -> tensor<?x7x7x160xf32>\n",
            "  %437 = \"tfl.cast\"(%436) : (tensor<?x7x7x160xf32>) -> tensor<?x7x7x160xf16>\n",
            "  %438 = \"tf.AddV2\"(%420, %437) {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "  %439 = \"tf.Conv2D\"(%438, %107) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %440 = \"tfl.cast\"(%439) : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf32>\n",
            "  %441 = \"tfl.mul\"(%440, %106) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %442 = \"tfl.add\"(%441, %105) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %443 = \"tfl.cast\"(%442) : (tensor<?x7x7x960xf32>) -> tensor<?x7x7x960xf16>\n",
            "  %444 = \"tf.Relu6\"(%443) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %445 = \"tf.DepthwiseConv2dNative\"(%444, %110) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %446 = \"tfl.cast\"(%445) : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf32>\n",
            "  %447 = \"tfl.mul\"(%446, %109) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %448 = \"tfl.add\"(%447, %108) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %449 = \"tfl.cast\"(%448) : (tensor<?x7x7x960xf32>) -> tensor<?x7x7x960xf16>\n",
            "  %450 = \"tf.Relu6\"(%449) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %451 = \"tf.Conv2D\"(%450, %104) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<1x1x960x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "  %452 = \"tfl.cast\"(%451) : (tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf32>\n",
            "  %453 = \"tfl.mul\"(%452, %103) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x160xf32>, tensor<160xf32>) -> tensor<?x7x7x160xf32>\n",
            "  %454 = \"tfl.add\"(%453, %102) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x160xf32>, tensor<160xf32>) -> tensor<?x7x7x160xf32>\n",
            "  %455 = \"tfl.cast\"(%454) : (tensor<?x7x7x160xf32>) -> tensor<?x7x7x160xf16>\n",
            "  %456 = \"tf.AddV2\"(%438, %455) {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "  %457 = \"tf.Conv2D\"(%456, %98) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %458 = \"tfl.cast\"(%457) : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf32>\n",
            "  %459 = \"tfl.mul\"(%458, %97) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %460 = \"tfl.add\"(%459, %96) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %461 = \"tfl.cast\"(%460) : (tensor<?x7x7x960xf32>) -> tensor<?x7x7x960xf16>\n",
            "  %462 = \"tf.Relu6\"(%461) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %463 = \"tf.DepthwiseConv2dNative\"(%462, %101) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %464 = \"tfl.cast\"(%463) : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf32>\n",
            "  %465 = \"tfl.mul\"(%464, %100) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %466 = \"tfl.add\"(%465, %99) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %467 = \"tfl.cast\"(%466) : (tensor<?x7x7x960xf32>) -> tensor<?x7x7x960xf16>\n",
            "  %468 = \"tf.Relu6\"(%467) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %469 = \"tf.Conv2D\"(%468, %95) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<1x1x960x320xf16>) -> tensor<?x7x7x320xf16>\n",
            "  %470 = \"tfl.cast\"(%469) : (tensor<?x7x7x320xf16>) -> tensor<?x7x7x320xf32>\n",
            "  %471 = \"tfl.mul\"(%470, %94) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x320xf32>, tensor<320xf32>) -> tensor<?x7x7x320xf32>\n",
            "  %472 = \"tfl.add\"(%471, %93) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x320xf32>, tensor<320xf32>) -> tensor<?x7x7x320xf32>\n",
            "  %473 = \"tfl.cast\"(%472) : (tensor<?x7x7x320xf32>) -> tensor<?x7x7x320xf16>\n",
            "  %474 = \"tf.Conv2D\"(%473, %8) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x320xf16>, tensor<1x1x320x1280xf16>) -> tensor<?x7x7x1280xf16>\n",
            "  %475 = \"tfl.cast\"(%474) : (tensor<?x7x7x1280xf16>) -> tensor<?x7x7x1280xf32>\n",
            "  %476 = \"tfl.mul\"(%475, %7) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x1280xf32>, tensor<1280xf32>) -> tensor<?x7x7x1280xf32>\n",
            "  %477 = \"tfl.add\"(%476, %6) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x1280xf32>, tensor<1280xf32>) -> tensor<?x7x7x1280xf32>\n",
            "  %478 = \"tfl.cast\"(%477) : (tensor<?x7x7x1280xf32>) -> tensor<?x7x7x1280xf16>\n",
            "  %479 = \"tf.Relu6\"(%478) {device = \"\"} : (tensor<?x7x7x1280xf16>) -> tensor<?x7x7x1280xf16>\n",
            "  %480 = \"tfl.cast\"(%479) : (tensor<?x7x7x1280xf16>) -> tensor<?x7x7x1280xf32>\n",
            "  %481 = \"tfl.mean\"(%480, %163) <{keep_dims = false}> : (tensor<?x7x7x1280xf32>, tensor<2xi32>) -> tensor<?x1280xf32>\n",
            "  %482 = \"tfl.cast\"(%481) : (tensor<?x1280xf32>) -> tensor<?x1280xf16>\n",
            "  %483 = \"tf.MatMul\"(%482, %165) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x1280xf16>, tensor<1024x1280xf16>) -> tensor<?x1024xf16>\n",
            "  %484 = \"tf.BiasAdd\"(%483, %156) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x1024xf16>, tensor<1024xf16>) -> tensor<?x1024xf16>\n",
            "  %485 = \"tfl.cast\"(%484) : (tensor<?x1024xf16>) -> tensor<?x1024xf32>\n",
            "  %486 = \"tfl.mul\"(%485, %159) <{fused_activation_function = \"NONE\"}> : (tensor<?x1024xf32>, tensor<1024xf32>) -> tensor<?x1024xf32>\n",
            "  %487 = \"tfl.add\"(%486, %158) <{fused_activation_function = \"NONE\"}> : (tensor<?x1024xf32>, tensor<1024xf32>) -> tensor<?x1024xf32>\n",
            "  %488 = \"tfl.cast\"(%487) : (tensor<?x1024xf32>) -> tensor<?x1024xf16>\n",
            "  %489 = \"tf.Relu\"(%488) {device = \"\"} : (tensor<?x1024xf16>) -> tensor<?x1024xf16>\n",
            "  %490 = \"tf.MatMul\"(%489, %166) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x1024xf16>, tensor<512x1024xf16>) -> tensor<?x512xf16>\n",
            "  %491 = \"tf.BiasAdd\"(%490, %157) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x512xf16>, tensor<512xf16>) -> tensor<?x512xf16>\n",
            "  %492 = \"tfl.cast\"(%491) : (tensor<?x512xf16>) -> tensor<?x512xf32>\n",
            "  %493 = \"tfl.mul\"(%492, %161) <{fused_activation_function = \"NONE\"}> : (tensor<?x512xf32>, tensor<512xf32>) -> tensor<?x512xf32>\n",
            "  %494 = \"tfl.add\"(%493, %160) <{fused_activation_function = \"NONE\"}> : (tensor<?x512xf32>, tensor<512xf32>) -> tensor<?x512xf32>\n",
            "  %495 = \"tfl.cast\"(%494) : (tensor<?x512xf32>) -> tensor<?x512xf16>\n",
            "  %496 = \"tf.Relu\"(%495) {device = \"\"} : (tensor<?x512xf16>) -> tensor<?x512xf16>\n",
            "  %497 = \"tfl.cast\"(%496) : (tensor<?x512xf16>) -> tensor<?x512xf32>\n",
            "  %498 = \"tfl.fully_connected\"(%497, %167, %164) <{asymmetric_quantize_inputs = true, fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"}> : (tensor<?x512xf32>, tensor<101x512x!quant.uniform<i8<-127:127>:f32:0, {0.0035196880655964527,0.003034102869784738,0.0029790898946326547,0.0034422384003015955,0.00286281789381673,0.0030478196819936198,0.003764879515790564,0.003543796032432496,0.0031637364015804502,0.0027744364081405278,0.0024476149889427847,0.0031610408167200765,0.0029627478967501424,0.0039326790280229465,0.0031387458636066108,0.0034845112346288728,0.0030931189773589609,0.0032800393780385417,0.0036839767703859823,0.003220272815133643,0.0034213941397629385,0.0032939842836124692,0.003287345640302643,0.0029239199292941357,0.0033778398055729904,0.003882211728358832,0.0025053380981205015,0.0045974465805714527,0.0032684007498223013,0.0044137145590594437,0.0032357212126724365,0.0038919059310372419,0.0032753409363153411,0.0042975835912809597,0.0033498709126720278,0.0029546031801719365,0.0029587001781763993,0.0033694843607624684,0.0028530413240898311,0.0028612221789172314,0.0038846372150060701,0.0040789673647542634,0.0028956363520284338,0.0032025132122940904,0.0031092629188627708,0.0038356220159004991,0.0038850044640969103,0.0034358346556115338,0.0038642432745986098,0.0031052731622861125,0.003401375661684772,0.003270303874503909,0.0036903842696993368,0.0035367948333109456,0.0042598463418915518,0.0028427222112971029,0.0032642432085172399,0.0032351033424767922,0.0039963919346726788,0.0027833245401307355,0.0042847981603126827,0.0034589420153400092,0.0034374484396356296,0.003624631898609672,0.0037511497970641127,0.0031140409116669904,0.0033612079977050542,0.0022823880976579319,0.0045012498465109999,0.0043973964969004232,0.0035856248825553833,0.0037857871355972891,0.0026739720753797395,0.0034905364663582149,0.0034097167919939896,0.0039276145105286847,0.0034584543836398388,0.0035291572255412426,0.0031558035865543394,0.0032488952471515327,0.0030280698941448541,0.003758564474075798,0.0027166464197354054,0.0039795975046833668,0.0031718977792995181,0.0025550577584214098,0.0030911316083172173,0.0027391469384741595,0.0036072670005437898,0.0029970374633008102,0.004290158823719175,0.0056339633746409976,0.0033783023282298891,0.0030751214252682183,0.0038292652040015996,0.0030709946249413678,0.0032122611530183806,0.0037165678392245076,0.0034231632713257799,0.0027992281857437975,0.0032070337787387876}>>, tensor<101xf32>) -> tensor<?x101xf32>\n",
            "  %499 = \"tfl.softmax\"(%498) <{beta = 1.000000e+00 : f32}> : (tensor<?x101xf32>) -> tensor<?x101xf32>\n",
            "  \"func.return\"(%499) : (tensor<?x101xf32>) -> ()\n",
            "}) {tf.entry_function = {control_outputs = \"\", inputs = \"serving_default_keras_tensor_155:0\", outputs = \"StatefulPartitionedCall_1:0\"}, tf_saved_model.exported_names = [\"serving_default\"]} : () -> ()\n",
            "\n",
            "âœ… Labels saved as 'labels.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def fix_tflite_conversion(model, train_dataset):\n",
        "    \"\"\"\n",
        "    Fixed TFLite conversion dengan beberapa strategi\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"ğŸ”§ Fixing TFLite conversion...\")\n",
        "\n",
        "    # Strategy 1: Convert to float32 first\n",
        "    print(\"\\nğŸ“Š Strategy 1: Converting to float32...\")\n",
        "    try:\n",
        "        # Clone model dengan float32\n",
        "        model_float32 = tf.keras.models.clone_model(model)\n",
        "        model_float32.set_weights(model.get_weights())\n",
        "\n",
        "        # Compile dengan float32\n",
        "        model_float32.compile(\n",
        "            optimizer='adam',\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        # Convert dengan float32 model\n",
        "        converter = tf.lite.TFLiteConverter.from_keras_model(model_float32)\n",
        "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "        tflite_model = converter.convert()\n",
        "\n",
        "        # Save TFLite model\n",
        "        with open('food_classifier_fixed.tflite', 'wb') as f:\n",
        "            f.write(tflite_model)\n",
        "\n",
        "        print(\"âœ… Strategy 1 successful! Model saved as 'food_classifier_fixed.tflite'\")\n",
        "        return tflite_model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Strategy 1 failed: {e}\")\n",
        "\n",
        "    # Strategy 2: Use representative dataset for quantization\n",
        "    print(\"\\nğŸ“Š Strategy 2: Using representative dataset...\")\n",
        "    try:\n",
        "        def representative_dataset():\n",
        "            for batch in train_dataset.take(100):  # Take 100 batches\n",
        "                images, _ = batch\n",
        "                yield [images.numpy().astype(np.float32)]\n",
        "\n",
        "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "        converter.representative_dataset = representative_dataset\n",
        "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "        converter.inference_input_type = tf.uint8\n",
        "        converter.inference_output_type = tf.uint8\n",
        "\n",
        "        tflite_model = converter.convert()\n",
        "\n",
        "        with open('food_classifier_quantized.tflite', 'wb') as f:\n",
        "            f.write(tflite_model)\n",
        "\n",
        "        print(\"âœ… Strategy 2 successful! Model saved as 'food_classifier_quantized.tflite'\")\n",
        "        return tflite_model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Strategy 2 failed: {e}\")\n",
        "\n",
        "    # Strategy 3: Enable SELECT_TF_OPS (Flex ops)\n",
        "    print(\"\\nğŸ“Š Strategy 3: Using TensorFlow Lite with Flex ops...\")\n",
        "    try:\n",
        "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "        converter.target_spec.supported_ops = [\n",
        "            tf.lite.OpsSet.TFLITE_BUILTINS,\n",
        "            tf.lite.OpsSet.SELECT_TF_OPS  # Enable TensorFlow ops\n",
        "        ]\n",
        "\n",
        "        tflite_model = converter.convert()\n",
        "\n",
        "        with open('food_classifier_flex.tflite', 'wb') as f:\n",
        "            f.write(tflite_model)\n",
        "\n",
        "        print(\"âœ… Strategy 3 successful! Model saved as 'food_classifier_flex.tflite'\")\n",
        "        print(\"âš ï¸  Note: This model requires TensorFlow Lite with Flex ops support\")\n",
        "        return tflite_model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Strategy 3 failed: {e}\")\n",
        "\n",
        "    # Strategy 4: Create a simpler model without problematic layers\n",
        "    print(\"\\nğŸ“Š Strategy 4: Creating simplified model...\")\n",
        "    try:\n",
        "        # Create inference-only model without training layers\n",
        "        inference_model = create_inference_model(model)\n",
        "\n",
        "        converter = tf.lite.TFLiteConverter.from_keras_model(inference_model)\n",
        "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "        tflite_model = converter.convert()\n",
        "\n",
        "        with open('food_classifier_simplified.tflite', 'wb') as f:\n",
        "            f.write(tflite_model)\n",
        "\n",
        "        print(\"âœ… Strategy 4 successful! Model saved as 'food_classifier_simplified.tflite'\")\n",
        "        return tflite_model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Strategy 4 failed: {e}\")\n",
        "\n",
        "    print(\"âŒ All conversion strategies failed. Keeping Keras model only.\")\n",
        "    return None\n",
        "\n",
        "def create_inference_model(trained_model):\n",
        "    \"\"\"\n",
        "    Create a clean inference model without training-specific layers\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"ğŸ—ï¸ Creating clean inference model...\")\n",
        "\n",
        "    # Create a new model with same architecture but without training layers\n",
        "    from tensorflow.keras.applications import MobileNetV2\n",
        "    from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "\n",
        "    # Base model\n",
        "    base_model = MobileNetV2(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(224, 224, 3)\n",
        "    )\n",
        "\n",
        "    # Simple inference model\n",
        "    inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "    x = base_model(inputs, training=False)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    outputs = Dense(101, activation='softmax')(x)\n",
        "\n",
        "    inference_model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    # Copy weights from trained model (carefully)\n",
        "    try:\n",
        "        # Get weights from trained model\n",
        "        trained_weights = trained_model.get_weights()\n",
        "\n",
        "        # Set weights to inference model\n",
        "        # Skip augmentation layers and batch norm layers\n",
        "        inference_model.set_weights(trained_weights[-6:])  # Last 6 layers (dense layers)\n",
        "\n",
        "        print(\"âœ… Weights transferred successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Weight transfer failed: {e}\")\n",
        "        print(\"Using random weights for inference model\")\n",
        "\n",
        "    return inference_model\n",
        "\n",
        "# Run the fixed conversion\n",
        "tflite_model = fix_tflite_conversion(model, train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U4IUMSK3j00",
        "outputId": "a1f19040-cf8f-4dce-c94b-c157f354598c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Fixing TFLite conversion...\n",
            "\n",
            "ğŸ“Š Strategy 1: Converting to float32...\n",
            "Saved artifact at '/tmp/tmpltdzuinb'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_155')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 101), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  137186902885392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902889616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902890384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902889808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902888848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902888656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902892112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902892880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902892688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902888464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902893264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902894032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902894800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902894608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902890576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902895184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902895952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902896720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902896528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902891536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902897104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902897872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902898640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902898448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902893648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902899792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902899408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902900176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902899984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902895568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902887120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902897488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190827152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902900560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186902899024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190826000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190824848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190825232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190824656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190826384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190824080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190822928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190823312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190822736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190826768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190829072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190831184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190828304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190828880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190825616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190830416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190830608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190830992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190830800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190823696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190832144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190835408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190833104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190835216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190822352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190836560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190834256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190834832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190834640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190832720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190835984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190836368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190837520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190836944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190831568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186927278608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191215568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191216144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190835792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190833680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191217104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191217488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191217872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191217680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191216336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191219024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191219408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191219792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191219600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191215376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191220944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191221328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191221712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191221520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191216720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191222864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191223248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191223632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191223440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191218640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191224784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191225168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191225552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191225360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191220560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191226704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191227088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191227472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191227280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191222480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191228624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191229008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191229392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191229200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191224400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191230544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191230160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191229776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191231120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191226320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191228240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190905232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190904272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190905040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190904080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190906768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190907152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190907536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190907344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190904464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190908688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190909072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190909456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190909264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190905616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190910608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190910992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190911376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190911184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190906384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190912528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190912912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190913296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190913104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190908304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190914448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190914832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190915216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190915024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190910224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190916368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190916752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190917136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190916944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190912144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190918288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190918672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190919056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190918864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190914064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190915984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191592400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191592976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190917904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185190917520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191593936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191594320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191594704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191594512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191593168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191595856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191596240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191596624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191596432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191592208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191597776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191598160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191598544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191598352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191593552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191599696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191600080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191600464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191600272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191595472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191601616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191602000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191602384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191602192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191597392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191603536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191603920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191604304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191604112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191599312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191605456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191605840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191606224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191606032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191601232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191607376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191606992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191606608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191607952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191603152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185191605072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334379920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334378960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334379728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334378768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334381456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334381840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334382224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334382032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334379152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334383376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334383760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334384144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334383952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334380304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334385296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334385680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334386064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334385872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334381072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334387216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334387600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334387984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334387792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334382992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334389136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334389520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334389904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334389712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334384912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334391056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334391440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334391824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334391632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334386832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334392976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334393360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334393744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334393552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334388752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334390672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334903248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334903824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334392592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334392208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334904784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334905168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334905552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334905360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334904016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334906704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334907088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334907472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334907280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334903440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334908624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334909008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334909392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334909200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334904400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334909776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334910928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334911120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334908240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334910544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334910736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334910352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334913808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334914000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334913232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334912272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334913616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334903632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137185334916688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "âŒ Strategy 1 failed: Could not translate MLIR to FlatBuffer.<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %171 = \"tf.Conv2D\"(%170, %9) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 2, 2, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x224x224x3xf16>, tensor<3x3x3x32xf16>) -> tensor<?x112x112x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %176 = \"tf.Relu6\"(%175) {device = \"\"} : (tensor<?x112x112x32xf16>) -> tensor<?x112x112x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %177 = \"tf.DepthwiseConv2dNative\"(%176, %5) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x112x112x32xf16>, tensor<3x3x32x1xf16>) -> tensor<?x112x112x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %182 = \"tf.Relu6\"(%181) {device = \"\"} : (tensor<?x112x112x32xf16>) -> tensor<?x112x112x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %183 = \"tf.Conv2D\"(%182, %2) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x112x112x32xf16>, tensor<1x1x32x16xf16>) -> tensor<?x112x112x16xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %188 = \"tf.Conv2D\"(%187, %89) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x112x112x16xf16>, tensor<1x1x16x96xf16>) -> tensor<?x112x112x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %193 = \"tf.Relu6\"(%192) {device = \"\"} : (tensor<?x112x112x96xf16>) -> tensor<?x112x112x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_pad_1/Pad@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Pad' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_pad_1/Pad@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %194 = \"tf.Pad\"(%193, %162) {device = \"\"} : (tensor<?x112x112x96xf16>, tensor<4x2xi32>) -> tensor<?x113x113x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_pad_1/Pad@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %195 = \"tf.DepthwiseConv2dNative\"(%194, %92) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x113x113x96xf16>, tensor<3x3x96x1xf16>) -> tensor<?x56x56x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %200 = \"tf.Relu6\"(%199) {device = \"\"} : (tensor<?x56x56x96xf16>) -> tensor<?x56x56x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %201 = \"tf.Conv2D\"(%200, %86) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x96xf16>, tensor<1x1x96x24xf16>) -> tensor<?x56x56x24xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %206 = \"tf.Conv2D\"(%205, %80) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x24xf16>, tensor<1x1x24x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %211 = \"tf.Relu6\"(%210) {device = \"\"} : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %212 = \"tf.DepthwiseConv2dNative\"(%211, %83) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x56x56x144xf16>, tensor<3x3x144x1xf16>) -> tensor<?x56x56x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %217 = \"tf.Relu6\"(%216) {device = \"\"} : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %218 = \"tf.Conv2D\"(%217, %77) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x144xf16>, tensor<1x1x144x24xf16>) -> tensor<?x56x56x24xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %223 = \"tf.AddV2\"(%205, %222) {device = \"\"} : (tensor<?x56x56x24xf16>, tensor<?x56x56x24xf16>) -> tensor<?x56x56x24xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %224 = \"tf.Conv2D\"(%223, %71) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x24xf16>, tensor<1x1x24x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %229 = \"tf.Relu6\"(%228) {device = \"\"} : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_pad_1/Pad@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Pad' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_pad_1/Pad@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %230 = \"tf.Pad\"(%229, %162) {device = \"\"} : (tensor<?x56x56x144xf16>, tensor<4x2xi32>) -> tensor<?x57x57x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_pad_1/Pad@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %231 = \"tf.DepthwiseConv2dNative\"(%230, %74) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x57x57x144xf16>, tensor<3x3x144x1xf16>) -> tensor<?x28x28x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %236 = \"tf.Relu6\"(%235) {device = \"\"} : (tensor<?x28x28x144xf16>) -> tensor<?x28x28x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %237 = \"tf.Conv2D\"(%236, %68) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x144xf16>, tensor<1x1x144x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %242 = \"tf.Conv2D\"(%241, %62) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %247 = \"tf.Relu6\"(%246) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %248 = \"tf.DepthwiseConv2dNative\"(%247, %65) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %253 = \"tf.Relu6\"(%252) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %254 = \"tf.Conv2D\"(%253, %59) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<1x1x192x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %259 = \"tf.AddV2\"(%241, %258) {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %260 = \"tf.Conv2D\"(%259, %53) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %265 = \"tf.Relu6\"(%264) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %266 = \"tf.DepthwiseConv2dNative\"(%265, %56) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %271 = \"tf.Relu6\"(%270) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %272 = \"tf.Conv2D\"(%271, %50) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<1x1x192x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %277 = \"tf.AddV2\"(%259, %276) {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %278 = \"tf.Conv2D\"(%277, %44) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %283 = \"tf.Relu6\"(%282) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_pad_1/Pad@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Pad' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_pad_1/Pad@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %284 = \"tf.Pad\"(%283, %162) {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<4x2xi32>) -> tensor<?x29x29x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_pad_1/Pad@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %285 = \"tf.DepthwiseConv2dNative\"(%284, %47) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x29x29x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x14x14x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %290 = \"tf.Relu6\"(%289) {device = \"\"} : (tensor<?x14x14x192xf16>) -> tensor<?x14x14x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %291 = \"tf.Conv2D\"(%290, %41) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x192xf16>, tensor<1x1x192x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %296 = \"tf.Conv2D\"(%295, %35) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %301 = \"tf.Relu6\"(%300) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %302 = \"tf.DepthwiseConv2dNative\"(%301, %38) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %307 = \"tf.Relu6\"(%306) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %308 = \"tf.Conv2D\"(%307, %32) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %313 = \"tf.AddV2\"(%295, %312) {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %314 = \"tf.Conv2D\"(%313, %26) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %319 = \"tf.Relu6\"(%318) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %320 = \"tf.DepthwiseConv2dNative\"(%319, %29) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %325 = \"tf.Relu6\"(%324) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %326 = \"tf.Conv2D\"(%325, %23) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %331 = \"tf.AddV2\"(%313, %330) {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %332 = \"tf.Conv2D\"(%331, %17) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %337 = \"tf.Relu6\"(%336) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %338 = \"tf.DepthwiseConv2dNative\"(%337, %20) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %343 = \"tf.Relu6\"(%342) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %344 = \"tf.Conv2D\"(%343, %14) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %349 = \"tf.AddV2\"(%331, %348) {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %350 = \"tf.Conv2D\"(%349, %152) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %355 = \"tf.Relu6\"(%354) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %356 = \"tf.DepthwiseConv2dNative\"(%355, %155) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %361 = \"tf.Relu6\"(%360) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %362 = \"tf.Conv2D\"(%361, %149) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %367 = \"tf.Conv2D\"(%366, %143) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %372 = \"tf.Relu6\"(%371) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %373 = \"tf.DepthwiseConv2dNative\"(%372, %146) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %378 = \"tf.Relu6\"(%377) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %379 = \"tf.Conv2D\"(%378, %140) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<1x1x576x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %384 = \"tf.AddV2\"(%366, %383) {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %385 = \"tf.Conv2D\"(%384, %134) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %390 = \"tf.Relu6\"(%389) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %391 = \"tf.DepthwiseConv2dNative\"(%390, %137) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %396 = \"tf.Relu6\"(%395) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %397 = \"tf.Conv2D\"(%396, %131) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<1x1x576x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %402 = \"tf.AddV2\"(%384, %401) {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %403 = \"tf.Conv2D\"(%402, %125) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %408 = \"tf.Relu6\"(%407) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_pad_1/Pad@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Pad' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_pad_1/Pad@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %409 = \"tf.Pad\"(%408, %162) {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<4x2xi32>) -> tensor<?x15x15x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_pad_1/Pad@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %410 = \"tf.DepthwiseConv2dNative\"(%409, %128) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x15x15x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x7x7x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %415 = \"tf.Relu6\"(%414) {device = \"\"} : (tensor<?x7x7x576xf16>) -> tensor<?x7x7x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %416 = \"tf.Conv2D\"(%415, %122) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x576xf16>, tensor<1x1x576x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %421 = \"tf.Conv2D\"(%420, %116) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %426 = \"tf.Relu6\"(%425) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %427 = \"tf.DepthwiseConv2dNative\"(%426, %119) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %432 = \"tf.Relu6\"(%431) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %433 = \"tf.Conv2D\"(%432, %113) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<1x1x960x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %438 = \"tf.AddV2\"(%420, %437) {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %439 = \"tf.Conv2D\"(%438, %107) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %444 = \"tf.Relu6\"(%443) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %445 = \"tf.DepthwiseConv2dNative\"(%444, %110) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %450 = \"tf.Relu6\"(%449) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %451 = \"tf.Conv2D\"(%450, %104) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<1x1x960x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %456 = \"tf.AddV2\"(%438, %455) {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_add_1/Add@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %457 = \"tf.Conv2D\"(%456, %98) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %462 = \"tf.Relu6\"(%461) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %463 = \"tf.DepthwiseConv2dNative\"(%462, %101) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_1/depthwise@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %468 = \"tf.Relu6\"(%467) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %469 = \"tf.Conv2D\"(%468, %95) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<1x1x960x320xf16>) -> tensor<?x7x7x320xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_project_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv_1_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv_1_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %474 = \"tf.Conv2D\"(%473, %8) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x320xf16>, tensor<1x1x320x1280xf16>) -> tensor<?x7x7x1280xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv_1_1/convolution@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/out_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/out_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %479 = \"tf.Relu6\"(%478) {device = \"\"} : (tensor<?x7x7x1280xf16>) -> tensor<?x7x7x1280xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/out_relu_1/Relu6@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1/MatMul@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.MatMul' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1/MatMul@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %483 = \"tf.MatMul\"(%482, %165) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x1280xf16>, tensor<1024x1280xf16>) -> tensor<?x1024xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1/MatMul@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1/BiasAdd@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.BiasAdd' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1/BiasAdd@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %484 = \"tf.BiasAdd\"(%483, %156) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x1024xf16>, tensor<1024xf16>) -> tensor<?x1024xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1/BiasAdd@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/Relu@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/Relu@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %489 = \"tf.Relu\"(%488) {device = \"\"} : (tensor<?x1024xf16>) -> tensor<?x1024xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/Relu@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1_2/MatMul@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.MatMul' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1_2/MatMul@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %490 = \"tf.MatMul\"(%489, %166) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x1024xf16>, tensor<512x1024xf16>) -> tensor<?x512xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1_2/MatMul@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1_2/BiasAdd@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.BiasAdd' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1_2/BiasAdd@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %491 = \"tf.BiasAdd\"(%490, %157) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x512xf16>, tensor<512xf16>) -> tensor<?x512xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1_2/BiasAdd@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/Relu_1@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/Relu_1@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %496 = \"tf.Relu\"(%495) {device = \"\"} : (tensor<?x512xf16>) -> tensor<?x512xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/Relu_1@__inference_function_624451\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_625558\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: failed while converting: 'main': \n",
            "Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \n",
            "TF Select ops: AddV2, BiasAdd, Conv2D, DepthwiseConv2dNative, MatMul, Pad, Relu, Relu6\n",
            "Details:\n",
            "\ttf.AddV2(tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> (tensor<?x14x14x64xf16>) : {device = \"\"}\n",
            "\ttf.AddV2(tensor<?x14x14x96xf16>, tensor<?x14x14x96xf16>) -> (tensor<?x14x14x96xf16>) : {device = \"\"}\n",
            "\ttf.AddV2(tensor<?x28x28x32xf16>, tensor<?x28x28x32xf16>) -> (tensor<?x28x28x32xf16>) : {device = \"\"}\n",
            "\ttf.AddV2(tensor<?x56x56x24xf16>, tensor<?x56x56x24xf16>) -> (tensor<?x56x56x24xf16>) : {device = \"\"}\n",
            "\ttf.AddV2(tensor<?x7x7x160xf16>, tensor<?x7x7x160xf16>) -> (tensor<?x7x7x160xf16>) : {device = \"\"}\n",
            "\ttf.BiasAdd(tensor<?x1024xf16>, tensor<1024xf16>) -> (tensor<?x1024xf16>) : {data_format = \"NHWC\", device = \"\"}\n",
            "\ttf.BiasAdd(tensor<?x512xf16>, tensor<512xf16>) -> (tensor<?x512xf16>) : {data_format = \"NHWC\", device = \"\"}\n",
            "\ttf.Conv2D(tensor<?x112x112x16xf16>, tensor<1x1x16x96xf16>) -> (tensor<?x112x112x96xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x112x112x32xf16>, tensor<1x1x32x16xf16>) -> (tensor<?x112x112x16xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x14x14x192xf16>, tensor<1x1x192x64xf16>) -> (tensor<?x14x14x64xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> (tensor<?x14x14x64xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x14x14x384xf16>, tensor<1x1x384x96xf16>) -> (tensor<?x14x14x96xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x14x14x576xf16>, tensor<1x1x576x96xf16>) -> (tensor<?x14x14x96xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> (tensor<?x14x14x384xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> (tensor<?x14x14x576xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x224x224x3xf16>, tensor<3x3x3x32xf16>) -> (tensor<?x112x112x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 2, 2, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x28x28x144xf16>, tensor<1x1x144x32xf16>) -> (tensor<?x28x28x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x28x28x192xf16>, tensor<1x1x192x32xf16>) -> (tensor<?x28x28x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> (tensor<?x28x28x192xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x56x56x144xf16>, tensor<1x1x144x24xf16>) -> (tensor<?x56x56x24xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x56x56x24xf16>, tensor<1x1x24x144xf16>) -> (tensor<?x56x56x144xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x56x56x96xf16>, tensor<1x1x96x24xf16>) -> (tensor<?x56x56x24xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> (tensor<?x7x7x960xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x7x7x320xf16>, tensor<1x1x320x1280xf16>) -> (tensor<?x7x7x1280xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x7x7x576xf16>, tensor<1x1x576x160xf16>) -> (tensor<?x7x7x160xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x7x7x960xf16>, tensor<1x1x960x160xf16>) -> (tensor<?x7x7x160xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x7x7x960xf16>, tensor<1x1x960x320xf16>) -> (tensor<?x7x7x320xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x112x112x32xf16>, tensor<3x3x32x1xf16>) -> (tensor<?x112x112x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x113x113x96xf16>, tensor<3x3x96x1xf16>) -> (tensor<?x56x56x96xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> (tensor<?x14x14x384xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x14x14x576xf16>, tensor<3x3x576x1xf16>) -> (tensor<?x14x14x576xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x15x15x576xf16>, tensor<3x3x576x1xf16>) -> (tensor<?x7x7x576xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x28x28x192xf16>, tensor<3x3x192x1xf16>) -> (tensor<?x28x28x192xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x29x29x192xf16>, tensor<3x3x192x1xf16>) -> (tensor<?x14x14x192xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x56x56x144xf16>, tensor<3x3x144x1xf16>) -> (tensor<?x56x56x144xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x57x57x144xf16>, tensor<3x3x144x1xf16>) -> (tensor<?x28x28x144xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> (tensor<?x7x7x960xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n",
            "\ttf.MatMul(tensor<?x1024xf16>, tensor<512x1024xf16>) -> (tensor<?x512xf16>) : {grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}\n",
            "\ttf.MatMul(tensor<?x1280xf16>, tensor<1024x1280xf16>) -> (tensor<?x1024xf16>) : {grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}\n",
            "\ttf.Pad(tensor<?x112x112x96xf16>, tensor<4x2xi32>) -> (tensor<?x113x113x96xf16>) : {device = \"\"}\n",
            "\ttf.Pad(tensor<?x14x14x576xf16>, tensor<4x2xi32>) -> (tensor<?x15x15x576xf16>) : {device = \"\"}\n",
            "\ttf.Pad(tensor<?x28x28x192xf16>, tensor<4x2xi32>) -> (tensor<?x29x29x192xf16>) : {device = \"\"}\n",
            "\ttf.Pad(tensor<?x56x56x144xf16>, tensor<4x2xi32>) -> (tensor<?x57x57x144xf16>) : {device = \"\"}\n",
            "\ttf.Relu(tensor<?x1024xf16>) -> (tensor<?x1024xf16>) : {device = \"\"}\n",
            "\ttf.Relu(tensor<?x512xf16>) -> (tensor<?x512xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x112x112x32xf16>) -> (tensor<?x112x112x32xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x112x112x96xf16>) -> (tensor<?x112x112x96xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x14x14x192xf16>) -> (tensor<?x14x14x192xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x14x14x384xf16>) -> (tensor<?x14x14x384xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x14x14x576xf16>) -> (tensor<?x14x14x576xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x28x28x144xf16>) -> (tensor<?x28x28x144xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x28x28x192xf16>) -> (tensor<?x28x28x192xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x56x56x144xf16>) -> (tensor<?x56x56x144xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x56x56x96xf16>) -> (tensor<?x56x56x96xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x7x7x1280xf16>) -> (tensor<?x7x7x1280xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x7x7x576xf16>) -> (tensor<?x7x7x576xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x7x7x960xf16>) -> (tensor<?x7x7x960xf16>) : {device = \"\"}\n",
            "\n",
            "<unknown>:0: note: see current operation: \n",
            "\"func.func\"() <{arg_attrs = [{tf_saved_model.index_path = [\"keras_tensor_155\"]}], function_type = (tensor<?x224x224x3xf32>) -> tensor<?x101xf32>, res_attrs = [{tf_saved_model.index_path = [\"output_0\"]}], sym_name = \"main\"}> ({\n",
            "^bb0(%arg0: tensor<?x224x224x3xf32>):\n",
            "  %0 = \"arith.constant\"() <{value = dense<[-1.79617035, 3.17504382, 16.2980614, 24.0359573, 13.8513613, 3.49967337, -0.0168914795, 7.31451178, -13.8333397, 3.81765318, -2.79449153, 18.9408112, -18.930912, -11.2135153, 7.71239185, -2.86968422]> : tensor<16xf32>}> : () -> tensor<16xf32>\n",
            "  %1 = \"arith.constant\"() <{value = dense<[4.83980179, 7.01497889, 6.59778308, 7.4643445, 3.82265639, 6.78915548, 5.1570487, 4.63491488, 4.46355867, 6.7842741, 7.31219244, 4.33215332, 6.80388832, 6.5691595, 5.91440105, 4.81517363]> : tensor<16xf32>}> : () -> tensor<16xf32>\n",
            "  %2 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x32x16xf16>}> : () -> tensor<1x1x32x16xf16>\n",
            "  %3 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %4 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %5 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x32x1xf16>}> : () -> tensor<3x3x32x1xf16>\n",
            "  %6 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1280xf32>}> : () -> tensor<1280xf32>\n",
            "  %7 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1280xf32>}> : () -> tensor<1280xf32>\n",
            "  %8 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x320x1280xf16>}> : () -> tensor<1x1x320x1280xf16>\n",
            "  %9 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x3x32xf16>}> : () -> tensor<3x3x3x32xf16>\n",
            "  %10 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %11 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %12 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %13 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %14 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x384x64xf16>}> : () -> tensor<1x1x384x64xf16>\n",
            "  %15 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %16 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %17 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x64x384xf16>}> : () -> tensor<1x1x64x384xf16>\n",
            "  %18 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %19 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %20 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x384x1xf16>}> : () -> tensor<3x3x384x1xf16>\n",
            "  %21 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %22 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %23 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x384x64xf16>}> : () -> tensor<1x1x384x64xf16>\n",
            "  %24 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %25 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %26 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x64x384xf16>}> : () -> tensor<1x1x64x384xf16>\n",
            "  %27 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %28 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %29 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x384x1xf16>}> : () -> tensor<3x3x384x1xf16>\n",
            "  %30 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %31 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %32 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x384x64xf16>}> : () -> tensor<1x1x384x64xf16>\n",
            "  %33 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %34 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %35 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x64x384xf16>}> : () -> tensor<1x1x64x384xf16>\n",
            "  %36 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %37 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %38 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x384x1xf16>}> : () -> tensor<3x3x384x1xf16>\n",
            "  %39 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %40 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %41 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x192x64xf16>}> : () -> tensor<1x1x192x64xf16>\n",
            "  %42 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %43 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %44 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x32x192xf16>}> : () -> tensor<1x1x32x192xf16>\n",
            "  %45 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %46 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %47 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x192x1xf16>}> : () -> tensor<3x3x192x1xf16>\n",
            "  %48 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %49 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %50 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x192x32xf16>}> : () -> tensor<1x1x192x32xf16>\n",
            "  %51 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %52 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %53 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x32x192xf16>}> : () -> tensor<1x1x32x192xf16>\n",
            "  %54 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %55 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %56 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x192x1xf16>}> : () -> tensor<3x3x192x1xf16>\n",
            "  %57 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %58 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %59 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x192x32xf16>}> : () -> tensor<1x1x192x32xf16>\n",
            "  %60 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %61 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %62 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x32x192xf16>}> : () -> tensor<1x1x32x192xf16>\n",
            "  %63 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %64 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %65 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x192x1xf16>}> : () -> tensor<3x3x192x1xf16>\n",
            "  %66 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %67 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %68 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x144x32xf16>}> : () -> tensor<1x1x144x32xf16>\n",
            "  %69 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %70 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %71 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x24x144xf16>}> : () -> tensor<1x1x24x144xf16>\n",
            "  %72 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %73 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %74 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x144x1xf16>}> : () -> tensor<3x3x144x1xf16>\n",
            "  %75 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<24xf32>}> : () -> tensor<24xf32>\n",
            "  %76 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<24xf32>}> : () -> tensor<24xf32>\n",
            "  %77 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x144x24xf16>}> : () -> tensor<1x1x144x24xf16>\n",
            "  %78 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %79 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %80 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x24x144xf16>}> : () -> tensor<1x1x24x144xf16>\n",
            "  %81 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %82 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %83 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x144x1xf16>}> : () -> tensor<3x3x144x1xf16>\n",
            "  %84 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<24xf32>}> : () -> tensor<24xf32>\n",
            "  %85 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<24xf32>}> : () -> tensor<24xf32>\n",
            "  %86 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x96x24xf16>}> : () -> tensor<1x1x96x24xf16>\n",
            "  %87 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %88 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %89 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x16x96xf16>}> : () -> tensor<1x1x16x96xf16>\n",
            "  %90 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %91 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %92 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x96x1xf16>}> : () -> tensor<3x3x96x1xf16>\n",
            "  %93 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<320xf32>}> : () -> tensor<320xf32>\n",
            "  %94 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<320xf32>}> : () -> tensor<320xf32>\n",
            "  %95 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x960x320xf16>}> : () -> tensor<1x1x960x320xf16>\n",
            "  %96 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %97 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %98 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x160x960xf16>}> : () -> tensor<1x1x160x960xf16>\n",
            "  %99 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %100 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %101 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x960x1xf16>}> : () -> tensor<3x3x960x1xf16>\n",
            "  %102 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n",
            "  %103 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n",
            "  %104 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x960x160xf16>}> : () -> tensor<1x1x960x160xf16>\n",
            "  %105 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %106 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %107 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x160x960xf16>}> : () -> tensor<1x1x160x960xf16>\n",
            "  %108 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %109 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %110 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x960x1xf16>}> : () -> tensor<3x3x960x1xf16>\n",
            "  %111 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n",
            "  %112 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n",
            "  %113 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x960x160xf16>}> : () -> tensor<1x1x960x160xf16>\n",
            "  %114 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %115 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %116 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x160x960xf16>}> : () -> tensor<1x1x160x960xf16>\n",
            "  %117 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %118 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %119 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x960x1xf16>}> : () -> tensor<3x3x960x1xf16>\n",
            "  %120 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n",
            "  %121 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n",
            "  %122 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x576x160xf16>}> : () -> tensor<1x1x576x160xf16>\n",
            "  %123 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %124 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %125 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x96x576xf16>}> : () -> tensor<1x1x96x576xf16>\n",
            "  %126 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %127 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %128 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x576x1xf16>}> : () -> tensor<3x3x576x1xf16>\n",
            "  %129 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %130 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %131 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x576x96xf16>}> : () -> tensor<1x1x576x96xf16>\n",
            "  %132 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %133 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %134 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x96x576xf16>}> : () -> tensor<1x1x96x576xf16>\n",
            "  %135 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %136 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %137 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x576x1xf16>}> : () -> tensor<3x3x576x1xf16>\n",
            "  %138 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %139 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %140 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x576x96xf16>}> : () -> tensor<1x1x576x96xf16>\n",
            "  %141 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %142 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %143 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x96x576xf16>}> : () -> tensor<1x1x96x576xf16>\n",
            "  %144 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %145 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %146 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x576x1xf16>}> : () -> tensor<3x3x576x1xf16>\n",
            "  %147 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %148 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %149 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x384x96xf16>}> : () -> tensor<1x1x384x96xf16>\n",
            "  %150 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %151 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %152 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x64x384xf16>}> : () -> tensor<1x1x64x384xf16>\n",
            "  %153 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %154 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %155 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x384x1xf16>}> : () -> tensor<3x3x384x1xf16>\n",
            "  %156 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1024xf16>}> : () -> tensor<1024xf16>\n",
            "  %157 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<512xf16>}> : () -> tensor<512xf16>\n",
            "  %158 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1024xf32>}> : () -> tensor<1024xf32>\n",
            "  %159 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1024xf32>}> : () -> tensor<1024xf32>\n",
            "  %160 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<512xf32>}> : () -> tensor<512xf32>\n",
            "  %161 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<512xf32>}> : () -> tensor<512xf32>\n",
            "  %162 = \"arith.constant\"() <{value = dense<[[0, 0], [0, 1], [0, 1], [0, 0]]> : tensor<4x2xi32>}> : () -> tensor<4x2xi32>\n",
            "  %163 = \"arith.constant\"() <{value = dense<[1, 2]> : tensor<2xi32>}> : () -> tensor<2xi32>\n",
            "  %164 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<101xf32>}> : () -> tensor<101xf32>\n",
            "  %165 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1024x1280xf16>}> : () -> tensor<1024x1280xf16>\n",
            "  %166 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<512x1024xf16>}> : () -> tensor<512x1024xf16>\n",
            "  %167 = \"tfl.pseudo_qconst\"() <{qtype = tensor<101x512x!quant.uniform<i8<-127:127>:f32:0, {0.0035196880655964527,0.003034102869784738,0.0029790898946326547,0.0034422384003015955,0.00286281789381673,0.0030478196819936198,0.003764879515790564,0.003543796032432496,0.0031637364015804502,0.0027744364081405278,0.0024476149889427847,0.0031610408167200765,0.0029627478967501424,0.0039326790280229465,0.0031387458636066108,0.0034845112346288728,0.0030931189773589609,0.0032800393780385417,0.0036839767703859823,0.003220272815133643,0.0034213941397629385,0.0032939842836124692,0.003287345640302643,0.0029239199292941357,0.0033778398055729904,0.003882211728358832,0.0025053380981205015,0.0045974465805714527,0.0032684007498223013,0.0044137145590594437,0.0032357212126724365,0.0038919059310372419,0.0032753409363153411,0.0042975835912809597,0.0033498709126720278,0.0029546031801719365,0.0029587001781763993,0.0033694843607624684,0.0028530413240898311,0.0028612221789172314,0.0038846372150060701,0.0040789673647542634,0.0028956363520284338,0.0032025132122940904,0.0031092629188627708,0.0038356220159004991,0.0038850044640969103,0.0034358346556115338,0.0038642432745986098,0.0031052731622861125,0.003401375661684772,0.003270303874503909,0.0036903842696993368,0.0035367948333109456,0.0042598463418915518,0.0028427222112971029,0.0032642432085172399,0.0032351033424767922,0.0039963919346726788,0.0027833245401307355,0.0042847981603126827,0.0034589420153400092,0.0034374484396356296,0.003624631898609672,0.0037511497970641127,0.0031140409116669904,0.0033612079977050542,0.0022823880976579319,0.0045012498465109999,0.0043973964969004232,0.0035856248825553833,0.0037857871355972891,0.0026739720753797395,0.0034905364663582149,0.0034097167919939896,0.0039276145105286847,0.0034584543836398388,0.0035291572255412426,0.0031558035865543394,0.0032488952471515327,0.0030280698941448541,0.003758564474075798,0.0027166464197354054,0.0039795975046833668,0.0031718977792995181,0.0025550577584214098,0.0030911316083172173,0.0027391469384741595,0.0036072670005437898,0.0029970374633008102,0.004290158823719175,0.0056339633746409976,0.0033783023282298891,0.0030751214252682183,0.0038292652040015996,0.0030709946249413678,0.0032122611530183806,0.0037165678392245076,0.0034231632713257799,0.0027992281857437975,0.0032070337787387876}>>, value = dense_resource<__elided__> : tensor<101x512xi8>}> : () -> tensor<101x512x!quant.uniform<i8<-127:127>:f32:0, {0.0035196880655964527,0.003034102869784738,0.0029790898946326547,0.0034422384003015955,0.00286281789381673,0.0030478196819936198,0.003764879515790564,0.003543796032432496,0.0031637364015804502,0.0027744364081405278,0.0024476149889427847,0.0031610408167200765,0.0029627478967501424,0.0039326790280229465,0.0031387458636066108,0.0034845112346288728,0.0030931189773589609,0.0032800393780385417,0.0036839767703859823,0.003220272815133643,0.0034213941397629385,0.0032939842836124692,0.003287345640302643,0.0029239199292941357,0.0033778398055729904,0.003882211728358832,0.0025053380981205015,0.0045974465805714527,0.0032684007498223013,0.0044137145590594437,0.0032357212126724365,0.0038919059310372419,0.0032753409363153411,0.0042975835912809597,0.0033498709126720278,0.0029546031801719365,0.0029587001781763993,0.0033694843607624684,0.0028530413240898311,0.0028612221789172314,0.0038846372150060701,0.0040789673647542634,0.0028956363520284338,0.0032025132122940904,0.0031092629188627708,0.0038356220159004991,0.0038850044640969103,0.0034358346556115338,0.0038642432745986098,0.0031052731622861125,0.003401375661684772,0.003270303874503909,0.0036903842696993368,0.0035367948333109456,0.0042598463418915518,0.0028427222112971029,0.0032642432085172399,0.0032351033424767922,0.0039963919346726788,0.0027833245401307355,0.0042847981603126827,0.0034589420153400092,0.0034374484396356296,0.003624631898609672,0.0037511497970641127,0.0031140409116669904,0.0033612079977050542,0.0022823880976579319,0.0045012498465109999,0.0043973964969004232,0.0035856248825553833,0.0037857871355972891,0.0026739720753797395,0.0034905364663582149,0.0034097167919939896,0.0039276145105286847,0.0034584543836398388,0.0035291572255412426,0.0031558035865543394,0.0032488952471515327,0.0030280698941448541,0.003758564474075798,0.0027166464197354054,0.0039795975046833668,0.0031718977792995181,0.0025550577584214098,0.0030911316083172173,0.0027391469384741595,0.0036072670005437898,0.0029970374633008102,0.004290158823719175,0.0056339633746409976,0.0033783023282298891,0.0030751214252682183,0.0038292652040015996,0.0030709946249413678,0.0032122611530183806,0.0037165678392245076,0.0034231632713257799,0.0027992281857437975,0.0032070337787387876}>>\n",
            "  %168 = \"tfl.cast\"(%arg0) : (tensor<?x224x224x3xf32>) -> tensor<?x224x224x3xf16>\n",
            "  %169 = \"tfl.cast\"(%168) : (tensor<?x224x224x3xf16>) -> tensor<?x224x224x3xf32>\n",
            "  %170 = \"tfl.cast\"(%169) : (tensor<?x224x224x3xf32>) -> tensor<?x224x224x3xf16>\n",
            "  %171 = \"tf.Conv2D\"(%170, %9) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 2, 2, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x224x224x3xf16>, tensor<3x3x3x32xf16>) -> tensor<?x112x112x32xf16>\n",
            "  %172 = \"tfl.cast\"(%171) : (tensor<?x112x112x32xf16>) -> tensor<?x112x112x32xf32>\n",
            "  %173 = \"tfl.mul\"(%172, %11) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x32xf32>, tensor<32xf32>) -> tensor<?x112x112x32xf32>\n",
            "  %174 = \"tfl.add\"(%173, %10) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x32xf32>, tensor<32xf32>) -> tensor<?x112x112x32xf32>\n",
            "  %175 = \"tfl.cast\"(%174) : (tensor<?x112x112x32xf32>) -> tensor<?x112x112x32xf16>\n",
            "  %176 = \"tf.Relu6\"(%175) {device = \"\"} : (tensor<?x112x112x32xf16>) -> tensor<?x112x112x32xf16>\n",
            "  %177 = \"tf.DepthwiseConv2dNative\"(%176, %5) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x112x112x32xf16>, tensor<3x3x32x1xf16>) -> tensor<?x112x112x32xf16>\n",
            "  %178 = \"tfl.cast\"(%177) : (tensor<?x112x112x32xf16>) -> tensor<?x112x112x32xf32>\n",
            "  %179 = \"tfl.mul\"(%178, %4) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x32xf32>, tensor<32xf32>) -> tensor<?x112x112x32xf32>\n",
            "  %180 = \"tfl.add\"(%179, %3) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x32xf32>, tensor<32xf32>) -> tensor<?x112x112x32xf32>\n",
            "  %181 = \"tfl.cast\"(%180) : (tensor<?x112x112x32xf32>) -> tensor<?x112x112x32xf16>\n",
            "  %182 = \"tf.Relu6\"(%181) {device = \"\"} : (tensor<?x112x112x32xf16>) -> tensor<?x112x112x32xf16>\n",
            "  %183 = \"tf.Conv2D\"(%182, %2) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x112x112x32xf16>, tensor<1x1x32x16xf16>) -> tensor<?x112x112x16xf16>\n",
            "  %184 = \"tfl.cast\"(%183) : (tensor<?x112x112x16xf16>) -> tensor<?x112x112x16xf32>\n",
            "  %185 = \"tfl.mul\"(%184, %1) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x16xf32>, tensor<16xf32>) -> tensor<?x112x112x16xf32>\n",
            "  %186 = \"tfl.add\"(%185, %0) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x16xf32>, tensor<16xf32>) -> tensor<?x112x112x16xf32>\n",
            "  %187 = \"tfl.cast\"(%186) : (tensor<?x112x112x16xf32>) -> tensor<?x112x112x16xf16>\n",
            "  %188 = \"tf.Conv2D\"(%187, %89) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x112x112x16xf16>, tensor<1x1x16x96xf16>) -> tensor<?x112x112x96xf16>\n",
            "  %189 = \"tfl.cast\"(%188) : (tensor<?x112x112x96xf16>) -> tensor<?x112x112x96xf32>\n",
            "  %190 = \"tfl.mul\"(%189, %88) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x96xf32>, tensor<96xf32>) -> tensor<?x112x112x96xf32>\n",
            "  %191 = \"tfl.add\"(%190, %87) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x96xf32>, tensor<96xf32>) -> tensor<?x112x112x96xf32>\n",
            "  %192 = \"tfl.cast\"(%191) : (tensor<?x112x112x96xf32>) -> tensor<?x112x112x96xf16>\n",
            "  %193 = \"tf.Relu6\"(%192) {device = \"\"} : (tensor<?x112x112x96xf16>) -> tensor<?x112x112x96xf16>\n",
            "  %194 = \"tf.Pad\"(%193, %162) {device = \"\"} : (tensor<?x112x112x96xf16>, tensor<4x2xi32>) -> tensor<?x113x113x96xf16>\n",
            "  %195 = \"tf.DepthwiseConv2dNative\"(%194, %92) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x113x113x96xf16>, tensor<3x3x96x1xf16>) -> tensor<?x56x56x96xf16>\n",
            "  %196 = \"tfl.cast\"(%195) : (tensor<?x56x56x96xf16>) -> tensor<?x56x56x96xf32>\n",
            "  %197 = \"tfl.mul\"(%196, %91) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x96xf32>, tensor<96xf32>) -> tensor<?x56x56x96xf32>\n",
            "  %198 = \"tfl.add\"(%197, %90) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x96xf32>, tensor<96xf32>) -> tensor<?x56x56x96xf32>\n",
            "  %199 = \"tfl.cast\"(%198) : (tensor<?x56x56x96xf32>) -> tensor<?x56x56x96xf16>\n",
            "  %200 = \"tf.Relu6\"(%199) {device = \"\"} : (tensor<?x56x56x96xf16>) -> tensor<?x56x56x96xf16>\n",
            "  %201 = \"tf.Conv2D\"(%200, %86) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x96xf16>, tensor<1x1x96x24xf16>) -> tensor<?x56x56x24xf16>\n",
            "  %202 = \"tfl.cast\"(%201) : (tensor<?x56x56x24xf16>) -> tensor<?x56x56x24xf32>\n",
            "  %203 = \"tfl.mul\"(%202, %85) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x24xf32>, tensor<24xf32>) -> tensor<?x56x56x24xf32>\n",
            "  %204 = \"tfl.add\"(%203, %84) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x24xf32>, tensor<24xf32>) -> tensor<?x56x56x24xf32>\n",
            "  %205 = \"tfl.cast\"(%204) : (tensor<?x56x56x24xf32>) -> tensor<?x56x56x24xf16>\n",
            "  %206 = \"tf.Conv2D\"(%205, %80) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x24xf16>, tensor<1x1x24x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "  %207 = \"tfl.cast\"(%206) : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf32>\n",
            "  %208 = \"tfl.mul\"(%207, %79) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x144xf32>, tensor<144xf32>) -> tensor<?x56x56x144xf32>\n",
            "  %209 = \"tfl.add\"(%208, %78) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x144xf32>, tensor<144xf32>) -> tensor<?x56x56x144xf32>\n",
            "  %210 = \"tfl.cast\"(%209) : (tensor<?x56x56x144xf32>) -> tensor<?x56x56x144xf16>\n",
            "  %211 = \"tf.Relu6\"(%210) {device = \"\"} : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "  %212 = \"tf.DepthwiseConv2dNative\"(%211, %83) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x56x56x144xf16>, tensor<3x3x144x1xf16>) -> tensor<?x56x56x144xf16>\n",
            "  %213 = \"tfl.cast\"(%212) : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf32>\n",
            "  %214 = \"tfl.mul\"(%213, %82) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x144xf32>, tensor<144xf32>) -> tensor<?x56x56x144xf32>\n",
            "  %215 = \"tfl.add\"(%214, %81) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x144xf32>, tensor<144xf32>) -> tensor<?x56x56x144xf32>\n",
            "  %216 = \"tfl.cast\"(%215) : (tensor<?x56x56x144xf32>) -> tensor<?x56x56x144xf16>\n",
            "  %217 = \"tf.Relu6\"(%216) {device = \"\"} : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "  %218 = \"tf.Conv2D\"(%217, %77) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x144xf16>, tensor<1x1x144x24xf16>) -> tensor<?x56x56x24xf16>\n",
            "  %219 = \"tfl.cast\"(%218) : (tensor<?x56x56x24xf16>) -> tensor<?x56x56x24xf32>\n",
            "  %220 = \"tfl.mul\"(%219, %76) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x24xf32>, tensor<24xf32>) -> tensor<?x56x56x24xf32>\n",
            "  %221 = \"tfl.add\"(%220, %75) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x24xf32>, tensor<24xf32>) -> tensor<?x56x56x24xf32>\n",
            "  %222 = \"tfl.cast\"(%221) : (tensor<?x56x56x24xf32>) -> tensor<?x56x56x24xf16>\n",
            "  %223 = \"tf.AddV2\"(%205, %222) {device = \"\"} : (tensor<?x56x56x24xf16>, tensor<?x56x56x24xf16>) -> tensor<?x56x56x24xf16>\n",
            "  %224 = \"tf.Conv2D\"(%223, %71) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x24xf16>, tensor<1x1x24x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "  %225 = \"tfl.cast\"(%224) : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf32>\n",
            "  %226 = \"tfl.mul\"(%225, %70) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x144xf32>, tensor<144xf32>) -> tensor<?x56x56x144xf32>\n",
            "  %227 = \"tfl.add\"(%226, %69) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x144xf32>, tensor<144xf32>) -> tensor<?x56x56x144xf32>\n",
            "  %228 = \"tfl.cast\"(%227) : (tensor<?x56x56x144xf32>) -> tensor<?x56x56x144xf16>\n",
            "  %229 = \"tf.Relu6\"(%228) {device = \"\"} : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "  %230 = \"tf.Pad\"(%229, %162) {device = \"\"} : (tensor<?x56x56x144xf16>, tensor<4x2xi32>) -> tensor<?x57x57x144xf16>\n",
            "  %231 = \"tf.DepthwiseConv2dNative\"(%230, %74) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x57x57x144xf16>, tensor<3x3x144x1xf16>) -> tensor<?x28x28x144xf16>\n",
            "  %232 = \"tfl.cast\"(%231) : (tensor<?x28x28x144xf16>) -> tensor<?x28x28x144xf32>\n",
            "  %233 = \"tfl.mul\"(%232, %73) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x144xf32>, tensor<144xf32>) -> tensor<?x28x28x144xf32>\n",
            "  %234 = \"tfl.add\"(%233, %72) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x144xf32>, tensor<144xf32>) -> tensor<?x28x28x144xf32>\n",
            "  %235 = \"tfl.cast\"(%234) : (tensor<?x28x28x144xf32>) -> tensor<?x28x28x144xf16>\n",
            "  %236 = \"tf.Relu6\"(%235) {device = \"\"} : (tensor<?x28x28x144xf16>) -> tensor<?x28x28x144xf16>\n",
            "  %237 = \"tf.Conv2D\"(%236, %68) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x144xf16>, tensor<1x1x144x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "  %238 = \"tfl.cast\"(%237) : (tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf32>\n",
            "  %239 = \"tfl.mul\"(%238, %67) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x32xf32>, tensor<32xf32>) -> tensor<?x28x28x32xf32>\n",
            "  %240 = \"tfl.add\"(%239, %66) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x32xf32>, tensor<32xf32>) -> tensor<?x28x28x32xf32>\n",
            "  %241 = \"tfl.cast\"(%240) : (tensor<?x28x28x32xf32>) -> tensor<?x28x28x32xf16>\n",
            "  %242 = \"tf.Conv2D\"(%241, %62) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %243 = \"tfl.cast\"(%242) : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf32>\n",
            "  %244 = \"tfl.mul\"(%243, %61) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %245 = \"tfl.add\"(%244, %60) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %246 = \"tfl.cast\"(%245) : (tensor<?x28x28x192xf32>) -> tensor<?x28x28x192xf16>\n",
            "  %247 = \"tf.Relu6\"(%246) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %248 = \"tf.DepthwiseConv2dNative\"(%247, %65) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %249 = \"tfl.cast\"(%248) : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf32>\n",
            "  %250 = \"tfl.mul\"(%249, %64) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %251 = \"tfl.add\"(%250, %63) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %252 = \"tfl.cast\"(%251) : (tensor<?x28x28x192xf32>) -> tensor<?x28x28x192xf16>\n",
            "  %253 = \"tf.Relu6\"(%252) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %254 = \"tf.Conv2D\"(%253, %59) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<1x1x192x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "  %255 = \"tfl.cast\"(%254) : (tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf32>\n",
            "  %256 = \"tfl.mul\"(%255, %58) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x32xf32>, tensor<32xf32>) -> tensor<?x28x28x32xf32>\n",
            "  %257 = \"tfl.add\"(%256, %57) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x32xf32>, tensor<32xf32>) -> tensor<?x28x28x32xf32>\n",
            "  %258 = \"tfl.cast\"(%257) : (tensor<?x28x28x32xf32>) -> tensor<?x28x28x32xf16>\n",
            "  %259 = \"tf.AddV2\"(%241, %258) {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "  %260 = \"tf.Conv2D\"(%259, %53) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %261 = \"tfl.cast\"(%260) : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf32>\n",
            "  %262 = \"tfl.mul\"(%261, %52) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %263 = \"tfl.add\"(%262, %51) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %264 = \"tfl.cast\"(%263) : (tensor<?x28x28x192xf32>) -> tensor<?x28x28x192xf16>\n",
            "  %265 = \"tf.Relu6\"(%264) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %266 = \"tf.DepthwiseConv2dNative\"(%265, %56) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %267 = \"tfl.cast\"(%266) : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf32>\n",
            "  %268 = \"tfl.mul\"(%267, %55) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %269 = \"tfl.add\"(%268, %54) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %270 = \"tfl.cast\"(%269) : (tensor<?x28x28x192xf32>) -> tensor<?x28x28x192xf16>\n",
            "  %271 = \"tf.Relu6\"(%270) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %272 = \"tf.Conv2D\"(%271, %50) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<1x1x192x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "  %273 = \"tfl.cast\"(%272) : (tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf32>\n",
            "  %274 = \"tfl.mul\"(%273, %49) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x32xf32>, tensor<32xf32>) -> tensor<?x28x28x32xf32>\n",
            "  %275 = \"tfl.add\"(%274, %48) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x32xf32>, tensor<32xf32>) -> tensor<?x28x28x32xf32>\n",
            "  %276 = \"tfl.cast\"(%275) : (tensor<?x28x28x32xf32>) -> tensor<?x28x28x32xf16>\n",
            "  %277 = \"tf.AddV2\"(%259, %276) {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "  %278 = \"tf.Conv2D\"(%277, %44) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %279 = \"tfl.cast\"(%278) : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf32>\n",
            "  %280 = \"tfl.mul\"(%279, %43) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %281 = \"tfl.add\"(%280, %42) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %282 = \"tfl.cast\"(%281) : (tensor<?x28x28x192xf32>) -> tensor<?x28x28x192xf16>\n",
            "  %283 = \"tf.Relu6\"(%282) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %284 = \"tf.Pad\"(%283, %162) {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<4x2xi32>) -> tensor<?x29x29x192xf16>\n",
            "  %285 = \"tf.DepthwiseConv2dNative\"(%284, %47) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x29x29x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x14x14x192xf16>\n",
            "  %286 = \"tfl.cast\"(%285) : (tensor<?x14x14x192xf16>) -> tensor<?x14x14x192xf32>\n",
            "  %287 = \"tfl.mul\"(%286, %46) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x192xf32>, tensor<192xf32>) -> tensor<?x14x14x192xf32>\n",
            "  %288 = \"tfl.add\"(%287, %45) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x192xf32>, tensor<192xf32>) -> tensor<?x14x14x192xf32>\n",
            "  %289 = \"tfl.cast\"(%288) : (tensor<?x14x14x192xf32>) -> tensor<?x14x14x192xf16>\n",
            "  %290 = \"tf.Relu6\"(%289) {device = \"\"} : (tensor<?x14x14x192xf16>) -> tensor<?x14x14x192xf16>\n",
            "  %291 = \"tf.Conv2D\"(%290, %41) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x192xf16>, tensor<1x1x192x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %292 = \"tfl.cast\"(%291) : (tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf32>\n",
            "  %293 = \"tfl.mul\"(%292, %40) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %294 = \"tfl.add\"(%293, %39) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %295 = \"tfl.cast\"(%294) : (tensor<?x14x14x64xf32>) -> tensor<?x14x14x64xf16>\n",
            "  %296 = \"tf.Conv2D\"(%295, %35) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %297 = \"tfl.cast\"(%296) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %298 = \"tfl.mul\"(%297, %34) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %299 = \"tfl.add\"(%298, %33) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %300 = \"tfl.cast\"(%299) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %301 = \"tf.Relu6\"(%300) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %302 = \"tf.DepthwiseConv2dNative\"(%301, %38) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %303 = \"tfl.cast\"(%302) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %304 = \"tfl.mul\"(%303, %37) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %305 = \"tfl.add\"(%304, %36) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %306 = \"tfl.cast\"(%305) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %307 = \"tf.Relu6\"(%306) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %308 = \"tf.Conv2D\"(%307, %32) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %309 = \"tfl.cast\"(%308) : (tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf32>\n",
            "  %310 = \"tfl.mul\"(%309, %31) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %311 = \"tfl.add\"(%310, %30) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %312 = \"tfl.cast\"(%311) : (tensor<?x14x14x64xf32>) -> tensor<?x14x14x64xf16>\n",
            "  %313 = \"tf.AddV2\"(%295, %312) {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %314 = \"tf.Conv2D\"(%313, %26) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %315 = \"tfl.cast\"(%314) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %316 = \"tfl.mul\"(%315, %25) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %317 = \"tfl.add\"(%316, %24) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %318 = \"tfl.cast\"(%317) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %319 = \"tf.Relu6\"(%318) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %320 = \"tf.DepthwiseConv2dNative\"(%319, %29) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %321 = \"tfl.cast\"(%320) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %322 = \"tfl.mul\"(%321, %28) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %323 = \"tfl.add\"(%322, %27) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %324 = \"tfl.cast\"(%323) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %325 = \"tf.Relu6\"(%324) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %326 = \"tf.Conv2D\"(%325, %23) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %327 = \"tfl.cast\"(%326) : (tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf32>\n",
            "  %328 = \"tfl.mul\"(%327, %22) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %329 = \"tfl.add\"(%328, %21) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %330 = \"tfl.cast\"(%329) : (tensor<?x14x14x64xf32>) -> tensor<?x14x14x64xf16>\n",
            "  %331 = \"tf.AddV2\"(%313, %330) {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %332 = \"tf.Conv2D\"(%331, %17) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %333 = \"tfl.cast\"(%332) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %334 = \"tfl.mul\"(%333, %16) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %335 = \"tfl.add\"(%334, %15) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %336 = \"tfl.cast\"(%335) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %337 = \"tf.Relu6\"(%336) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %338 = \"tf.DepthwiseConv2dNative\"(%337, %20) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %339 = \"tfl.cast\"(%338) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %340 = \"tfl.mul\"(%339, %19) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %341 = \"tfl.add\"(%340, %18) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %342 = \"tfl.cast\"(%341) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %343 = \"tf.Relu6\"(%342) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %344 = \"tf.Conv2D\"(%343, %14) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %345 = \"tfl.cast\"(%344) : (tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf32>\n",
            "  %346 = \"tfl.mul\"(%345, %13) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %347 = \"tfl.add\"(%346, %12) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %348 = \"tfl.cast\"(%347) : (tensor<?x14x14x64xf32>) -> tensor<?x14x14x64xf16>\n",
            "  %349 = \"tf.AddV2\"(%331, %348) {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %350 = \"tf.Conv2D\"(%349, %152) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %351 = \"tfl.cast\"(%350) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %352 = \"tfl.mul\"(%351, %151) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %353 = \"tfl.add\"(%352, %150) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %354 = \"tfl.cast\"(%353) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %355 = \"tf.Relu6\"(%354) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %356 = \"tf.DepthwiseConv2dNative\"(%355, %155) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %357 = \"tfl.cast\"(%356) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %358 = \"tfl.mul\"(%357, %154) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %359 = \"tfl.add\"(%358, %153) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %360 = \"tfl.cast\"(%359) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %361 = \"tf.Relu6\"(%360) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %362 = \"tf.Conv2D\"(%361, %149) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "  %363 = \"tfl.cast\"(%362) : (tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf32>\n",
            "  %364 = \"tfl.mul\"(%363, %148) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x96xf32>, tensor<96xf32>) -> tensor<?x14x14x96xf32>\n",
            "  %365 = \"tfl.add\"(%364, %147) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x96xf32>, tensor<96xf32>) -> tensor<?x14x14x96xf32>\n",
            "  %366 = \"tfl.cast\"(%365) : (tensor<?x14x14x96xf32>) -> tensor<?x14x14x96xf16>\n",
            "  %367 = \"tf.Conv2D\"(%366, %143) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %368 = \"tfl.cast\"(%367) : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf32>\n",
            "  %369 = \"tfl.mul\"(%368, %142) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %370 = \"tfl.add\"(%369, %141) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %371 = \"tfl.cast\"(%370) : (tensor<?x14x14x576xf32>) -> tensor<?x14x14x576xf16>\n",
            "  %372 = \"tf.Relu6\"(%371) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %373 = \"tf.DepthwiseConv2dNative\"(%372, %146) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %374 = \"tfl.cast\"(%373) : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf32>\n",
            "  %375 = \"tfl.mul\"(%374, %145) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %376 = \"tfl.add\"(%375, %144) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %377 = \"tfl.cast\"(%376) : (tensor<?x14x14x576xf32>) -> tensor<?x14x14x576xf16>\n",
            "  %378 = \"tf.Relu6\"(%377) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %379 = \"tf.Conv2D\"(%378, %140) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<1x1x576x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "  %380 = \"tfl.cast\"(%379) : (tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf32>\n",
            "  %381 = \"tfl.mul\"(%380, %139) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x96xf32>, tensor<96xf32>) -> tensor<?x14x14x96xf32>\n",
            "  %382 = \"tfl.add\"(%381, %138) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x96xf32>, tensor<96xf32>) -> tensor<?x14x14x96xf32>\n",
            "  %383 = \"tfl.cast\"(%382) : (tensor<?x14x14x96xf32>) -> tensor<?x14x14x96xf16>\n",
            "  %384 = \"tf.AddV2\"(%366, %383) {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "  %385 = \"tf.Conv2D\"(%384, %134) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %386 = \"tfl.cast\"(%385) : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf32>\n",
            "  %387 = \"tfl.mul\"(%386, %133) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %388 = \"tfl.add\"(%387, %132) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %389 = \"tfl.cast\"(%388) : (tensor<?x14x14x576xf32>) -> tensor<?x14x14x576xf16>\n",
            "  %390 = \"tf.Relu6\"(%389) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %391 = \"tf.DepthwiseConv2dNative\"(%390, %137) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %392 = \"tfl.cast\"(%391) : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf32>\n",
            "  %393 = \"tfl.mul\"(%392, %136) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %394 = \"tfl.add\"(%393, %135) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %395 = \"tfl.cast\"(%394) : (tensor<?x14x14x576xf32>) -> tensor<?x14x14x576xf16>\n",
            "  %396 = \"tf.Relu6\"(%395) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %397 = \"tf.Conv2D\"(%396, %131) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<1x1x576x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "  %398 = \"tfl.cast\"(%397) : (tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf32>\n",
            "  %399 = \"tfl.mul\"(%398, %130) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x96xf32>, tensor<96xf32>) -> tensor<?x14x14x96xf32>\n",
            "  %400 = \"tfl.add\"(%399, %129) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x96xf32>, tensor<96xf32>) -> tensor<?x14x14x96xf32>\n",
            "  %401 = \"tfl.cast\"(%400) : (tensor<?x14x14x96xf32>) -> tensor<?x14x14x96xf16>\n",
            "  %402 = \"tf.AddV2\"(%384, %401) {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "  %403 = \"tf.Conv2D\"(%402, %125) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %404 = \"tfl.cast\"(%403) : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf32>\n",
            "  %405 = \"tfl.mul\"(%404, %124) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %406 = \"tfl.add\"(%405, %123) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %407 = \"tfl.cast\"(%406) : (tensor<?x14x14x576xf32>) -> tensor<?x14x14x576xf16>\n",
            "  %408 = \"tf.Relu6\"(%407) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %409 = \"tf.Pad\"(%408, %162) {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<4x2xi32>) -> tensor<?x15x15x576xf16>\n",
            "  %410 = \"tf.DepthwiseConv2dNative\"(%409, %128) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x15x15x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x7x7x576xf16>\n",
            "  %411 = \"tfl.cast\"(%410) : (tensor<?x7x7x576xf16>) -> tensor<?x7x7x576xf32>\n",
            "  %412 = \"tfl.mul\"(%411, %127) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x576xf32>, tensor<576xf32>) -> tensor<?x7x7x576xf32>\n",
            "  %413 = \"tfl.add\"(%412, %126) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x576xf32>, tensor<576xf32>) -> tensor<?x7x7x576xf32>\n",
            "  %414 = \"tfl.cast\"(%413) : (tensor<?x7x7x576xf32>) -> tensor<?x7x7x576xf16>\n",
            "  %415 = \"tf.Relu6\"(%414) {device = \"\"} : (tensor<?x7x7x576xf16>) -> tensor<?x7x7x576xf16>\n",
            "  %416 = \"tf.Conv2D\"(%415, %122) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x576xf16>, tensor<1x1x576x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "  %417 = \"tfl.cast\"(%416) : (tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf32>\n",
            "  %418 = \"tfl.mul\"(%417, %121) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x160xf32>, tensor<160xf32>) -> tensor<?x7x7x160xf32>\n",
            "  %419 = \"tfl.add\"(%418, %120) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x160xf32>, tensor<160xf32>) -> tensor<?x7x7x160xf32>\n",
            "  %420 = \"tfl.cast\"(%419) : (tensor<?x7x7x160xf32>) -> tensor<?x7x7x160xf16>\n",
            "  %421 = \"tf.Conv2D\"(%420, %116) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %422 = \"tfl.cast\"(%421) : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf32>\n",
            "  %423 = \"tfl.mul\"(%422, %115) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %424 = \"tfl.add\"(%423, %114) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %425 = \"tfl.cast\"(%424) : (tensor<?x7x7x960xf32>) -> tensor<?x7x7x960xf16>\n",
            "  %426 = \"tf.Relu6\"(%425) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %427 = \"tf.DepthwiseConv2dNative\"(%426, %119) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %428 = \"tfl.cast\"(%427) : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf32>\n",
            "  %429 = \"tfl.mul\"(%428, %118) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %430 = \"tfl.add\"(%429, %117) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %431 = \"tfl.cast\"(%430) : (tensor<?x7x7x960xf32>) -> tensor<?x7x7x960xf16>\n",
            "  %432 = \"tf.Relu6\"(%431) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %433 = \"tf.Conv2D\"(%432, %113) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<1x1x960x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "  %434 = \"tfl.cast\"(%433) : (tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf32>\n",
            "  %435 = \"tfl.mul\"(%434, %112) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x160xf32>, tensor<160xf32>) -> tensor<?x7x7x160xf32>\n",
            "  %436 = \"tfl.add\"(%435, %111) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x160xf32>, tensor<160xf32>) -> tensor<?x7x7x160xf32>\n",
            "  %437 = \"tfl.cast\"(%436) : (tensor<?x7x7x160xf32>) -> tensor<?x7x7x160xf16>\n",
            "  %438 = \"tf.AddV2\"(%420, %437) {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "  %439 = \"tf.Conv2D\"(%438, %107) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %440 = \"tfl.cast\"(%439) : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf32>\n",
            "  %441 = \"tfl.mul\"(%440, %106) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %442 = \"tfl.add\"(%441, %105) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %443 = \"tfl.cast\"(%442) : (tensor<?x7x7x960xf32>) -> tensor<?x7x7x960xf16>\n",
            "  %444 = \"tf.Relu6\"(%443) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %445 = \"tf.DepthwiseConv2dNative\"(%444, %110) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %446 = \"tfl.cast\"(%445) : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf32>\n",
            "  %447 = \"tfl.mul\"(%446, %109) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %448 = \"tfl.add\"(%447, %108) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %449 = \"tfl.cast\"(%448) : (tensor<?x7x7x960xf32>) -> tensor<?x7x7x960xf16>\n",
            "  %450 = \"tf.Relu6\"(%449) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %451 = \"tf.Conv2D\"(%450, %104) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<1x1x960x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "  %452 = \"tfl.cast\"(%451) : (tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf32>\n",
            "  %453 = \"tfl.mul\"(%452, %103) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x160xf32>, tensor<160xf32>) -> tensor<?x7x7x160xf32>\n",
            "  %454 = \"tfl.add\"(%453, %102) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x160xf32>, tensor<160xf32>) -> tensor<?x7x7x160xf32>\n",
            "  %455 = \"tfl.cast\"(%454) : (tensor<?x7x7x160xf32>) -> tensor<?x7x7x160xf16>\n",
            "  %456 = \"tf.AddV2\"(%438, %455) {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "  %457 = \"tf.Conv2D\"(%456, %98) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %458 = \"tfl.cast\"(%457) : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf32>\n",
            "  %459 = \"tfl.mul\"(%458, %97) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %460 = \"tfl.add\"(%459, %96) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %461 = \"tfl.cast\"(%460) : (tensor<?x7x7x960xf32>) -> tensor<?x7x7x960xf16>\n",
            "  %462 = \"tf.Relu6\"(%461) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %463 = \"tf.DepthwiseConv2dNative\"(%462, %101) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %464 = \"tfl.cast\"(%463) : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf32>\n",
            "  %465 = \"tfl.mul\"(%464, %100) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %466 = \"tfl.add\"(%465, %99) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %467 = \"tfl.cast\"(%466) : (tensor<?x7x7x960xf32>) -> tensor<?x7x7x960xf16>\n",
            "  %468 = \"tf.Relu6\"(%467) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %469 = \"tf.Conv2D\"(%468, %95) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<1x1x960x320xf16>) -> tensor<?x7x7x320xf16>\n",
            "  %470 = \"tfl.cast\"(%469) : (tensor<?x7x7x320xf16>) -> tensor<?x7x7x320xf32>\n",
            "  %471 = \"tfl.mul\"(%470, %94) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x320xf32>, tensor<320xf32>) -> tensor<?x7x7x320xf32>\n",
            "  %472 = \"tfl.add\"(%471, %93) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x320xf32>, tensor<320xf32>) -> tensor<?x7x7x320xf32>\n",
            "  %473 = \"tfl.cast\"(%472) : (tensor<?x7x7x320xf32>) -> tensor<?x7x7x320xf16>\n",
            "  %474 = \"tf.Conv2D\"(%473, %8) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x320xf16>, tensor<1x1x320x1280xf16>) -> tensor<?x7x7x1280xf16>\n",
            "  %475 = \"tfl.cast\"(%474) : (tensor<?x7x7x1280xf16>) -> tensor<?x7x7x1280xf32>\n",
            "  %476 = \"tfl.mul\"(%475, %7) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x1280xf32>, tensor<1280xf32>) -> tensor<?x7x7x1280xf32>\n",
            "  %477 = \"tfl.add\"(%476, %6) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x1280xf32>, tensor<1280xf32>) -> tensor<?x7x7x1280xf32>\n",
            "  %478 = \"tfl.cast\"(%477) : (tensor<?x7x7x1280xf32>) -> tensor<?x7x7x1280xf16>\n",
            "  %479 = \"tf.Relu6\"(%478) {device = \"\"} : (tensor<?x7x7x1280xf16>) -> tensor<?x7x7x1280xf16>\n",
            "  %480 = \"tfl.cast\"(%479) : (tensor<?x7x7x1280xf16>) -> tensor<?x7x7x1280xf32>\n",
            "  %481 = \"tfl.mean\"(%480, %163) <{keep_dims = false}> : (tensor<?x7x7x1280xf32>, tensor<2xi32>) -> tensor<?x1280xf32>\n",
            "  %482 = \"tfl.cast\"(%481) : (tensor<?x1280xf32>) -> tensor<?x1280xf16>\n",
            "  %483 = \"tf.MatMul\"(%482, %165) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x1280xf16>, tensor<1024x1280xf16>) -> tensor<?x1024xf16>\n",
            "  %484 = \"tf.BiasAdd\"(%483, %156) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x1024xf16>, tensor<1024xf16>) -> tensor<?x1024xf16>\n",
            "  %485 = \"tfl.cast\"(%484) : (tensor<?x1024xf16>) -> tensor<?x1024xf32>\n",
            "  %486 = \"tfl.mul\"(%485, %159) <{fused_activation_function = \"NONE\"}> : (tensor<?x1024xf32>, tensor<1024xf32>) -> tensor<?x1024xf32>\n",
            "  %487 = \"tfl.add\"(%486, %158) <{fused_activation_function = \"NONE\"}> : (tensor<?x1024xf32>, tensor<1024xf32>) -> tensor<?x1024xf32>\n",
            "  %488 = \"tfl.cast\"(%487) : (tensor<?x1024xf32>) -> tensor<?x1024xf16>\n",
            "  %489 = \"tf.Relu\"(%488) {device = \"\"} : (tensor<?x1024xf16>) -> tensor<?x1024xf16>\n",
            "  %490 = \"tf.MatMul\"(%489, %166) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x1024xf16>, tensor<512x1024xf16>) -> tensor<?x512xf16>\n",
            "  %491 = \"tf.BiasAdd\"(%490, %157) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x512xf16>, tensor<512xf16>) -> tensor<?x512xf16>\n",
            "  %492 = \"tfl.cast\"(%491) : (tensor<?x512xf16>) -> tensor<?x512xf32>\n",
            "  %493 = \"tfl.mul\"(%492, %161) <{fused_activation_function = \"NONE\"}> : (tensor<?x512xf32>, tensor<512xf32>) -> tensor<?x512xf32>\n",
            "  %494 = \"tfl.add\"(%493, %160) <{fused_activation_function = \"NONE\"}> : (tensor<?x512xf32>, tensor<512xf32>) -> tensor<?x512xf32>\n",
            "  %495 = \"tfl.cast\"(%494) : (tensor<?x512xf32>) -> tensor<?x512xf16>\n",
            "  %496 = \"tf.Relu\"(%495) {device = \"\"} : (tensor<?x512xf16>) -> tensor<?x512xf16>\n",
            "  %497 = \"tfl.cast\"(%496) : (tensor<?x512xf16>) -> tensor<?x512xf32>\n",
            "  %498 = \"tfl.fully_connected\"(%497, %167, %164) <{asymmetric_quantize_inputs = true, fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"}> : (tensor<?x512xf32>, tensor<101x512x!quant.uniform<i8<-127:127>:f32:0, {0.0035196880655964527,0.003034102869784738,0.0029790898946326547,0.0034422384003015955,0.00286281789381673,0.0030478196819936198,0.003764879515790564,0.003543796032432496,0.0031637364015804502,0.0027744364081405278,0.0024476149889427847,0.0031610408167200765,0.0029627478967501424,0.0039326790280229465,0.0031387458636066108,0.0034845112346288728,0.0030931189773589609,0.0032800393780385417,0.0036839767703859823,0.003220272815133643,0.0034213941397629385,0.0032939842836124692,0.003287345640302643,0.0029239199292941357,0.0033778398055729904,0.003882211728358832,0.0025053380981205015,0.0045974465805714527,0.0032684007498223013,0.0044137145590594437,0.0032357212126724365,0.0038919059310372419,0.0032753409363153411,0.0042975835912809597,0.0033498709126720278,0.0029546031801719365,0.0029587001781763993,0.0033694843607624684,0.0028530413240898311,0.0028612221789172314,0.0038846372150060701,0.0040789673647542634,0.0028956363520284338,0.0032025132122940904,0.0031092629188627708,0.0038356220159004991,0.0038850044640969103,0.0034358346556115338,0.0038642432745986098,0.0031052731622861125,0.003401375661684772,0.003270303874503909,0.0036903842696993368,0.0035367948333109456,0.0042598463418915518,0.0028427222112971029,0.0032642432085172399,0.0032351033424767922,0.0039963919346726788,0.0027833245401307355,0.0042847981603126827,0.0034589420153400092,0.0034374484396356296,0.003624631898609672,0.0037511497970641127,0.0031140409116669904,0.0033612079977050542,0.0022823880976579319,0.0045012498465109999,0.0043973964969004232,0.0035856248825553833,0.0037857871355972891,0.0026739720753797395,0.0034905364663582149,0.0034097167919939896,0.0039276145105286847,0.0034584543836398388,0.0035291572255412426,0.0031558035865543394,0.0032488952471515327,0.0030280698941448541,0.003758564474075798,0.0027166464197354054,0.0039795975046833668,0.0031718977792995181,0.0025550577584214098,0.0030911316083172173,0.0027391469384741595,0.0036072670005437898,0.0029970374633008102,0.004290158823719175,0.0056339633746409976,0.0033783023282298891,0.0030751214252682183,0.0038292652040015996,0.0030709946249413678,0.0032122611530183806,0.0037165678392245076,0.0034231632713257799,0.0027992281857437975,0.0032070337787387876}>>, tensor<101xf32>) -> tensor<?x101xf32>\n",
            "  %499 = \"tfl.softmax\"(%498) <{beta = 1.000000e+00 : f32}> : (tensor<?x101xf32>) -> tensor<?x101xf32>\n",
            "  \"func.return\"(%499) : (tensor<?x101xf32>) -> ()\n",
            "}) {tf.entry_function = {control_outputs = \"\", inputs = \"serving_default_keras_tensor_155:0\", outputs = \"StatefulPartitionedCall_1:0\"}, tf_saved_model.exported_names = [\"serving_default\"]} : () -> ()\n",
            "\n",
            "\n",
            "ğŸ“Š Strategy 2: Using representative dataset...\n",
            "Saved artifact at '/tmp/tmp6afdt6g8'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_155')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 101), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  137187261267216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256713680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256714640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187261267408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187261267792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187261267600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256713488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256714448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256715792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256714256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256716368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256717904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256716560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256717712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256716752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256719632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256720976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256721360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256721168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256720016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256722512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256722128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256722320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256722704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256720592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256724816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256725200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256725584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256725392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256720208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256726736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256727120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256727504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256727312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256723664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256728656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256728272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256727888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256729232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256724432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916958480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916959824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916960208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916960016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916958672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916959248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916961936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916962320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916962128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916958288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916963472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916963856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916964240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916964048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916960976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916965392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916965776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916966160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916965968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916961552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916967312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916967696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916968080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916967888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916963088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916969232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916969616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916970000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916969808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916965008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916971152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916971536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916971920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916971728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916966928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916973072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916973456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916974224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916973648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916968848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916972304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917351504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917352656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916972688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917352464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917353808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917354192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917354576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917354384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917352272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917355728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917356112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917356496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917356304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917351696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917357648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917358032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917358416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917358224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917353424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917359568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917359952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917360336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917360144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917355344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917361488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917361872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917362256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917362064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917357264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917363408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917363792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917364176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917363984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917359184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917365328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917365712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917366096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917365904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917361104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917367248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917351888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917367632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917367440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917364944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917876176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917877520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917877904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917877712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917876368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917879056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917879440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917879824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917879632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917876944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917880976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917881360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917881744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917881552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917877136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917882896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917883280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917883664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917883472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917878672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917884816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917885200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917885584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917885392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917880592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917886736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917887120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917887504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917887312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917882512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917888656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917889040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917889424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917889232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917884432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917890576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917890960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917891728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917891152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917886352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917889808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918269968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918269008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917890192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918269200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918271312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918271696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918272080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918271888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918269392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918273232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918273616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918274000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918273808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918269776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918275152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918275536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918275920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918275728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918270928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918277072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918277456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918277840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918277648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918272848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918278992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918279376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918279760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918279568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918274768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918280912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918281296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918281680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918281488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918276688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918282832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918283216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918283600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918283408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918278608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918284752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918270160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918285136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918284944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918282448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918647184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918647568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918647952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918647760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918646032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918649104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918649488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918649872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918649680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918645840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918651024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918651408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918651792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918651600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918646608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918652944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918653328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918653712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918653520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918648720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918654864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918655248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918655632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918655440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918650640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918656784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918657168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918657552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918657360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918652560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918658704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918659088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918659472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918659280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918654480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918660624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918661008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918661776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918661200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918656400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918659856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251487760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251486800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918660240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251486992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251489104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251489488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251489872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251487952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251487184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251492176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251493520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251493712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251492944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251488336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251493328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251492752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251496208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251496400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251495632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251494480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251496016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251495440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251498896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âŒ Strategy 2 failed: Could not translate MLIR to FlatBuffer.<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %171 = \"tf.Conv2D\"(%170, %158) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 2, 2, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x224x224x3xf16>, tensor<3x3x3x32xf16>) -> tensor<?x112x112x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %176 = \"tf.Relu6\"(%175) {device = \"\"} : (tensor<?x112x112x32xf16>) -> tensor<?x112x112x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %177 = \"tf.DepthwiseConv2dNative\"(%176, %162) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x112x112x32xf16>, tensor<3x3x32x1xf16>) -> tensor<?x112x112x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %182 = \"tf.Relu6\"(%181) {device = \"\"} : (tensor<?x112x112x32xf16>) -> tensor<?x112x112x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %183 = \"tf.Conv2D\"(%182, %165) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x112x112x32xf16>, tensor<1x1x32x16xf16>) -> tensor<?x112x112x16xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %188 = \"tf.Conv2D\"(%187, %78) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x112x112x16xf16>, tensor<1x1x16x96xf16>) -> tensor<?x112x112x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %193 = \"tf.Relu6\"(%192) {device = \"\"} : (tensor<?x112x112x96xf16>) -> tensor<?x112x112x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_pad_1/Pad@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Pad' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_pad_1/Pad@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %194 = \"tf.Pad\"(%193, %5) {device = \"\"} : (tensor<?x112x112x96xf16>, tensor<4x2xi32>) -> tensor<?x113x113x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_pad_1/Pad@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %195 = \"tf.DepthwiseConv2dNative\"(%194, %75) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x113x113x96xf16>, tensor<3x3x96x1xf16>) -> tensor<?x56x56x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %200 = \"tf.Relu6\"(%199) {device = \"\"} : (tensor<?x56x56x96xf16>) -> tensor<?x56x56x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %201 = \"tf.Conv2D\"(%200, %81) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x96xf16>, tensor<1x1x96x24xf16>) -> tensor<?x56x56x24xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %206 = \"tf.Conv2D\"(%205, %87) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x24xf16>, tensor<1x1x24x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %211 = \"tf.Relu6\"(%210) {device = \"\"} : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %212 = \"tf.DepthwiseConv2dNative\"(%211, %84) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x56x56x144xf16>, tensor<3x3x144x1xf16>) -> tensor<?x56x56x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %217 = \"tf.Relu6\"(%216) {device = \"\"} : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %218 = \"tf.Conv2D\"(%217, %90) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x144xf16>, tensor<1x1x144x24xf16>) -> tensor<?x56x56x24xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %223 = \"tf.AddV2\"(%205, %222) {device = \"\"} : (tensor<?x56x56x24xf16>, tensor<?x56x56x24xf16>) -> tensor<?x56x56x24xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %224 = \"tf.Conv2D\"(%223, %96) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x24xf16>, tensor<1x1x24x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %229 = \"tf.Relu6\"(%228) {device = \"\"} : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_pad_1/Pad@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Pad' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_pad_1/Pad@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %230 = \"tf.Pad\"(%229, %5) {device = \"\"} : (tensor<?x56x56x144xf16>, tensor<4x2xi32>) -> tensor<?x57x57x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_pad_1/Pad@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %231 = \"tf.DepthwiseConv2dNative\"(%230, %93) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x57x57x144xf16>, tensor<3x3x144x1xf16>) -> tensor<?x28x28x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %236 = \"tf.Relu6\"(%235) {device = \"\"} : (tensor<?x28x28x144xf16>) -> tensor<?x28x28x144xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %237 = \"tf.Conv2D\"(%236, %99) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x144xf16>, tensor<1x1x144x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %242 = \"tf.Conv2D\"(%241, %105) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %247 = \"tf.Relu6\"(%246) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %248 = \"tf.DepthwiseConv2dNative\"(%247, %102) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %253 = \"tf.Relu6\"(%252) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %254 = \"tf.Conv2D\"(%253, %108) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<1x1x192x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %259 = \"tf.AddV2\"(%241, %258) {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %260 = \"tf.Conv2D\"(%259, %114) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %265 = \"tf.Relu6\"(%264) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %266 = \"tf.DepthwiseConv2dNative\"(%265, %111) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %271 = \"tf.Relu6\"(%270) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %272 = \"tf.Conv2D\"(%271, %117) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<1x1x192x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %277 = \"tf.AddV2\"(%259, %276) {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %278 = \"tf.Conv2D\"(%277, %123) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %283 = \"tf.Relu6\"(%282) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_pad_1/Pad@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Pad' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_pad_1/Pad@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %284 = \"tf.Pad\"(%283, %5) {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<4x2xi32>) -> tensor<?x29x29x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_pad_1/Pad@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %285 = \"tf.DepthwiseConv2dNative\"(%284, %120) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x29x29x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x14x14x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %290 = \"tf.Relu6\"(%289) {device = \"\"} : (tensor<?x14x14x192xf16>) -> tensor<?x14x14x192xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %291 = \"tf.Conv2D\"(%290, %126) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x192xf16>, tensor<1x1x192x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %296 = \"tf.Conv2D\"(%295, %132) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %301 = \"tf.Relu6\"(%300) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %302 = \"tf.DepthwiseConv2dNative\"(%301, %129) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %307 = \"tf.Relu6\"(%306) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %308 = \"tf.Conv2D\"(%307, %135) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %313 = \"tf.AddV2\"(%295, %312) {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %314 = \"tf.Conv2D\"(%313, %141) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %319 = \"tf.Relu6\"(%318) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %320 = \"tf.DepthwiseConv2dNative\"(%319, %138) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %325 = \"tf.Relu6\"(%324) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %326 = \"tf.Conv2D\"(%325, %144) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %331 = \"tf.AddV2\"(%313, %330) {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %332 = \"tf.Conv2D\"(%331, %150) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %337 = \"tf.Relu6\"(%336) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %338 = \"tf.DepthwiseConv2dNative\"(%337, %147) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %343 = \"tf.Relu6\"(%342) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %344 = \"tf.Conv2D\"(%343, %153) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %349 = \"tf.AddV2\"(%331, %348) {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %350 = \"tf.Conv2D\"(%349, %15) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %355 = \"tf.Relu6\"(%354) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %356 = \"tf.DepthwiseConv2dNative\"(%355, %12) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %361 = \"tf.Relu6\"(%360) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %362 = \"tf.Conv2D\"(%361, %18) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %367 = \"tf.Conv2D\"(%366, %24) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %372 = \"tf.Relu6\"(%371) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %373 = \"tf.DepthwiseConv2dNative\"(%372, %21) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %378 = \"tf.Relu6\"(%377) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %379 = \"tf.Conv2D\"(%378, %27) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<1x1x576x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %384 = \"tf.AddV2\"(%366, %383) {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %385 = \"tf.Conv2D\"(%384, %33) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %390 = \"tf.Relu6\"(%389) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %391 = \"tf.DepthwiseConv2dNative\"(%390, %30) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %396 = \"tf.Relu6\"(%395) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %397 = \"tf.Conv2D\"(%396, %36) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<1x1x576x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %402 = \"tf.AddV2\"(%384, %401) {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %403 = \"tf.Conv2D\"(%402, %42) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %408 = \"tf.Relu6\"(%407) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_pad_1/Pad@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Pad' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_pad_1/Pad@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %409 = \"tf.Pad\"(%408, %5) {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<4x2xi32>) -> tensor<?x15x15x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_pad_1/Pad@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %410 = \"tf.DepthwiseConv2dNative\"(%409, %39) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x15x15x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x7x7x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %415 = \"tf.Relu6\"(%414) {device = \"\"} : (tensor<?x7x7x576xf16>) -> tensor<?x7x7x576xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %416 = \"tf.Conv2D\"(%415, %45) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x576xf16>, tensor<1x1x576x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %421 = \"tf.Conv2D\"(%420, %51) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %426 = \"tf.Relu6\"(%425) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %427 = \"tf.DepthwiseConv2dNative\"(%426, %48) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %432 = \"tf.Relu6\"(%431) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %433 = \"tf.Conv2D\"(%432, %54) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<1x1x960x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %438 = \"tf.AddV2\"(%420, %437) {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %439 = \"tf.Conv2D\"(%438, %60) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %444 = \"tf.Relu6\"(%443) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %445 = \"tf.DepthwiseConv2dNative\"(%444, %57) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %450 = \"tf.Relu6\"(%449) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %451 = \"tf.Conv2D\"(%450, %63) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<1x1x960x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %456 = \"tf.AddV2\"(%438, %455) {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_add_1/Add@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %457 = \"tf.Conv2D\"(%456, %69) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %462 = \"tf.Relu6\"(%461) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %463 = \"tf.DepthwiseConv2dNative\"(%462, %66) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_1/depthwise@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %468 = \"tf.Relu6\"(%467) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %469 = \"tf.Conv2D\"(%468, %72) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<1x1x960x320xf16>) -> tensor<?x7x7x320xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_project_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv_1_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv_1_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %474 = \"tf.Conv2D\"(%473, %159) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x320xf16>, tensor<1x1x320x1280xf16>) -> tensor<?x7x7x1280xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv_1_1/convolution@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/out_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/out_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %479 = \"tf.Relu6\"(%478) {device = \"\"} : (tensor<?x7x7x1280xf16>) -> tensor<?x7x7x1280xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/out_relu_1/Relu6@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1/MatMul@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.MatMul' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1/MatMul@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %483 = \"tf.MatMul\"(%482, %2) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x1280xf16>, tensor<1024x1280xf16>) -> tensor<?x1024xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1/MatMul@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1/BiasAdd@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.BiasAdd' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1/BiasAdd@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %484 = \"tf.BiasAdd\"(%483, %11) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x1024xf16>, tensor<1024xf16>) -> tensor<?x1024xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1/BiasAdd@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/Relu@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/Relu@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %489 = \"tf.Relu\"(%488) {device = \"\"} : (tensor<?x1024xf16>) -> tensor<?x1024xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/Relu@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1_2/MatMul@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.MatMul' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1_2/MatMul@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %490 = \"tf.MatMul\"(%489, %1) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x1024xf16>, tensor<512x1024xf16>) -> tensor<?x512xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1_2/MatMul@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1_2/BiasAdd@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.BiasAdd' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1_2/BiasAdd@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %491 = \"tf.BiasAdd\"(%490, %10) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x512xf16>, tensor<512xf16>) -> tensor<?x512xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1_2/BiasAdd@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/Relu_1@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu' op is neither a custom op nor a flex op\n",
            "<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/Relu_1@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %496 = \"tf.Relu\"(%495) {device = \"\"} : (tensor<?x512xf16>) -> tensor<?x512xf16>\n",
            "<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/Relu_1@__inference_function_647594\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_648701\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n",
            "<unknown>:0: error: failed while converting: 'main': \n",
            "Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \n",
            "TF Select ops: AddV2, BiasAdd, Conv2D, DepthwiseConv2dNative, MatMul, Pad, Relu, Relu6\n",
            "Details:\n",
            "\ttf.AddV2(tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> (tensor<?x14x14x64xf16>) : {device = \"\"}\n",
            "\ttf.AddV2(tensor<?x14x14x96xf16>, tensor<?x14x14x96xf16>) -> (tensor<?x14x14x96xf16>) : {device = \"\"}\n",
            "\ttf.AddV2(tensor<?x28x28x32xf16>, tensor<?x28x28x32xf16>) -> (tensor<?x28x28x32xf16>) : {device = \"\"}\n",
            "\ttf.AddV2(tensor<?x56x56x24xf16>, tensor<?x56x56x24xf16>) -> (tensor<?x56x56x24xf16>) : {device = \"\"}\n",
            "\ttf.AddV2(tensor<?x7x7x160xf16>, tensor<?x7x7x160xf16>) -> (tensor<?x7x7x160xf16>) : {device = \"\"}\n",
            "\ttf.BiasAdd(tensor<?x1024xf16>, tensor<1024xf16>) -> (tensor<?x1024xf16>) : {data_format = \"NHWC\", device = \"\"}\n",
            "\ttf.BiasAdd(tensor<?x512xf16>, tensor<512xf16>) -> (tensor<?x512xf16>) : {data_format = \"NHWC\", device = \"\"}\n",
            "\ttf.Conv2D(tensor<?x112x112x16xf16>, tensor<1x1x16x96xf16>) -> (tensor<?x112x112x96xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x112x112x32xf16>, tensor<1x1x32x16xf16>) -> (tensor<?x112x112x16xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x14x14x192xf16>, tensor<1x1x192x64xf16>) -> (tensor<?x14x14x64xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> (tensor<?x14x14x64xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x14x14x384xf16>, tensor<1x1x384x96xf16>) -> (tensor<?x14x14x96xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x14x14x576xf16>, tensor<1x1x576x96xf16>) -> (tensor<?x14x14x96xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> (tensor<?x14x14x384xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> (tensor<?x14x14x576xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x224x224x3xf16>, tensor<3x3x3x32xf16>) -> (tensor<?x112x112x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 2, 2, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x28x28x144xf16>, tensor<1x1x144x32xf16>) -> (tensor<?x28x28x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x28x28x192xf16>, tensor<1x1x192x32xf16>) -> (tensor<?x28x28x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> (tensor<?x28x28x192xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x56x56x144xf16>, tensor<1x1x144x24xf16>) -> (tensor<?x56x56x24xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x56x56x24xf16>, tensor<1x1x24x144xf16>) -> (tensor<?x56x56x144xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x56x56x96xf16>, tensor<1x1x96x24xf16>) -> (tensor<?x56x56x24xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> (tensor<?x7x7x960xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x7x7x320xf16>, tensor<1x1x320x1280xf16>) -> (tensor<?x7x7x1280xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x7x7x576xf16>, tensor<1x1x576x160xf16>) -> (tensor<?x7x7x160xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x7x7x960xf16>, tensor<1x1x960x160xf16>) -> (tensor<?x7x7x160xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<?x7x7x960xf16>, tensor<1x1x960x320xf16>) -> (tensor<?x7x7x320xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x112x112x32xf16>, tensor<3x3x32x1xf16>) -> (tensor<?x112x112x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x113x113x96xf16>, tensor<3x3x96x1xf16>) -> (tensor<?x56x56x96xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> (tensor<?x14x14x384xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x14x14x576xf16>, tensor<3x3x576x1xf16>) -> (tensor<?x14x14x576xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x15x15x576xf16>, tensor<3x3x576x1xf16>) -> (tensor<?x7x7x576xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x28x28x192xf16>, tensor<3x3x192x1xf16>) -> (tensor<?x28x28x192xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x29x29x192xf16>, tensor<3x3x192x1xf16>) -> (tensor<?x14x14x192xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x56x56x144xf16>, tensor<3x3x144x1xf16>) -> (tensor<?x56x56x144xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x57x57x144xf16>, tensor<3x3x144x1xf16>) -> (tensor<?x28x28x144xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}\n",
            "\ttf.DepthwiseConv2dNative(tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> (tensor<?x7x7x960xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n",
            "\ttf.MatMul(tensor<?x1024xf16>, tensor<512x1024xf16>) -> (tensor<?x512xf16>) : {grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}\n",
            "\ttf.MatMul(tensor<?x1280xf16>, tensor<1024x1280xf16>) -> (tensor<?x1024xf16>) : {grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}\n",
            "\ttf.Pad(tensor<?x112x112x96xf16>, tensor<4x2xi32>) -> (tensor<?x113x113x96xf16>) : {device = \"\"}\n",
            "\ttf.Pad(tensor<?x14x14x576xf16>, tensor<4x2xi32>) -> (tensor<?x15x15x576xf16>) : {device = \"\"}\n",
            "\ttf.Pad(tensor<?x28x28x192xf16>, tensor<4x2xi32>) -> (tensor<?x29x29x192xf16>) : {device = \"\"}\n",
            "\ttf.Pad(tensor<?x56x56x144xf16>, tensor<4x2xi32>) -> (tensor<?x57x57x144xf16>) : {device = \"\"}\n",
            "\ttf.Relu(tensor<?x1024xf16>) -> (tensor<?x1024xf16>) : {device = \"\"}\n",
            "\ttf.Relu(tensor<?x512xf16>) -> (tensor<?x512xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x112x112x32xf16>) -> (tensor<?x112x112x32xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x112x112x96xf16>) -> (tensor<?x112x112x96xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x14x14x192xf16>) -> (tensor<?x14x14x192xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x14x14x384xf16>) -> (tensor<?x14x14x384xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x14x14x576xf16>) -> (tensor<?x14x14x576xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x28x28x144xf16>) -> (tensor<?x28x28x144xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x28x28x192xf16>) -> (tensor<?x28x28x192xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x56x56x144xf16>) -> (tensor<?x56x56x144xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x56x56x96xf16>) -> (tensor<?x56x56x96xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x7x7x1280xf16>) -> (tensor<?x7x7x1280xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x7x7x576xf16>) -> (tensor<?x7x7x576xf16>) : {device = \"\"}\n",
            "\ttf.Relu6(tensor<?x7x7x960xf16>) -> (tensor<?x7x7x960xf16>) : {device = \"\"}\n",
            "\n",
            "<unknown>:0: note: see current operation: \n",
            "\"func.func\"() <{arg_attrs = [{tf_saved_model.index_path = [\"keras_tensor_155\"]}], function_type = (tensor<?x224x224x3xf32>) -> tensor<?x101xf32>, res_attrs = [{tf_saved_model.index_path = [\"output_0\"]}], sym_name = \"main\"}> ({\n",
            "^bb0(%arg0: tensor<?x224x224x3xf32>):\n",
            "  %0 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<101x512xf32>}> : () -> tensor<101x512xf32>\n",
            "  %1 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<512x1024xf16>}> : () -> tensor<512x1024xf16>\n",
            "  %2 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1024x1280xf16>}> : () -> tensor<1024x1280xf16>\n",
            "  %3 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<101xf32>}> : () -> tensor<101xf32>\n",
            "  %4 = \"arith.constant\"() <{value = dense<[1, 2]> : tensor<2xi32>}> : () -> tensor<2xi32>\n",
            "  %5 = \"arith.constant\"() <{value = dense<[[0, 0], [0, 1], [0, 1], [0, 0]]> : tensor<4x2xi32>}> : () -> tensor<4x2xi32>\n",
            "  %6 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<512xf32>}> : () -> tensor<512xf32>\n",
            "  %7 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<512xf32>}> : () -> tensor<512xf32>\n",
            "  %8 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1024xf32>}> : () -> tensor<1024xf32>\n",
            "  %9 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1024xf32>}> : () -> tensor<1024xf32>\n",
            "  %10 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<512xf16>}> : () -> tensor<512xf16>\n",
            "  %11 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1024xf16>}> : () -> tensor<1024xf16>\n",
            "  %12 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x384x1xf16>}> : () -> tensor<3x3x384x1xf16>\n",
            "  %13 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %14 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %15 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x64x384xf16>}> : () -> tensor<1x1x64x384xf16>\n",
            "  %16 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %17 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %18 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x384x96xf16>}> : () -> tensor<1x1x384x96xf16>\n",
            "  %19 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %20 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %21 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x576x1xf16>}> : () -> tensor<3x3x576x1xf16>\n",
            "  %22 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %23 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %24 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x96x576xf16>}> : () -> tensor<1x1x96x576xf16>\n",
            "  %25 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %26 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %27 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x576x96xf16>}> : () -> tensor<1x1x576x96xf16>\n",
            "  %28 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %29 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %30 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x576x1xf16>}> : () -> tensor<3x3x576x1xf16>\n",
            "  %31 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %32 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %33 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x96x576xf16>}> : () -> tensor<1x1x96x576xf16>\n",
            "  %34 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %35 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %36 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x576x96xf16>}> : () -> tensor<1x1x576x96xf16>\n",
            "  %37 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %38 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %39 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x576x1xf16>}> : () -> tensor<3x3x576x1xf16>\n",
            "  %40 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %41 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %42 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x96x576xf16>}> : () -> tensor<1x1x96x576xf16>\n",
            "  %43 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %44 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n",
            "  %45 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x576x160xf16>}> : () -> tensor<1x1x576x160xf16>\n",
            "  %46 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n",
            "  %47 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n",
            "  %48 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x960x1xf16>}> : () -> tensor<3x3x960x1xf16>\n",
            "  %49 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %50 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %51 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x160x960xf16>}> : () -> tensor<1x1x160x960xf16>\n",
            "  %52 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %53 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %54 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x960x160xf16>}> : () -> tensor<1x1x960x160xf16>\n",
            "  %55 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n",
            "  %56 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n",
            "  %57 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x960x1xf16>}> : () -> tensor<3x3x960x1xf16>\n",
            "  %58 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %59 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %60 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x160x960xf16>}> : () -> tensor<1x1x160x960xf16>\n",
            "  %61 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %62 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %63 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x960x160xf16>}> : () -> tensor<1x1x960x160xf16>\n",
            "  %64 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n",
            "  %65 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n",
            "  %66 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x960x1xf16>}> : () -> tensor<3x3x960x1xf16>\n",
            "  %67 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %68 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %69 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x160x960xf16>}> : () -> tensor<1x1x160x960xf16>\n",
            "  %70 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %71 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n",
            "  %72 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x960x320xf16>}> : () -> tensor<1x1x960x320xf16>\n",
            "  %73 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<320xf32>}> : () -> tensor<320xf32>\n",
            "  %74 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<320xf32>}> : () -> tensor<320xf32>\n",
            "  %75 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x96x1xf16>}> : () -> tensor<3x3x96x1xf16>\n",
            "  %76 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %77 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %78 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x16x96xf16>}> : () -> tensor<1x1x16x96xf16>\n",
            "  %79 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %80 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n",
            "  %81 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x96x24xf16>}> : () -> tensor<1x1x96x24xf16>\n",
            "  %82 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<24xf32>}> : () -> tensor<24xf32>\n",
            "  %83 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<24xf32>}> : () -> tensor<24xf32>\n",
            "  %84 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x144x1xf16>}> : () -> tensor<3x3x144x1xf16>\n",
            "  %85 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %86 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %87 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x24x144xf16>}> : () -> tensor<1x1x24x144xf16>\n",
            "  %88 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %89 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %90 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x144x24xf16>}> : () -> tensor<1x1x144x24xf16>\n",
            "  %91 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<24xf32>}> : () -> tensor<24xf32>\n",
            "  %92 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<24xf32>}> : () -> tensor<24xf32>\n",
            "  %93 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x144x1xf16>}> : () -> tensor<3x3x144x1xf16>\n",
            "  %94 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %95 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %96 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x24x144xf16>}> : () -> tensor<1x1x24x144xf16>\n",
            "  %97 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %98 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n",
            "  %99 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x144x32xf16>}> : () -> tensor<1x1x144x32xf16>\n",
            "  %100 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %101 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %102 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x192x1xf16>}> : () -> tensor<3x3x192x1xf16>\n",
            "  %103 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %104 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %105 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x32x192xf16>}> : () -> tensor<1x1x32x192xf16>\n",
            "  %106 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %107 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %108 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x192x32xf16>}> : () -> tensor<1x1x192x32xf16>\n",
            "  %109 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %110 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %111 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x192x1xf16>}> : () -> tensor<3x3x192x1xf16>\n",
            "  %112 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %113 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %114 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x32x192xf16>}> : () -> tensor<1x1x32x192xf16>\n",
            "  %115 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %116 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %117 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x192x32xf16>}> : () -> tensor<1x1x192x32xf16>\n",
            "  %118 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %119 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %120 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x192x1xf16>}> : () -> tensor<3x3x192x1xf16>\n",
            "  %121 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %122 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %123 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x32x192xf16>}> : () -> tensor<1x1x32x192xf16>\n",
            "  %124 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %125 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n",
            "  %126 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x192x64xf16>}> : () -> tensor<1x1x192x64xf16>\n",
            "  %127 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %128 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %129 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x384x1xf16>}> : () -> tensor<3x3x384x1xf16>\n",
            "  %130 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %131 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %132 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x64x384xf16>}> : () -> tensor<1x1x64x384xf16>\n",
            "  %133 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %134 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %135 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x384x64xf16>}> : () -> tensor<1x1x384x64xf16>\n",
            "  %136 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %137 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %138 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x384x1xf16>}> : () -> tensor<3x3x384x1xf16>\n",
            "  %139 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %140 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %141 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x64x384xf16>}> : () -> tensor<1x1x64x384xf16>\n",
            "  %142 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %143 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %144 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x384x64xf16>}> : () -> tensor<1x1x384x64xf16>\n",
            "  %145 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %146 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %147 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x384x1xf16>}> : () -> tensor<3x3x384x1xf16>\n",
            "  %148 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %149 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %150 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x64x384xf16>}> : () -> tensor<1x1x64x384xf16>\n",
            "  %151 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %152 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n",
            "  %153 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x384x64xf16>}> : () -> tensor<1x1x384x64xf16>\n",
            "  %154 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %155 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n",
            "  %156 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %157 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %158 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x3x32xf16>}> : () -> tensor<3x3x3x32xf16>\n",
            "  %159 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x320x1280xf16>}> : () -> tensor<1x1x320x1280xf16>\n",
            "  %160 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1280xf32>}> : () -> tensor<1280xf32>\n",
            "  %161 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1280xf32>}> : () -> tensor<1280xf32>\n",
            "  %162 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x32x1xf16>}> : () -> tensor<3x3x32x1xf16>\n",
            "  %163 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %164 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n",
            "  %165 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x32x16xf16>}> : () -> tensor<1x1x32x16xf16>\n",
            "  %166 = \"arith.constant\"() <{value = dense<[4.83980179, 7.01497889, 6.59778308, 7.4643445, 3.82265639, 6.78915548, 5.1570487, 4.63491488, 4.46355867, 6.7842741, 7.31219244, 4.33215332, 6.80388832, 6.5691595, 5.91440105, 4.81517363]> : tensor<16xf32>}> : () -> tensor<16xf32>\n",
            "  %167 = \"arith.constant\"() <{value = dense<[-1.79617035, 3.17504382, 16.2980614, 24.0359573, 13.8513613, 3.49967337, -0.0168914795, 7.31451178, -13.8333397, 3.81765318, -2.79449153, 18.9408112, -18.930912, -11.2135153, 7.71239185, -2.86968422]> : tensor<16xf32>}> : () -> tensor<16xf32>\n",
            "  %168 = \"tfl.cast\"(%arg0) : (tensor<?x224x224x3xf32>) -> tensor<?x224x224x3xf16>\n",
            "  %169 = \"tfl.cast\"(%168) : (tensor<?x224x224x3xf16>) -> tensor<?x224x224x3xf32>\n",
            "  %170 = \"tfl.cast\"(%169) : (tensor<?x224x224x3xf32>) -> tensor<?x224x224x3xf16>\n",
            "  %171 = \"tf.Conv2D\"(%170, %158) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 2, 2, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x224x224x3xf16>, tensor<3x3x3x32xf16>) -> tensor<?x112x112x32xf16>\n",
            "  %172 = \"tfl.cast\"(%171) : (tensor<?x112x112x32xf16>) -> tensor<?x112x112x32xf32>\n",
            "  %173 = \"tfl.mul\"(%172, %156) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x32xf32>, tensor<32xf32>) -> tensor<?x112x112x32xf32>\n",
            "  %174 = \"tfl.add\"(%173, %157) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x32xf32>, tensor<32xf32>) -> tensor<?x112x112x32xf32>\n",
            "  %175 = \"tfl.cast\"(%174) : (tensor<?x112x112x32xf32>) -> tensor<?x112x112x32xf16>\n",
            "  %176 = \"tf.Relu6\"(%175) {device = \"\"} : (tensor<?x112x112x32xf16>) -> tensor<?x112x112x32xf16>\n",
            "  %177 = \"tf.DepthwiseConv2dNative\"(%176, %162) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x112x112x32xf16>, tensor<3x3x32x1xf16>) -> tensor<?x112x112x32xf16>\n",
            "  %178 = \"tfl.cast\"(%177) : (tensor<?x112x112x32xf16>) -> tensor<?x112x112x32xf32>\n",
            "  %179 = \"tfl.mul\"(%178, %163) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x32xf32>, tensor<32xf32>) -> tensor<?x112x112x32xf32>\n",
            "  %180 = \"tfl.add\"(%179, %164) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x32xf32>, tensor<32xf32>) -> tensor<?x112x112x32xf32>\n",
            "  %181 = \"tfl.cast\"(%180) : (tensor<?x112x112x32xf32>) -> tensor<?x112x112x32xf16>\n",
            "  %182 = \"tf.Relu6\"(%181) {device = \"\"} : (tensor<?x112x112x32xf16>) -> tensor<?x112x112x32xf16>\n",
            "  %183 = \"tf.Conv2D\"(%182, %165) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x112x112x32xf16>, tensor<1x1x32x16xf16>) -> tensor<?x112x112x16xf16>\n",
            "  %184 = \"tfl.cast\"(%183) : (tensor<?x112x112x16xf16>) -> tensor<?x112x112x16xf32>\n",
            "  %185 = \"tfl.mul\"(%184, %166) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x16xf32>, tensor<16xf32>) -> tensor<?x112x112x16xf32>\n",
            "  %186 = \"tfl.add\"(%185, %167) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x16xf32>, tensor<16xf32>) -> tensor<?x112x112x16xf32>\n",
            "  %187 = \"tfl.cast\"(%186) : (tensor<?x112x112x16xf32>) -> tensor<?x112x112x16xf16>\n",
            "  %188 = \"tf.Conv2D\"(%187, %78) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x112x112x16xf16>, tensor<1x1x16x96xf16>) -> tensor<?x112x112x96xf16>\n",
            "  %189 = \"tfl.cast\"(%188) : (tensor<?x112x112x96xf16>) -> tensor<?x112x112x96xf32>\n",
            "  %190 = \"tfl.mul\"(%189, %79) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x96xf32>, tensor<96xf32>) -> tensor<?x112x112x96xf32>\n",
            "  %191 = \"tfl.add\"(%190, %80) <{fused_activation_function = \"NONE\"}> : (tensor<?x112x112x96xf32>, tensor<96xf32>) -> tensor<?x112x112x96xf32>\n",
            "  %192 = \"tfl.cast\"(%191) : (tensor<?x112x112x96xf32>) -> tensor<?x112x112x96xf16>\n",
            "  %193 = \"tf.Relu6\"(%192) {device = \"\"} : (tensor<?x112x112x96xf16>) -> tensor<?x112x112x96xf16>\n",
            "  %194 = \"tf.Pad\"(%193, %5) {device = \"\"} : (tensor<?x112x112x96xf16>, tensor<4x2xi32>) -> tensor<?x113x113x96xf16>\n",
            "  %195 = \"tf.DepthwiseConv2dNative\"(%194, %75) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x113x113x96xf16>, tensor<3x3x96x1xf16>) -> tensor<?x56x56x96xf16>\n",
            "  %196 = \"tfl.cast\"(%195) : (tensor<?x56x56x96xf16>) -> tensor<?x56x56x96xf32>\n",
            "  %197 = \"tfl.mul\"(%196, %76) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x96xf32>, tensor<96xf32>) -> tensor<?x56x56x96xf32>\n",
            "  %198 = \"tfl.add\"(%197, %77) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x96xf32>, tensor<96xf32>) -> tensor<?x56x56x96xf32>\n",
            "  %199 = \"tfl.cast\"(%198) : (tensor<?x56x56x96xf32>) -> tensor<?x56x56x96xf16>\n",
            "  %200 = \"tf.Relu6\"(%199) {device = \"\"} : (tensor<?x56x56x96xf16>) -> tensor<?x56x56x96xf16>\n",
            "  %201 = \"tf.Conv2D\"(%200, %81) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x96xf16>, tensor<1x1x96x24xf16>) -> tensor<?x56x56x24xf16>\n",
            "  %202 = \"tfl.cast\"(%201) : (tensor<?x56x56x24xf16>) -> tensor<?x56x56x24xf32>\n",
            "  %203 = \"tfl.mul\"(%202, %82) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x24xf32>, tensor<24xf32>) -> tensor<?x56x56x24xf32>\n",
            "  %204 = \"tfl.add\"(%203, %83) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x24xf32>, tensor<24xf32>) -> tensor<?x56x56x24xf32>\n",
            "  %205 = \"tfl.cast\"(%204) : (tensor<?x56x56x24xf32>) -> tensor<?x56x56x24xf16>\n",
            "  %206 = \"tf.Conv2D\"(%205, %87) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x24xf16>, tensor<1x1x24x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "  %207 = \"tfl.cast\"(%206) : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf32>\n",
            "  %208 = \"tfl.mul\"(%207, %88) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x144xf32>, tensor<144xf32>) -> tensor<?x56x56x144xf32>\n",
            "  %209 = \"tfl.add\"(%208, %89) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x144xf32>, tensor<144xf32>) -> tensor<?x56x56x144xf32>\n",
            "  %210 = \"tfl.cast\"(%209) : (tensor<?x56x56x144xf32>) -> tensor<?x56x56x144xf16>\n",
            "  %211 = \"tf.Relu6\"(%210) {device = \"\"} : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "  %212 = \"tf.DepthwiseConv2dNative\"(%211, %84) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x56x56x144xf16>, tensor<3x3x144x1xf16>) -> tensor<?x56x56x144xf16>\n",
            "  %213 = \"tfl.cast\"(%212) : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf32>\n",
            "  %214 = \"tfl.mul\"(%213, %85) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x144xf32>, tensor<144xf32>) -> tensor<?x56x56x144xf32>\n",
            "  %215 = \"tfl.add\"(%214, %86) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x144xf32>, tensor<144xf32>) -> tensor<?x56x56x144xf32>\n",
            "  %216 = \"tfl.cast\"(%215) : (tensor<?x56x56x144xf32>) -> tensor<?x56x56x144xf16>\n",
            "  %217 = \"tf.Relu6\"(%216) {device = \"\"} : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "  %218 = \"tf.Conv2D\"(%217, %90) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x144xf16>, tensor<1x1x144x24xf16>) -> tensor<?x56x56x24xf16>\n",
            "  %219 = \"tfl.cast\"(%218) : (tensor<?x56x56x24xf16>) -> tensor<?x56x56x24xf32>\n",
            "  %220 = \"tfl.mul\"(%219, %91) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x24xf32>, tensor<24xf32>) -> tensor<?x56x56x24xf32>\n",
            "  %221 = \"tfl.add\"(%220, %92) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x24xf32>, tensor<24xf32>) -> tensor<?x56x56x24xf32>\n",
            "  %222 = \"tfl.cast\"(%221) : (tensor<?x56x56x24xf32>) -> tensor<?x56x56x24xf16>\n",
            "  %223 = \"tf.AddV2\"(%205, %222) {device = \"\"} : (tensor<?x56x56x24xf16>, tensor<?x56x56x24xf16>) -> tensor<?x56x56x24xf16>\n",
            "  %224 = \"tf.Conv2D\"(%223, %96) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x56x56x24xf16>, tensor<1x1x24x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "  %225 = \"tfl.cast\"(%224) : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf32>\n",
            "  %226 = \"tfl.mul\"(%225, %97) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x144xf32>, tensor<144xf32>) -> tensor<?x56x56x144xf32>\n",
            "  %227 = \"tfl.add\"(%226, %98) <{fused_activation_function = \"NONE\"}> : (tensor<?x56x56x144xf32>, tensor<144xf32>) -> tensor<?x56x56x144xf32>\n",
            "  %228 = \"tfl.cast\"(%227) : (tensor<?x56x56x144xf32>) -> tensor<?x56x56x144xf16>\n",
            "  %229 = \"tf.Relu6\"(%228) {device = \"\"} : (tensor<?x56x56x144xf16>) -> tensor<?x56x56x144xf16>\n",
            "  %230 = \"tf.Pad\"(%229, %5) {device = \"\"} : (tensor<?x56x56x144xf16>, tensor<4x2xi32>) -> tensor<?x57x57x144xf16>\n",
            "  %231 = \"tf.DepthwiseConv2dNative\"(%230, %93) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x57x57x144xf16>, tensor<3x3x144x1xf16>) -> tensor<?x28x28x144xf16>\n",
            "  %232 = \"tfl.cast\"(%231) : (tensor<?x28x28x144xf16>) -> tensor<?x28x28x144xf32>\n",
            "  %233 = \"tfl.mul\"(%232, %94) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x144xf32>, tensor<144xf32>) -> tensor<?x28x28x144xf32>\n",
            "  %234 = \"tfl.add\"(%233, %95) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x144xf32>, tensor<144xf32>) -> tensor<?x28x28x144xf32>\n",
            "  %235 = \"tfl.cast\"(%234) : (tensor<?x28x28x144xf32>) -> tensor<?x28x28x144xf16>\n",
            "  %236 = \"tf.Relu6\"(%235) {device = \"\"} : (tensor<?x28x28x144xf16>) -> tensor<?x28x28x144xf16>\n",
            "  %237 = \"tf.Conv2D\"(%236, %99) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x144xf16>, tensor<1x1x144x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "  %238 = \"tfl.cast\"(%237) : (tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf32>\n",
            "  %239 = \"tfl.mul\"(%238, %100) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x32xf32>, tensor<32xf32>) -> tensor<?x28x28x32xf32>\n",
            "  %240 = \"tfl.add\"(%239, %101) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x32xf32>, tensor<32xf32>) -> tensor<?x28x28x32xf32>\n",
            "  %241 = \"tfl.cast\"(%240) : (tensor<?x28x28x32xf32>) -> tensor<?x28x28x32xf16>\n",
            "  %242 = \"tf.Conv2D\"(%241, %105) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %243 = \"tfl.cast\"(%242) : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf32>\n",
            "  %244 = \"tfl.mul\"(%243, %106) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %245 = \"tfl.add\"(%244, %107) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %246 = \"tfl.cast\"(%245) : (tensor<?x28x28x192xf32>) -> tensor<?x28x28x192xf16>\n",
            "  %247 = \"tf.Relu6\"(%246) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %248 = \"tf.DepthwiseConv2dNative\"(%247, %102) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %249 = \"tfl.cast\"(%248) : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf32>\n",
            "  %250 = \"tfl.mul\"(%249, %103) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %251 = \"tfl.add\"(%250, %104) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %252 = \"tfl.cast\"(%251) : (tensor<?x28x28x192xf32>) -> tensor<?x28x28x192xf16>\n",
            "  %253 = \"tf.Relu6\"(%252) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %254 = \"tf.Conv2D\"(%253, %108) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<1x1x192x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "  %255 = \"tfl.cast\"(%254) : (tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf32>\n",
            "  %256 = \"tfl.mul\"(%255, %109) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x32xf32>, tensor<32xf32>) -> tensor<?x28x28x32xf32>\n",
            "  %257 = \"tfl.add\"(%256, %110) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x32xf32>, tensor<32xf32>) -> tensor<?x28x28x32xf32>\n",
            "  %258 = \"tfl.cast\"(%257) : (tensor<?x28x28x32xf32>) -> tensor<?x28x28x32xf16>\n",
            "  %259 = \"tf.AddV2\"(%241, %258) {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "  %260 = \"tf.Conv2D\"(%259, %114) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %261 = \"tfl.cast\"(%260) : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf32>\n",
            "  %262 = \"tfl.mul\"(%261, %115) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %263 = \"tfl.add\"(%262, %116) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %264 = \"tfl.cast\"(%263) : (tensor<?x28x28x192xf32>) -> tensor<?x28x28x192xf16>\n",
            "  %265 = \"tf.Relu6\"(%264) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %266 = \"tf.DepthwiseConv2dNative\"(%265, %111) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %267 = \"tfl.cast\"(%266) : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf32>\n",
            "  %268 = \"tfl.mul\"(%267, %112) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %269 = \"tfl.add\"(%268, %113) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %270 = \"tfl.cast\"(%269) : (tensor<?x28x28x192xf32>) -> tensor<?x28x28x192xf16>\n",
            "  %271 = \"tf.Relu6\"(%270) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %272 = \"tf.Conv2D\"(%271, %117) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<1x1x192x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "  %273 = \"tfl.cast\"(%272) : (tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf32>\n",
            "  %274 = \"tfl.mul\"(%273, %118) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x32xf32>, tensor<32xf32>) -> tensor<?x28x28x32xf32>\n",
            "  %275 = \"tfl.add\"(%274, %119) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x32xf32>, tensor<32xf32>) -> tensor<?x28x28x32xf32>\n",
            "  %276 = \"tfl.cast\"(%275) : (tensor<?x28x28x32xf32>) -> tensor<?x28x28x32xf16>\n",
            "  %277 = \"tf.AddV2\"(%259, %276) {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<?x28x28x32xf16>) -> tensor<?x28x28x32xf16>\n",
            "  %278 = \"tf.Conv2D\"(%277, %123) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x28x28x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %279 = \"tfl.cast\"(%278) : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf32>\n",
            "  %280 = \"tfl.mul\"(%279, %124) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %281 = \"tfl.add\"(%280, %125) <{fused_activation_function = \"NONE\"}> : (tensor<?x28x28x192xf32>, tensor<192xf32>) -> tensor<?x28x28x192xf32>\n",
            "  %282 = \"tfl.cast\"(%281) : (tensor<?x28x28x192xf32>) -> tensor<?x28x28x192xf16>\n",
            "  %283 = \"tf.Relu6\"(%282) {device = \"\"} : (tensor<?x28x28x192xf16>) -> tensor<?x28x28x192xf16>\n",
            "  %284 = \"tf.Pad\"(%283, %5) {device = \"\"} : (tensor<?x28x28x192xf16>, tensor<4x2xi32>) -> tensor<?x29x29x192xf16>\n",
            "  %285 = \"tf.DepthwiseConv2dNative\"(%284, %120) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x29x29x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x14x14x192xf16>\n",
            "  %286 = \"tfl.cast\"(%285) : (tensor<?x14x14x192xf16>) -> tensor<?x14x14x192xf32>\n",
            "  %287 = \"tfl.mul\"(%286, %121) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x192xf32>, tensor<192xf32>) -> tensor<?x14x14x192xf32>\n",
            "  %288 = \"tfl.add\"(%287, %122) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x192xf32>, tensor<192xf32>) -> tensor<?x14x14x192xf32>\n",
            "  %289 = \"tfl.cast\"(%288) : (tensor<?x14x14x192xf32>) -> tensor<?x14x14x192xf16>\n",
            "  %290 = \"tf.Relu6\"(%289) {device = \"\"} : (tensor<?x14x14x192xf16>) -> tensor<?x14x14x192xf16>\n",
            "  %291 = \"tf.Conv2D\"(%290, %126) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x192xf16>, tensor<1x1x192x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %292 = \"tfl.cast\"(%291) : (tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf32>\n",
            "  %293 = \"tfl.mul\"(%292, %127) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %294 = \"tfl.add\"(%293, %128) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %295 = \"tfl.cast\"(%294) : (tensor<?x14x14x64xf32>) -> tensor<?x14x14x64xf16>\n",
            "  %296 = \"tf.Conv2D\"(%295, %132) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %297 = \"tfl.cast\"(%296) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %298 = \"tfl.mul\"(%297, %133) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %299 = \"tfl.add\"(%298, %134) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %300 = \"tfl.cast\"(%299) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %301 = \"tf.Relu6\"(%300) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %302 = \"tf.DepthwiseConv2dNative\"(%301, %129) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %303 = \"tfl.cast\"(%302) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %304 = \"tfl.mul\"(%303, %130) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %305 = \"tfl.add\"(%304, %131) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %306 = \"tfl.cast\"(%305) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %307 = \"tf.Relu6\"(%306) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %308 = \"tf.Conv2D\"(%307, %135) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %309 = \"tfl.cast\"(%308) : (tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf32>\n",
            "  %310 = \"tfl.mul\"(%309, %136) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %311 = \"tfl.add\"(%310, %137) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %312 = \"tfl.cast\"(%311) : (tensor<?x14x14x64xf32>) -> tensor<?x14x14x64xf16>\n",
            "  %313 = \"tf.AddV2\"(%295, %312) {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %314 = \"tf.Conv2D\"(%313, %141) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %315 = \"tfl.cast\"(%314) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %316 = \"tfl.mul\"(%315, %142) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %317 = \"tfl.add\"(%316, %143) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %318 = \"tfl.cast\"(%317) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %319 = \"tf.Relu6\"(%318) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %320 = \"tf.DepthwiseConv2dNative\"(%319, %138) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %321 = \"tfl.cast\"(%320) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %322 = \"tfl.mul\"(%321, %139) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %323 = \"tfl.add\"(%322, %140) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %324 = \"tfl.cast\"(%323) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %325 = \"tf.Relu6\"(%324) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %326 = \"tf.Conv2D\"(%325, %144) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %327 = \"tfl.cast\"(%326) : (tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf32>\n",
            "  %328 = \"tfl.mul\"(%327, %145) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %329 = \"tfl.add\"(%328, %146) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %330 = \"tfl.cast\"(%329) : (tensor<?x14x14x64xf32>) -> tensor<?x14x14x64xf16>\n",
            "  %331 = \"tf.AddV2\"(%313, %330) {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %332 = \"tf.Conv2D\"(%331, %150) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %333 = \"tfl.cast\"(%332) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %334 = \"tfl.mul\"(%333, %151) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %335 = \"tfl.add\"(%334, %152) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %336 = \"tfl.cast\"(%335) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %337 = \"tf.Relu6\"(%336) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %338 = \"tf.DepthwiseConv2dNative\"(%337, %147) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %339 = \"tfl.cast\"(%338) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %340 = \"tfl.mul\"(%339, %148) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %341 = \"tfl.add\"(%340, %149) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %342 = \"tfl.cast\"(%341) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %343 = \"tf.Relu6\"(%342) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %344 = \"tf.Conv2D\"(%343, %153) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %345 = \"tfl.cast\"(%344) : (tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf32>\n",
            "  %346 = \"tfl.mul\"(%345, %154) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %347 = \"tfl.add\"(%346, %155) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x64xf32>, tensor<64xf32>) -> tensor<?x14x14x64xf32>\n",
            "  %348 = \"tfl.cast\"(%347) : (tensor<?x14x14x64xf32>) -> tensor<?x14x14x64xf16>\n",
            "  %349 = \"tf.AddV2\"(%331, %348) {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<?x14x14x64xf16>) -> tensor<?x14x14x64xf16>\n",
            "  %350 = \"tf.Conv2D\"(%349, %15) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %351 = \"tfl.cast\"(%350) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %352 = \"tfl.mul\"(%351, %16) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %353 = \"tfl.add\"(%352, %17) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %354 = \"tfl.cast\"(%353) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %355 = \"tf.Relu6\"(%354) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %356 = \"tf.DepthwiseConv2dNative\"(%355, %12) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %357 = \"tfl.cast\"(%356) : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf32>\n",
            "  %358 = \"tfl.mul\"(%357, %13) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %359 = \"tfl.add\"(%358, %14) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x384xf32>, tensor<384xf32>) -> tensor<?x14x14x384xf32>\n",
            "  %360 = \"tfl.cast\"(%359) : (tensor<?x14x14x384xf32>) -> tensor<?x14x14x384xf16>\n",
            "  %361 = \"tf.Relu6\"(%360) {device = \"\"} : (tensor<?x14x14x384xf16>) -> tensor<?x14x14x384xf16>\n",
            "  %362 = \"tf.Conv2D\"(%361, %18) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x384xf16>, tensor<1x1x384x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "  %363 = \"tfl.cast\"(%362) : (tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf32>\n",
            "  %364 = \"tfl.mul\"(%363, %19) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x96xf32>, tensor<96xf32>) -> tensor<?x14x14x96xf32>\n",
            "  %365 = \"tfl.add\"(%364, %20) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x96xf32>, tensor<96xf32>) -> tensor<?x14x14x96xf32>\n",
            "  %366 = \"tfl.cast\"(%365) : (tensor<?x14x14x96xf32>) -> tensor<?x14x14x96xf16>\n",
            "  %367 = \"tf.Conv2D\"(%366, %24) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %368 = \"tfl.cast\"(%367) : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf32>\n",
            "  %369 = \"tfl.mul\"(%368, %25) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %370 = \"tfl.add\"(%369, %26) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %371 = \"tfl.cast\"(%370) : (tensor<?x14x14x576xf32>) -> tensor<?x14x14x576xf16>\n",
            "  %372 = \"tf.Relu6\"(%371) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %373 = \"tf.DepthwiseConv2dNative\"(%372, %21) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %374 = \"tfl.cast\"(%373) : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf32>\n",
            "  %375 = \"tfl.mul\"(%374, %22) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %376 = \"tfl.add\"(%375, %23) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %377 = \"tfl.cast\"(%376) : (tensor<?x14x14x576xf32>) -> tensor<?x14x14x576xf16>\n",
            "  %378 = \"tf.Relu6\"(%377) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %379 = \"tf.Conv2D\"(%378, %27) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<1x1x576x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "  %380 = \"tfl.cast\"(%379) : (tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf32>\n",
            "  %381 = \"tfl.mul\"(%380, %28) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x96xf32>, tensor<96xf32>) -> tensor<?x14x14x96xf32>\n",
            "  %382 = \"tfl.add\"(%381, %29) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x96xf32>, tensor<96xf32>) -> tensor<?x14x14x96xf32>\n",
            "  %383 = \"tfl.cast\"(%382) : (tensor<?x14x14x96xf32>) -> tensor<?x14x14x96xf16>\n",
            "  %384 = \"tf.AddV2\"(%366, %383) {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "  %385 = \"tf.Conv2D\"(%384, %33) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %386 = \"tfl.cast\"(%385) : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf32>\n",
            "  %387 = \"tfl.mul\"(%386, %34) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %388 = \"tfl.add\"(%387, %35) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %389 = \"tfl.cast\"(%388) : (tensor<?x14x14x576xf32>) -> tensor<?x14x14x576xf16>\n",
            "  %390 = \"tf.Relu6\"(%389) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %391 = \"tf.DepthwiseConv2dNative\"(%390, %30) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %392 = \"tfl.cast\"(%391) : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf32>\n",
            "  %393 = \"tfl.mul\"(%392, %31) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %394 = \"tfl.add\"(%393, %32) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %395 = \"tfl.cast\"(%394) : (tensor<?x14x14x576xf32>) -> tensor<?x14x14x576xf16>\n",
            "  %396 = \"tf.Relu6\"(%395) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %397 = \"tf.Conv2D\"(%396, %36) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<1x1x576x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "  %398 = \"tfl.cast\"(%397) : (tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf32>\n",
            "  %399 = \"tfl.mul\"(%398, %37) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x96xf32>, tensor<96xf32>) -> tensor<?x14x14x96xf32>\n",
            "  %400 = \"tfl.add\"(%399, %38) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x96xf32>, tensor<96xf32>) -> tensor<?x14x14x96xf32>\n",
            "  %401 = \"tfl.cast\"(%400) : (tensor<?x14x14x96xf32>) -> tensor<?x14x14x96xf16>\n",
            "  %402 = \"tf.AddV2\"(%384, %401) {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<?x14x14x96xf16>) -> tensor<?x14x14x96xf16>\n",
            "  %403 = \"tf.Conv2D\"(%402, %42) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x14x14x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %404 = \"tfl.cast\"(%403) : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf32>\n",
            "  %405 = \"tfl.mul\"(%404, %43) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %406 = \"tfl.add\"(%405, %44) <{fused_activation_function = \"NONE\"}> : (tensor<?x14x14x576xf32>, tensor<576xf32>) -> tensor<?x14x14x576xf32>\n",
            "  %407 = \"tfl.cast\"(%406) : (tensor<?x14x14x576xf32>) -> tensor<?x14x14x576xf16>\n",
            "  %408 = \"tf.Relu6\"(%407) {device = \"\"} : (tensor<?x14x14x576xf16>) -> tensor<?x14x14x576xf16>\n",
            "  %409 = \"tf.Pad\"(%408, %5) {device = \"\"} : (tensor<?x14x14x576xf16>, tensor<4x2xi32>) -> tensor<?x15x15x576xf16>\n",
            "  %410 = \"tf.DepthwiseConv2dNative\"(%409, %39) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x15x15x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x7x7x576xf16>\n",
            "  %411 = \"tfl.cast\"(%410) : (tensor<?x7x7x576xf16>) -> tensor<?x7x7x576xf32>\n",
            "  %412 = \"tfl.mul\"(%411, %40) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x576xf32>, tensor<576xf32>) -> tensor<?x7x7x576xf32>\n",
            "  %413 = \"tfl.add\"(%412, %41) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x576xf32>, tensor<576xf32>) -> tensor<?x7x7x576xf32>\n",
            "  %414 = \"tfl.cast\"(%413) : (tensor<?x7x7x576xf32>) -> tensor<?x7x7x576xf16>\n",
            "  %415 = \"tf.Relu6\"(%414) {device = \"\"} : (tensor<?x7x7x576xf16>) -> tensor<?x7x7x576xf16>\n",
            "  %416 = \"tf.Conv2D\"(%415, %45) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x576xf16>, tensor<1x1x576x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "  %417 = \"tfl.cast\"(%416) : (tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf32>\n",
            "  %418 = \"tfl.mul\"(%417, %46) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x160xf32>, tensor<160xf32>) -> tensor<?x7x7x160xf32>\n",
            "  %419 = \"tfl.add\"(%418, %47) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x160xf32>, tensor<160xf32>) -> tensor<?x7x7x160xf32>\n",
            "  %420 = \"tfl.cast\"(%419) : (tensor<?x7x7x160xf32>) -> tensor<?x7x7x160xf16>\n",
            "  %421 = \"tf.Conv2D\"(%420, %51) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %422 = \"tfl.cast\"(%421) : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf32>\n",
            "  %423 = \"tfl.mul\"(%422, %52) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %424 = \"tfl.add\"(%423, %53) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %425 = \"tfl.cast\"(%424) : (tensor<?x7x7x960xf32>) -> tensor<?x7x7x960xf16>\n",
            "  %426 = \"tf.Relu6\"(%425) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %427 = \"tf.DepthwiseConv2dNative\"(%426, %48) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %428 = \"tfl.cast\"(%427) : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf32>\n",
            "  %429 = \"tfl.mul\"(%428, %49) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %430 = \"tfl.add\"(%429, %50) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %431 = \"tfl.cast\"(%430) : (tensor<?x7x7x960xf32>) -> tensor<?x7x7x960xf16>\n",
            "  %432 = \"tf.Relu6\"(%431) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %433 = \"tf.Conv2D\"(%432, %54) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<1x1x960x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "  %434 = \"tfl.cast\"(%433) : (tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf32>\n",
            "  %435 = \"tfl.mul\"(%434, %55) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x160xf32>, tensor<160xf32>) -> tensor<?x7x7x160xf32>\n",
            "  %436 = \"tfl.add\"(%435, %56) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x160xf32>, tensor<160xf32>) -> tensor<?x7x7x160xf32>\n",
            "  %437 = \"tfl.cast\"(%436) : (tensor<?x7x7x160xf32>) -> tensor<?x7x7x160xf16>\n",
            "  %438 = \"tf.AddV2\"(%420, %437) {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "  %439 = \"tf.Conv2D\"(%438, %60) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %440 = \"tfl.cast\"(%439) : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf32>\n",
            "  %441 = \"tfl.mul\"(%440, %61) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %442 = \"tfl.add\"(%441, %62) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %443 = \"tfl.cast\"(%442) : (tensor<?x7x7x960xf32>) -> tensor<?x7x7x960xf16>\n",
            "  %444 = \"tf.Relu6\"(%443) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %445 = \"tf.DepthwiseConv2dNative\"(%444, %57) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %446 = \"tfl.cast\"(%445) : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf32>\n",
            "  %447 = \"tfl.mul\"(%446, %58) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %448 = \"tfl.add\"(%447, %59) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %449 = \"tfl.cast\"(%448) : (tensor<?x7x7x960xf32>) -> tensor<?x7x7x960xf16>\n",
            "  %450 = \"tf.Relu6\"(%449) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %451 = \"tf.Conv2D\"(%450, %63) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<1x1x960x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "  %452 = \"tfl.cast\"(%451) : (tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf32>\n",
            "  %453 = \"tfl.mul\"(%452, %64) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x160xf32>, tensor<160xf32>) -> tensor<?x7x7x160xf32>\n",
            "  %454 = \"tfl.add\"(%453, %65) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x160xf32>, tensor<160xf32>) -> tensor<?x7x7x160xf32>\n",
            "  %455 = \"tfl.cast\"(%454) : (tensor<?x7x7x160xf32>) -> tensor<?x7x7x160xf16>\n",
            "  %456 = \"tf.AddV2\"(%438, %455) {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<?x7x7x160xf16>) -> tensor<?x7x7x160xf16>\n",
            "  %457 = \"tf.Conv2D\"(%456, %69) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %458 = \"tfl.cast\"(%457) : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf32>\n",
            "  %459 = \"tfl.mul\"(%458, %70) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %460 = \"tfl.add\"(%459, %71) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %461 = \"tfl.cast\"(%460) : (tensor<?x7x7x960xf32>) -> tensor<?x7x7x960xf16>\n",
            "  %462 = \"tf.Relu6\"(%461) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %463 = \"tf.DepthwiseConv2dNative\"(%462, %66) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %464 = \"tfl.cast\"(%463) : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf32>\n",
            "  %465 = \"tfl.mul\"(%464, %67) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %466 = \"tfl.add\"(%465, %68) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x960xf32>, tensor<960xf32>) -> tensor<?x7x7x960xf32>\n",
            "  %467 = \"tfl.cast\"(%466) : (tensor<?x7x7x960xf32>) -> tensor<?x7x7x960xf16>\n",
            "  %468 = \"tf.Relu6\"(%467) {device = \"\"} : (tensor<?x7x7x960xf16>) -> tensor<?x7x7x960xf16>\n",
            "  %469 = \"tf.Conv2D\"(%468, %72) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x960xf16>, tensor<1x1x960x320xf16>) -> tensor<?x7x7x320xf16>\n",
            "  %470 = \"tfl.cast\"(%469) : (tensor<?x7x7x320xf16>) -> tensor<?x7x7x320xf32>\n",
            "  %471 = \"tfl.mul\"(%470, %73) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x320xf32>, tensor<320xf32>) -> tensor<?x7x7x320xf32>\n",
            "  %472 = \"tfl.add\"(%471, %74) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x320xf32>, tensor<320xf32>) -> tensor<?x7x7x320xf32>\n",
            "  %473 = \"tfl.cast\"(%472) : (tensor<?x7x7x320xf32>) -> tensor<?x7x7x320xf16>\n",
            "  %474 = \"tf.Conv2D\"(%473, %159) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x7x7x320xf16>, tensor<1x1x320x1280xf16>) -> tensor<?x7x7x1280xf16>\n",
            "  %475 = \"tfl.cast\"(%474) : (tensor<?x7x7x1280xf16>) -> tensor<?x7x7x1280xf32>\n",
            "  %476 = \"tfl.mul\"(%475, %160) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x1280xf32>, tensor<1280xf32>) -> tensor<?x7x7x1280xf32>\n",
            "  %477 = \"tfl.add\"(%476, %161) <{fused_activation_function = \"NONE\"}> : (tensor<?x7x7x1280xf32>, tensor<1280xf32>) -> tensor<?x7x7x1280xf32>\n",
            "  %478 = \"tfl.cast\"(%477) : (tensor<?x7x7x1280xf32>) -> tensor<?x7x7x1280xf16>\n",
            "  %479 = \"tf.Relu6\"(%478) {device = \"\"} : (tensor<?x7x7x1280xf16>) -> tensor<?x7x7x1280xf16>\n",
            "  %480 = \"tfl.cast\"(%479) : (tensor<?x7x7x1280xf16>) -> tensor<?x7x7x1280xf32>\n",
            "  %481 = \"tfl.mean\"(%480, %4) <{keep_dims = false}> : (tensor<?x7x7x1280xf32>, tensor<2xi32>) -> tensor<?x1280xf32>\n",
            "  %482 = \"tfl.cast\"(%481) : (tensor<?x1280xf32>) -> tensor<?x1280xf16>\n",
            "  %483 = \"tf.MatMul\"(%482, %2) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x1280xf16>, tensor<1024x1280xf16>) -> tensor<?x1024xf16>\n",
            "  %484 = \"tf.BiasAdd\"(%483, %11) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x1024xf16>, tensor<1024xf16>) -> tensor<?x1024xf16>\n",
            "  %485 = \"tfl.cast\"(%484) : (tensor<?x1024xf16>) -> tensor<?x1024xf32>\n",
            "  %486 = \"tfl.mul\"(%485, %8) <{fused_activation_function = \"NONE\"}> : (tensor<?x1024xf32>, tensor<1024xf32>) -> tensor<?x1024xf32>\n",
            "  %487 = \"tfl.add\"(%486, %9) <{fused_activation_function = \"NONE\"}> : (tensor<?x1024xf32>, tensor<1024xf32>) -> tensor<?x1024xf32>\n",
            "  %488 = \"tfl.cast\"(%487) : (tensor<?x1024xf32>) -> tensor<?x1024xf16>\n",
            "  %489 = \"tf.Relu\"(%488) {device = \"\"} : (tensor<?x1024xf16>) -> tensor<?x1024xf16>\n",
            "  %490 = \"tf.MatMul\"(%489, %1) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x1024xf16>, tensor<512x1024xf16>) -> tensor<?x512xf16>\n",
            "  %491 = \"tf.BiasAdd\"(%490, %10) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x512xf16>, tensor<512xf16>) -> tensor<?x512xf16>\n",
            "  %492 = \"tfl.cast\"(%491) : (tensor<?x512xf16>) -> tensor<?x512xf32>\n",
            "  %493 = \"tfl.mul\"(%492, %6) <{fused_activation_function = \"NONE\"}> : (tensor<?x512xf32>, tensor<512xf32>) -> tensor<?x512xf32>\n",
            "  %494 = \"tfl.add\"(%493, %7) <{fused_activation_function = \"NONE\"}> : (tensor<?x512xf32>, tensor<512xf32>) -> tensor<?x512xf32>\n",
            "  %495 = \"tfl.cast\"(%494) : (tensor<?x512xf32>) -> tensor<?x512xf16>\n",
            "  %496 = \"tf.Relu\"(%495) {device = \"\"} : (tensor<?x512xf16>) -> tensor<?x512xf16>\n",
            "  %497 = \"tfl.cast\"(%496) : (tensor<?x512xf16>) -> tensor<?x512xf32>\n",
            "  %498 = \"tfl.fully_connected\"(%497, %0, %3) <{fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"}> : (tensor<?x512xf32>, tensor<101x512xf32>, tensor<101xf32>) -> tensor<?x101xf32>\n",
            "  %499 = \"tfl.softmax\"(%498) <{beta = 1.000000e+00 : f32}> : (tensor<?x101xf32>) -> tensor<?x101xf32>\n",
            "  \"func.return\"(%499) : (tensor<?x101xf32>) -> ()\n",
            "}) {tf.entry_function = {control_outputs = \"\", inputs = \"serving_default_keras_tensor_155:0\", outputs = \"StatefulPartitionedCall_1:0\"}, tf_saved_model.exported_names = [\"serving_default\"]} : () -> ()\n",
            "\n",
            "\n",
            "ğŸ“Š Strategy 3: Using TensorFlow Lite with Flex ops...\n",
            "Saved artifact at '/tmp/tmpebpxqs28'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_155')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 101), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  137187261267216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256713680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256714640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187261267408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187261267792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187261267600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256713488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256714448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256715792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256714256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256716368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256717904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256716560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256717712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256716752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256719632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256720976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256721360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256721168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256720016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256722512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256722128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256722320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256722704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256720592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256724816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256725200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256725584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256725392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256720208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256726736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256727120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256727504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256727312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256723664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256728656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256728272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256727888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256729232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187256724432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916958480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916959824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916960208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916960016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916958672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916959248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916961936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916962320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916962128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916958288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916963472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916963856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916964240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916964048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916960976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916965392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916965776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916966160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916965968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916961552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916967312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916967696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916968080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916967888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916963088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916969232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916969616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916970000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916969808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916965008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916971152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916971536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916971920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916971728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916966928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916973072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916973456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916974224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916973648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916968848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916972304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917351504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917352656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186916972688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917352464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917353808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917354192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917354576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917354384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917352272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917355728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917356112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917356496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917356304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917351696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917357648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917358032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917358416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917358224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917353424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917359568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917359952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917360336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917360144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917355344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917361488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917361872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917362256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917362064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917357264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917363408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917363792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917364176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917363984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917359184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917365328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917365712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917366096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917365904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917361104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917367248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917351888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917367632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917367440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917364944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917876176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917877520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917877904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917877712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917876368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917879056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917879440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917879824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917879632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917876944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917880976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917881360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917881744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917881552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917877136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917882896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917883280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917883664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917883472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917878672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917884816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917885200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917885584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917885392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917880592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917886736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917887120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917887504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917887312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917882512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917888656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917889040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917889424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917889232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917884432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917890576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917890960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917891728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917891152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917886352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917889808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918269968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918269008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186917890192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918269200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918271312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918271696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918272080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918271888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918269392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918273232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918273616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918274000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918273808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918269776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918275152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918275536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918275920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918275728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918270928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918277072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918277456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918277840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918277648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918272848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918278992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918279376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918279760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918279568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918274768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918280912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918281296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918281680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918281488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918276688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918282832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918283216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918283600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918283408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918278608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918284752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918270160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918285136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918284944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918282448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918647184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918647568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918647952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918647760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918646032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918649104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918649488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918649872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918649680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918645840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918651024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918651408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918651792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918651600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918646608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918652944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918653328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918653712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918653520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918648720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918654864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918655248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918655632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918655440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918650640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918656784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918657168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918657552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918657360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918652560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918658704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918659088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918659472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918659280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918654480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918660624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918661008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918661776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918661200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918656400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918659856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251487760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251486800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137186918660240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251486992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251489104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251489488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251489872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251487952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251487184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251492176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251493520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251493712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251492944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251488336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251493328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251492752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251496208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251496400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251495632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251494480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251496016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251495440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137187251498896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "âœ… Strategy 3 successful! Model saved as 'food_classifier_flex.tflite'\n",
            "âš ï¸  Note: This model requires TensorFlow Lite with Flex ops support\n"
          ]
        }
      ]
    }
  ]
}